# Multivariate Analysis 


<p style = "margin-bottom: 0px; font-size: 20px; ">**Multivariate Analysis**</p>

- **Multivariate analysis**
  - **Concept**
    - Any analyses involving more than one outcome variable 
    - The process of estimating the relationship between the linear combination of several outcome variables and one or more predictor variables  

- **Strengths of Multivariate Analysis**
  - A multivariate model allows the relationships between the set of predictor variables and multiple outcome variables to be tested in one test, which can maintain the Type I error rate (as opposed to testing the relationship between the same set of predictor variables and different outcome variables with separate models, which would inflate Type I error rate due to multiple testing)
  - Takes into account the relationship between the outcome variables 
  - More power to detect whether groups differ along a combination of dimensions 
  - MANOVA has greater power than ANOVA to detect effects because it takes account of the correlations between the outcome variables (Huberty & Morris, 1989)
  
  
- **Centroids**
  - **Concept**
    - A centre point in a multidimensional geometric space 
  - **Mean centroid**
    - **Concept**
      - A centre point that represents the mean on multiple dimensions
      - A mean centroid is represented by a column vector 
    - **Mathematics (Matrix)**
      - ${\boldsymbol{\mu}} = \begin{pmatrix} \mu_{1} \\ \mu_{2} \\ \mu_{3} \\ \vdots \\ \mu_{d} \end{pmatrix}$
        - ***Where***
          - $d$ - The total number of outcome variables (variates)
        - 
  
- **Hotelling's $t^2$ (Hotelling, 1931)**
  - **Concept**
    - Multivariate version of the t-statistic
    - A test statistic for the difference between 2 mean centroids (or multivariate means; means on multiple outcome variables)
    - Tests whether 2 means differ on the linear combination of multiple outcome variables 
  - **Mathematics (Matrix)**
    - $t^2 = \frac{n_{1}n_{2}}{n_{1}n_{2}}(\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{'}(\mathbf{S})^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})$
      - ***Where***
        - $\mathbf{S}$ - Variance covariance matrix 
        - $\mathbf{Y_{1}}$ - Mean centroid for group 1
        - $\mathbf{Y_{2}}$ - Mean centroid for group 2
  - **The relationship between Mahalanobis $D^2$ and Hotelling's $t^2$**
    - **Concept**
      - The relationship between Mahalanobis $D^2$ and Hotelling's $t^2$ is like the relationship between Cohen's $d$ and Student's $t$
    - **Mathematics**
      - $t^2 = D^2 (\frac{n_{1}n_{2}}{n_{1}+n_{2}})$ 
  - **Distribution of Hotelling's $t^2$**
    - **Distribution of Hotelling's $t^2$ as itself**
      - **Concept**
        - Hotelling's $t^2$ has a Hotelling's T-squared distribution with degrees of freedom 1 of $p$ and degree of freedom 2 of $n_{1} + n_{2} - 2$
      - **Mathematics**
        - $t^2 \sim T^2(p, n_{1} + n_{2} - 2)$
    - **Distribution of Hotelling's $t^2$ as $F$**
      - **Concept**
        - Hotelling's $T^2$ can be converted into an $F$ statistic that has an $F$ distribution with degrees of freedom 1 of $p$ and degrees of freedom 2 of $n_{1} + n_{2} - p -1$
        - Hence, you can convert Hotelling's $T^2$ into an $F$ statistic and significance test with the $F$ distribution
      - **Mathematics**
        - $F = t^2\times\frac{v-p+1}{p(v)}$
      - **Distribution of $F$**
        - $F = t^2\times\frac{v-p+1}{p(v)} \sim F(p, n_{1} + n_{2} - p -1)$
  - **NHST**
    - **Hypotheses**
      - $H_0: \mathbf{Y_1} = \mathbf{Y_2}$ - The 2 mean centroids are the same - The 2 mean centroids are from the same population
      - $H_1: \mathbf{Y_1} â‰  \mathbf{Y_2}$ - The 2 mean centroids are not the same - The 2 mean centroids are from different populations


<p style = "margin-bottom: 0px; font-size: 20px; ">**Assumptions**</p>

- **Assumptions of MANOVA**
  - Uncorrelated errors 
  - Random sampling 
  - Multivariate normality 
  - Homogeneity of Covariance 
  - 
  
- **Homogeneity of Covariance**
  - **Concept**
    - The variance of each outcome variable and the covariance between any outcome variables is equal between all groups
    - The variance-covariance matrices are identical between groups
  - **Mathematics**
    - $\Sigma_1 = \Sigma_2 = \Sigma_3 = \cdots = \Sigma_k$
      - ***Where***
        - $\Sigma_k$ - The variance Covariance matrix for group $k$
        - $k$ - The total number of groups in the MANOVA
  - **Effects of violation of this assumption**
    - Only under this condition that the sampling distribution of $t^2\times\frac{v-p+1}{p(v)}$ has an $F$ distribution with degrees of freedom 1 of $p$ and degrees of freedom 2 of $n_{1} + n_{2} - p -1$
  - **Testing for Homogeneity of Covariance**
    - **Statistical Tests for Homogeneity of Covariance**
      - Box M test
    - **Box M test**
      - **Concept**
        - A test that test the null hypothesis that multiple covariance matrices are equal 
      - **The Box M statistic**
        - **Mathematics (Huberty & Olejnik, 2006, p.41)**   
         - $M = v_{e}\ln{|\mathbf{S_e}| - \sum_{k=1}^{k}{v_{k}\ln{|\mathbf{S_k}|}}}$
          - ***Where***
            - $k$ - Number of groups
            - $v_k$ - Degrees of freedom for group k (which is $n_k - 1$)
            - $v_e$ - Error degrees of freedom (which is $\sum_{k=1}^{k}{v_k}$)
            - $\mathbf{S_k}$ - Covariance matrix for group $k$
            - $\mathbf{S_e}$ - Error covariance matrix (Which is $\frac{\mathbf{E}}{v_e}$)
      - **Distribution of Box M**
        - **Chi-squared transformed Box M**
          - **Concept**
            - Box M can be transformed to a statistic that has a Chi-Squared distribution with degrees of freedom of $\frac{(k-1)(p+1)p}{2}$ 
          - **Transforming Box M**
            - **Mathematics** 
              - **For equal sample sizes**
                - $M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \right]$
              - **For unequal sample sizes**
                - $M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \times \left( \sum_{k=1}^{k}{v_{k}^{-1} - v_{e}^{-1}} \right) \right]$
          - **Distribution of $M_{\chi}$**
            - $M_{\chi} \sim \chi^2\left(\frac{\left(k-1\right)(p+1)p}{2}\right)$
        - **F-transformed Box M**
          - **Concept**   
            - Box M is transformed to a statistic that has an $F$ distribution
            - Details can be found in Rencher (2002, p.255-259)
          - **Assumptions**
            - Uncorrelated errors 
            - Multivariate normality 

- **Robust multivariate test of difference**
  - **Robust multivariate test of differences**
    - Yao Test
  - **Yao Test**
    - **Concept**
      - A robust Hotelling's $t^2$
    - **The Yao statistic**
      - **Mathematics**
        - $T^{2}_{Yao} = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{'}\left(\frac{\mathbf{S_1}}{n_1}+\frac{\mathbf{S_2}}{n_2}\right)^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})$
    - **Distribution of Yao statistic**
      - **F-transformed Yao**
        - **Concept**
          - $T^{2}_{Yao}$ can be transformed to a statistic that has an F distribution with degrees of freedom 1 of $p$ and degrees of freedom of $f - p + 1$
        - **Mathematics**
          - $ T^{2}_{Yao, F} = T^{2}_{Yao} \times \frac{f-p+1}{pf}$
            - ***Where***
              - $f = \sum_{k=1}^{k}{\frac{1}{n_k - 1}\left(\frac{V_k}{T^{2}_{Yao}}\right)^2}$
                - ***Where***
                  - $V_k = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{'} \mathbf{W}^{-1} \mathbf{W}_k \mathbf{W}^{-1} (\mathbf{Y_{1}-\mathbf{Y_2}})$
                    - ***Where***
                      - $\mathbf{W}= \sum_{k=1}^{k}{\mathbf{W_{k}}}$
                      - $\mathbf{W}_k = \frac{\mathbf{S}_k}{n_k}$
        - **Distribution of $T^{2}_{Yao, F}$**
          - $T^{2}_{Yao, F} \sim F \left( p, f-p+1\right)$






<p style = "margin-bottom: 0px; font-size: 20px; ">**Multivariate ANOVA**</p>

- **MANOVA**
  - **Concept**
    - **MANOVA** - Multivariate Analysis of Variance
    - The comparison of multiple group mean centroids 
    - The comparison of multiple group means on multiple outcome variables 
  - **Hypotheses**
    - **Null Hypothesis**
      - Multiple group mean centroids are identical (or are from the same population)
      - $\begin{alignat*}{1} &H_0:  ~~~~~\boldsymbol{\mu}_{1}& &= ~~~~~\boldsymbol{\mu}_{2}& &= ~~~~~\boldsymbol{\mu}_{3}& &= \cdots& &= ~~~~~\boldsymbol{\mu}_{k}& \\ &H_0: \begin{pmatrix} \mu_{1, 1} \\ \mu_{2, 1} \\ \mu_{3, 1} \\ \vdots \\ \mu_{v, 1}\end{pmatrix}&  &= \begin{pmatrix} \mu_{1, 2} \\ \mu_{2, 2} \\ \mu_{3, 2} \\ \vdots \\ \mu_{v, 2}\end{pmatrix}& &= \begin{pmatrix} \mu_{1, 3} \\ \mu_{2, 3} \\ \mu_{3, 3} \\ \vdots \\ \mu_{v, 3}\end{pmatrix}& &= \cdots& &= \begin{pmatrix} \mu_{1, k} \\ \mu_{2, k} \\ \mu_{3, k} \\ \vdots \\ \mu_{v, k}\end{pmatrix}& \end{alignat*}$
        - ***Where***
          - $\mu_{v, k}$ - Mean of group $k$ on outcome $v$ 
          - $k$ - Group 
          - $v$ - Variate

- **Multivariate F-ratio: The $\mathbf{HE}^{-1}$ matrix**
  - **Concept**
    - A matrix that represents the multivariate version of the F-ratio
    - A matrix that represents the ratio of systematic variance in a set of outcome variables (the variance in the linear combination of multiple outcome variables explained by the model) to the unsystematic variance in the set of outcome variables (the variance in the linear combination of multiple outcome variables not explained by the model)
  - **Eigenvectors of the $\mathbf{HE}^{-1}$ matrix**
    - Each of the Eigenvectors of the $\mathbf{HE}^{-1}$ matrix represents each of the underlying variates and the Eigenvalue of each Eigenvector represents the effect of the associated variate in the form of F-ratio (ratio of variance explained to variance unexplained)
  - **Mathematics (Matrix)**
    - $\mathbf{HE}^{-1}$
      - ***Where***
        - $\mathbf{H}$ - Hypothesis SSCP matrix
        - $\mathbf{E}$ - Error SSCP matrix 
      - ***Note***
        - $\textbf{H}$ and $\textbf{E}^{-1}$ are both symmetric, hence, the order in which they are multiplied does not matter, which means that $\mathbf{HE}^{-1} = \mathbf{E}^{-1}\mathbf{H}$
        
  - **Eigenvalues of the $\mathbf{HE}^{-1}$ matrix**
    - Each of the eigenvalues of the $\mathbf{HE}^{-1}$ matrix represents the F-ratio of each of the underlying variates 


- **Test statistic for MANOVA**
  - Wilk Lambda
  - Pillai-Bartlett trace
  - Hotelling-Lawley trace
  - Roy's largest root 
  
- **Wilks lambda (Wilks, 1932)**
  - **Wilks lambda statistic**
    - **Introduction**
      - The oldest and perhaps the most commonly used criterion 
    - **Mathematical description**
      - It is the ratio of the determinant of the Error SSCP matrix to the determinant of the Total SSCP matrix 
      - It represents the ratio of error variance to total variance (or the proportion of unexplained variance)
      - Represents the ratio of the variance in all of the outcome variables not explained by the model to the total variance in all of the outcome variables 
    - **Method 1**
      - **Description**
      - **Mathematics**
        - $\displaystyle \Lambda = \cfrac{|\mathbf{E}|}{|\mathbf{H} + \mathbf{E}|} = \cfrac{|\mathbf{E}|}{|\mathbf{T}|}$
    - **Method 2**
      - **Description**
        - The product of the ratio of variance unexplained to total variance for all variates
      - **Mathematics**
        - $\displaystyle \Lambda = \prod\limits_{v=1}^{v}{\frac{1}{1+\lambda_{v}}}$ 
          - ***Where***
            - $v$ - Variate
            - $\lambda_{v}$ - Eigenvalue for variate $v$
              - ***Note***
                - $\frac{1}{1+\lambda_{v}}$ changes the ratio of variance explained to the variance unexplained for variate $v$ ($\lambda_{v}$) to a ratio of variance unexplained to the total variance for variate $v$
                  - $\begin{aligned} \frac{1}{1+\lambda_{v}} &= \frac{1}{1+\frac{SS_{M,v}}{SS_{R,v}}} \\ &= \frac{1}{\frac{SS_{R,v}}{SS_{R,v}}+\frac{SS_{M,v}}{SS_{R,v}}} \\ &= \frac{SS_{R,v}}{SS_{R,v} + SS_{M,v}} \\ &= \frac{SS_{R,v}}{SS_{T,v}} \end{aligned}$
  - **Distribution of Wilks lambda**
    - **F-transformed Lambda**
      - **Description**
        - Wilks lambda is transformed to a test statistic that has an F distribution
      - **F-transformed Lambda for two groups**
        - $\Lambda_{F} = \cfrac{1-\Lambda}{\Lambda}\times \cfrac{v_{e} - p + 1}{p} \sim F\left( p, v_e - p + 1\right)$
      - **F-transformed Lambda for three groups**
          - $\Lambda_{F} = \cfrac{1-\sqrt{\Lambda}}{\sqrt{\Lambda}} \times \cfrac{v_{e} - p + 1}{p} \sim F\left( 2p, 2v_e - 2p + 2\right)$
          

- **Pillai-Bartlett trace**
  - **Concept** 
    - Aka Pillai's trace
    - Represents the ratio of variance in all the variates explained by the predictor variable to the total variance in all the variates $\left( \frac{SS_M}{SS_T} \right)$
    - It is the sum of the ratio of variance explained to the total variance of each of the variates 
  - **Mathematics**
    - $\displaystyle V = \sum_{v=1}^{v}{\frac{\lambda_{v}}{1 + \lambda_v}}$
      - ***Note***
        - $\frac{\lambda_{v}}{1 + \lambda_v}$ 
          - The squared canonical correlation between the grouping variable and the $v^{th}$ linear discriminant function 
          - This transforms $\lambda_v$ to the ratio of variance explained to the total variance 
            - $\begin{aligned} \frac{\lambda_{v}}{1 + \lambda_v} &= \frac{\frac{SS_M}{SS_R}}{1 + \frac{SS_M}{SS_R}} \\ &= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R}{SS_R} + \frac{SS_M}{SS_R}} \\ &= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R + SS_M}{SS_R}} \\ &= \frac{\frac{SS_M}{SS_R}}{\frac{SS_T}{SS_R}} \\ &= \frac{SS_M}{SS_R} \times \frac{SS_R}{SS_T} \\ &= \frac{SS_M}{SS_T} \end{aligned}$
  - **Distribution of Pillai's trace**
    - **F-transformed Pillai's trace**
      - **Concept**
        - Pillai's trace is transformed to a test statistic that has an $F$ distribution with degrees of freedom 1 of $br$ and degrees of freedom of $r\left( df_e - p + r \right)$
      - **Mathematics**
        - $\displaystyle V_F = \frac{V}{r - V} \times \frac{v_e - p + r}{b}$
          - ***Where***
            - $b = \max{\left( p, df_h \right)}$
            - $r = \min{\left( p, df_h \right)}$
      - **Distribution of F-transformed Pillai's trace**
        - $\displaystyle V_F \sim F \left( br, r\left( df_e - p + r \right)\right)$
        
        
- **Hotelling-Lawley trace**
  - **Concept**
    - It represents the ratio of variance explained to the variance unexplained 
    - It is the sum of the eigenvalues for all the variates 
  - **Mathematics**
    - $\displaystyle T = \sum_{v=1}^{v}{\lambda_{v}}$
  - **F-transformed Hotelling-Lawley trace**
    - **Description**
      - $T$ is transformed to a statistic so that it has a central $F$ distribution 
    - **Mathematics**
      - $\displaystyle T_F = T \times \left[ \frac{r\left( df_e - p - 1\right) + 2}{br^2 } \right] $
    - **Distribution of F-transformed Hotelling-Lawley trace**
      - $\displaystyle T_F \sim F \left( br, r(df_e - p -1) + 2 \right)$
        - ***Where***
          - $b = \max \left(p, df_h \right)$
          - $r = \min \left(p, df_h \right)$

- **Roy's largest Root**
  - **Concept**
    - Roy's largest root suggests only to consider the largest eigenvalue (which is this $\lambda_1$, because the first eigenvalue is the largest one) and ignore the rest (if there are multiple eigenvalues)
    - You can interpret it in anyway you like, for example, you can convert this this eigenvalue to a ratio of variance explained to the total variance using this $\frac{\lambda_1}{1+\lambda_1}$, the point is that you only need to consider the largest eigenvalue and do whatever you want with it
    - This method tends to be advocated
    - Because it has the largest ratio of variance explained to variance unexplained, it has the the most power
  - **Mathematics**
    - $\Theta = \lambda_1$
  - **F-transformed Roy's largest Root**
    - **Concept**
      - Roy's largest root can be transformed to a statistic that has an $F$ distribution
      - Roy's largest root is transformed to this statistic for NHST
    - **Mathematics**
      - $\Theta_F = \Theta \times \frac{N-b-1}{b}$
    - **Distribution of $\Theta_F$**
      - $\Theta_F \sim F \left( b, N - b - 1\right)$
        - ***Where*** 
          - $b = \max{\left( p, df_h\right)}$
        - ***Notes***
          - Rencher (2002) suggest that $df_e$ might be used instead of $N$


- **Central vs noncentral distribution**
  - **Central distribution** - Distributions that assume the null hypothesis is true
  - **Noncentral distribution** - Distributions that assume the alterative hypothesis is true


- **MANOVA effect sizes**
  - **MANOVA effect sizes**
    - Multivariate Eta-squared
    - Multivariate Omega-squared 
    - Tau-squared
    - Xi-squared 
    - Zeta-squared
  - **Multivariate Eta-squared**  
    - **Concept**
      - The original Eta-squared generalized to multivariate contexts
      - Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (proportion of variance explained) (Regardless of the number of constructs underlying the outcome variable)
    - **Mathematics**
      - **Huberty and Olejnik (2006)**
        - $\eta^{2}_{Multivariate}= 1 - \Lambda$
      - <a href="https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize?action=AttachFile&do=view&target=mvwl.doc#:~:text=Attachment%20%27mvwl.doc%27-,Download,-Current%20configuration%20does" target="_blank">**Cambridge**</a>
        - $\eta^{2}_{Multivariate}= 1 - \Lambda^{\frac{1}{r}}$
          - ***Where***
            - $r = \min\left(k - 1, y\right)$
              - ***Where***
                - $k$ - The number of levels of the factor
                - $y$ - The number of outcome variables 
  - **Multivariate Eta-squared**  
    - **Concept**
      - The multivariate extension of the univariate Omega-squared
      - Suggested by Tatsuoka (1970)
      - Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (regardless of the number of constructs underlying the outcome variables)
    - **Mathematics**
      - $\omega^{2}_{multivariate} = 1 - \frac{N\Lambda}{\left( N - df_H - 1 \right) + \Lambda}$


  - **Tau-squared (Cramer & Nicewander, 1979)**  
    - **Concept**
    - **Mathematics**
      - **Method 1**
        - $\displaystyle \tau^2 = 1 - \left( \prod \limits_{v=1}^{v}{1 - \frac{\lambda_v}{1+\lambda_v}} \right)^{\frac{1}{r}}$
          - ***Where***
            - $r = \min{\left(y, df_M \right)}$
              - ***Where***
                - $y$ - The number of outcome variables 
                - $df_M$ - Model degrees of freedom
              - ***Note***
                - $\left( \prod \limits_{v=1}^{v}{1 - \frac{\lambda_v}{1+\lambda_v}} \right)^{\frac{1}{r}}$ - This is the geometric mean proportion of variation in all of the variates that is not explained by the model
                

- **Xi squared**
  - **Concept**
    - This is the mean squared canonical correlation (mean proportion of variance in all the variates that is explained by the model; the variance explained per variate; it takes into account the number of variates estimated)
  - **Mathematics**
    - $\xi^2 = \frac{V}{s}$
      - ***Where***
        - $s$ - The number of variates estimated 
  - **Adjustment for positive bias of variance-based effect size**
    - **Serlin's Adjustment (Serlin, 1982)**  
      - **Mathematics**
        - $ \displaystyle \xi^2_{adj} = 1-\frac{N-1}{N-b-1} \times \left( 1 - \xi^2 \right)$
      - **Evaluation**
        - Kim and Olejnik (2005) provided simulation support for Serlin's adjustment 
        - It can also be used with $\tau^2$ and $\zeta^2$ but they work best when there are only 2 levels and when $df_M â‰¥ 2$ if the sample is large 


- **Hotelling-Lawley Effect size (Zeta-squared)**
  - **Description**
    - This effect size measure is associated with the Hotelling-Lawley test statistic
    - It takes into account the number of variates estimated, it represents the variance explained per variate (the average variance explained by each variate)
  - **Mathematics**
    - $\displaystyle \zeta^2 = \frac{V}{s + V}$
      - ***Where***
        - $s = \min \left( y, df_M\right)$
          - ***Where***
            - $y$ - The number of variates estimated 

- **Canonical Correlation**
  - **Description**
    - The correlation between the linear combination of predictor variables and the linear combination of outcome variables
    - In other words, it is the correlation between the set of predictor variables and $v^{th}$ variate 
  - **Mathematics**
    - $r_c = \sqrt{V}$
            
ans$adj.r.squared <- 1 - (1 - ans$r.squared) * ((n - 
            df.int)/rdf)



- **Positive bias of proportion of variance explained**
  - **Concept**
    - The sample proportion of variance explained is a positively-based estimator of the population proportion of variance explained 
    - There are methods for adjusting for this bias 
  - **Moderators of this bias**
    - The degree of positive bias increases as
      - The number of levels of the predictor increases
      - The number of outcome variables increases
      - The group sample size decreases
  - **Adjustments for $R^2$**
    - **The Smith formula (Ezekiel, 1928)**
      - **Mathematics**
        - $\displaystyle R_{adj}^{2} = 1 - \left( 1 - R^2\right) \times \frac{n}{n-p}$
          - ***Where***
            - $p$ - The number of predictors 
    - **The Wherry formula-1**
      - **Mathematics**
        - $\displaystyle R_{adj}^{2} = 1 - \left( 1 - R^2\right) \times \frac{n-1}{n-p-1}$
    - **The Wherry formula-2**
      - **Introduction**
        - This is implemented by many statistical packages (e.g. SAS, SPSS, R)
        - `lm()` uses this 
      - **Mathematics**
        - $\displaystyle R_{adj}^{2} = 1 - \left( 1 - R^2\right) \times \frac{n-1}{n-p}$
    - **The Olkin and Pratt formula (Olkin & Pratt, 1958)**
      - **Mathematics**
        - $\displaystyle R_{adj}^{2} = 1 - \frac{\left( N-3 \right) ( 1-R^2 )}{N - p - 1} \times \left[ 1 + \frac{2 (1 - R^2)}{N - p + 1}\right]$
    - **The Pratt formula**
      - **Mathematics**
        - $\displaystyle R^2 = 1 - \frac{(N - 3)(1 - R^2)}{N - p - 1} \times \left[ 1 + \frac{2 (1 - R^2)}{N - p - 2.3}\right]$
    - **The Claudy formula-3**
      - **Mathematics**
        - $\displaystyle R_{adj}^{2} = 1 - \frac{(N - 4)(1 - R^2)}{N - p - 1} \times \left[ 1 + \frac{2 (1 - R^2)}{N - p + 1}\right]$
  - **Which is the best??**
    - Yin and Fan (2001) conducted a simulation study in which they compared the 6 methods, they found that when the $\frac{N}{p}$ is large, almost all of those 6 methods were unbiased, and the Pratt formula better than all the others especially when the $\frac{N}{p}$ ratio is small. So Pratt's method is probably the way to go, but most statistical packages don't use it... (Also see <a href="https://stats.stackexchange.com/questions/48703/what-is-the-adjusted-r-squared-formula-in-lm-in-r-and-how-should-it-be-interpret" target="_blank">Stackexchange</a>)



- **Contrasts**
  - **Concept**
    - Contrasts are operationalised as a linear combination in which each term is the product of a group mean and its weight
    - A specific contrast is defined by the specific weights set for each mean such that it compares a specific pair of means 
  - **Mathematics**
    - $\displaystyle \begin{aligned} \psi =& \sum_{k=1}^{k}{w_{k}\bar{x}_{k}} \\ \psi =& w_{1}\bar{x}_{1} + w_{2}\bar{x}_{2} + w_{3}\bar{x}_{3} + \cdots + w_{k}\bar{x}_{k} \end{aligned}$
      - ***Where***
        - $k$ - The number of levels of the categorical predictor 
        - $w_k$ - Weight for the mean of group $k$
        - $\bar{x}_k$ - The mean of group $k$
      - ***Note***
        - $\displaystyle \sum_{k = 1}^{k}{w_k} = 0$
  - **Example**
    - **A 3-group example**
      - $\psi = w_{1}\bar{x}_{1} + w_{2}\bar{x}_{2} + w_{3}\bar{x}_{3}$
      - **A comparison of group 1 and 3**
        - $\psi = 1\bar{x}_{1} + 0\bar{x}_{2} - 1\bar{x}_{3}$
      - **A comparison of group 2 and 1**
        - $\psi = -1\bar{x}_{1} + 1\bar{x}_{2} + 0\bar{x}_{3}$
  - **Sampling variance of a contrast**
    - **Mathematics**
      - $\displaystyle s_{\psi}^{2} = s_{e}^{2}\sum_{k=1}^{k}{\frac{w_k}{n_k}}$
        - ***Where***
          - $s_{e}^{2}$ - Error variance - The error variance is based on all $k$ groups in the study - This is the error variance across all the groups
  - **Standard error of a contrast**
    - **Mathematics**
      - $\displaystyle s_{\psi} = \sqrt{s_{e}^{2}\sum_{k=1}^{k}{\frac{w_k}{n_k}}}$
  - **Student's $t$ of a contrast**
    - **Mathematics**
      - $\displaystyle t = \frac{\psi}{s_{\psi}}$
      
      
      
- **Multivariate contrasts**
  - **Concept**
    - A comparison between any two group centroids 
    - A specific multivariate contrast is defined by the specific set of weights for the mean centroids 
  - **Mathematics**
    - $\begin{aligned}\displaystyle \boldsymbol{\psi} &= \sum_{k=1}^{k}{w_{k}\mathbf{y}_{k}} \\ &= w_{1}\mathbf{y}_{1} + w_{2}\mathbf{y}_{2} + w_{3}\mathbf{y}_{3} + \cdots + w_{k}\mathbf{y}_{k} \end{aligned}$
      - ***Where***
        - $\boldsymbol{\psi}$ - A column vector representing a contrast
        - $w_k$ - A scalar or weight for group $k$
        - $\mathbf{y}_k$ - Mean centroid of group $k$
  - **Test statistics for multivariate contrasts**
    - **Hotelling's $T^2$**
      - **Mathematics**
        - $\displaystyle T^{2}_{\psi} = \boldsymbol{\psi}' \left( \sum_{k = 1}^{k}{\textbf{S}_e \frac{w_{k}^{2}}{n_k} } \right)^{-1} \boldsymbol{\psi}$
          - ***Where***
            - $\textbf{S}_e$ - Error variance-covariance matrix
            - $w_k$ - Weight of group $k$
            - $n_k$ - Sample size of group $k$
    - **Multivariate $F$**
      - **The multivariate $F$**
        - **Mathematics**
          - $\mathbf{H_{\psi}}\mathbf{E}^{-1}$
      - **The multivariate F test statistic**
        - Wilks lambda
        - Pillai's trace
        - Roy's largest root
        - Hotelling-Lawley

<p style = "margin-bottom: 0px; font-size: 20px; ">**SSCP matrix for contrasts**</p>

- **Hypothesis SSCP for a contrast**
  - **Mathematics**
    - $\displaystyle \begin{aligned}\mathbf{H_{\psi}} &= \frac{1}{\sum_{k=1}^{k}{\frac{w_{k}^{2}}{n_k}}} \left(\boldsymbol{\psi \psi'} \right) \\ &= \begin{bmatrix} SS_{\psi, y_{1}} & CP_{\psi, y_{1}, y_{2}} & CP_{\psi, y_{1}, y_{3}} & \cdots & CP_{\psi, y_{1}, y_{y}} \\ CP_{\psi, y_{2}, y_{1}} & SS_{\psi, y_{2}, y_{2}} & CP_{\psi, y_{2}, y_{3}} & \cdots & CP_{\psi, y_{2}, y_{y}} \\ CP_{\psi, y_{3}, y_{1}} & SS_{\psi, y_{3}, y_{2}} & SS_{\psi, y_{3}} & \cdots & CP_{\psi, y_{3}, y_{y}} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ CP_{\psi, y_{y}y_{1}} & CP_{\psi, y_{y}, y_{2}} & CP_{\psi, y_{y}, y_{3}} & \cdots & SS_{\psi, y_{y}}\end{bmatrix} \end{aligned}$
      - ***Notes***
        - When sample sizes are equal, it is simplified to 
          - $\displaystyle \mathbf{H_{\psi}} = \frac{n}{\sum_{k=1}^{k}{w_{k}^{2}}} \left(\boldsymbol{\psi \psi'} \right)$

<p style = "margin-bottom: 0px; font-size: 20px; ">**Contrasts**</p>

- **Complex contrasts**
  - **Concept**
    - Comparing 2 sets of group means (not just 2 group means)
  - **Mathematics**
    - $\psi_{ab} = w_{a}\left(\sum_{k=1}^{k_a}{\bar{x}_k} \right) - w_{b}\left(\sum_{k=1}^{k_b}{\bar{x}_k} \right)$
      - ***Where***
        - $\psi_{ab}$ - Contrast for set $a$ set $b$
        - $w_a$ - The weight for the $a$ set of group means 
        - $w_b$ - The weight for the $b$ set of group means
        - $(\sum_{k=1}^{k_a}{\bar{x}_k} \right)$ - The sum of means in the $a$ set
        - $(\sum_{k=1}^{k_b}{\bar{x}_k} \right)$ - The sum of means in the $b$ set
      - ***Notes***
        - **Weights are summed to 0**
          - **Concept**
            - The sum of the weights must be 0
          - **Mathematics**
            - $w_{a} + w_{b} = 0$
          - **Ensuring weights sum to 0**
            - $\displaystyle w_a = \frac{k_b}{k}$
            - $\displaystyle w_b = \frac{k_a}{k}$
              - ***Where***
                - $k_b$ - The number of groups/levels in set $b$
                - $k_a$ - The number of groups/levels in set $a$
                - $k$ - The total number of groups/levels of the categorical predictor variable 
  - **Example**
    - **A 5-group example**
      - **A complex contrast comparing group 1 and 2 with group 3, 4, and 5**
        - $\displaystyle \begin{aligned}\psi_{ab} &= w_{a}\left(\bar{x}_{1} + \bar{x}_{2}\right) - w_{b}\left(\bar{x}_{3} + \bar{x}_{4} + \bar{x}_{5} \right) \\ &= \frac{3}{5}\left(\bar{x}_{1} + \bar{x}_{2}\right) - \frac{2}{5}\left(\bar{x}_{3} + \bar{x}_{4} + \bar{x}_{5} \right) \\ &= \frac{3}{5}\bar{x}_{1} + \frac{3}{5}\bar{x}_{2}- \frac{2}{5}\bar{x}_{3} - \frac{2}{5}\bar{x}_{4} - \frac{2}{5}\bar{x}_{5}  \end{aligned}$
        - 



<p style = "margin-bottom: 0px; font-size: 20px; ">**Discriminant Function Analysis**</p>

- **Concept**
  - The process of finding a variate that can best separate groups (maximising the $F$ ratio for the groups) so that it can best discriminate between groups
  - The groups are transformed in a way that their $F$ ratio are maximised 
  - The first variate is calculated by maximising the $F$ ratio, each of the subsequent variates is calculated after all preceding variates are partialled out. This means that the variates are orthogonal to each other 
- **Variate** 
  - **Concept**
    - A linear combination of outcome variables 
    - A variate is also called a linear discriminant function to reflect its goal of discriminating groups
    - In a discriminant function analysis, different variates are identified and they are defined by the specific set of weights for the outcome variables (outcome variables are combined differently)
    - Variates are latent variables (aka underlying dimensions/constructs of the outcome variables)
    - **Mathematics**
      - $\displaystyle \begin{aligned} V &= \sum_{p=1}^{p}{\beta_{p}y_{p}} \\ &= \beta_{1}y_{1} + \beta_{2}y_{2} + \beta_{3}y_{3} + \cdots + \beta_{p}y_{p}\end{aligned}$
        - ***Where*** 
          - 
- **The eigenvectors and eigenvalues of the $\mathbf{HE}^{-1}$ matrix**
  - **Concept**
    - Each of the eigenvectors of the $\mathbf{HE}^{-1}$ matrix represents a variate and the components in an eigenvector are the set of weights for the outcome variables in the variate and the eigenvalue associated with the eigenvector represents the ratio of variance in the variate explained by the predictor variable(s) to the variance in that variate unexplained by the predictor variable(s)
  - **Mathematics**
    - $\displaystyle \begin{aligned}\lambda_v &= \frac{SS_{M_{v}}}{SS_{R_{v}}} \\ &= \frac{\sum_{k = 1}^{k}{n_k (\bar{x}_{k} - \bar{x}_{grand})}}{\sum_{n=1, k = 1}^{n, k}{(x_{i, k} - \bar{x}_{k})}} \end{aligned}$
  - **Example**
    - $\lambda_1$ is the ratio of the variance on LDF 1 explained by the predictor variable(s) to the variance on LDF 1 not explained by the predictor variable(s)

  
  
- **Factor loading**
  - **Concept**
    - Aka structure r's
    - The relationship between each component of a dimension/construct and the dimension/construct itself

- **Factor loading in discriminant analysis**
  - **Concept**
    - The relationship between each outcome variable in the variate and the variate itself (sometimes called the LDF-Variable correlations)
    - That is how much each outcome variable in the variate contribute to the variate 
  - **Assessing factor loading**
    - **Methods**
      - Total group structure correlations
      - Between-group structure correlation 
      - Within-group structure correlation 
    - **Which is the best one??**
      - Huberty and Olejnik (2006) advocates within-group structure correlation as it takes into account the group differences of mean vectors (which is meaningful)
  - **Interpreting variates**
    - **Label of a variate**
      - The label of a variate should be based on the outcome variables with higher factor loading because these outcome variables are strongly contributing to the variate
    - **Dropping variates**
      - You may choose to drop the variates that are less meaningful (e.g. those with low eigenvalues)
      
      
- **The LDF plot**
  - **Concept**
    - A plot in which each of the axes represents each of the LDFs/variates and the scores represent scores of the original data that are transformed into variate scores (by putting the original scores into the LDFs)
  - **Types of LDF plot**
    - The mean LDF plot
  - **The mean LDF plot**
    - **Description**
      - An LDF plot which plots the group LDF mean centroids (the group mean centroid but transformed into variate score)
      - It can be used to determine the number of LDFs to retain for interpretation 
    - **Constructing the mean LDF plot**
      - For group g on variate v, put the original group means on the discriminant variables into the corresponding places in the vth linear discriminant function, the result of that is the mean for group g on variate v
      


- **Dropping/retaining variates**
  - **Methods for dropping/retaining variates**
    - Proportion of variance based









