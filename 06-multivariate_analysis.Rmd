# Multivariate Analysis 


<p style = "margin-bottom: 0px; font-size: 20px; ">**Multivariate Analysis**</p>

- **Multivariate analysis**
  - **Concept**
    - Any analyses involving more than one outcome variable 
    - The process of estimating the relationship between the linear combination of several outcome variables and one or more predictor variables  

- **Strengths of Multivariate Analysis**
  - A multivariate model allows the relationships between the set of predictor variables and multiple outcome variables to be tested in one test, which can maintain the Type I error rate (as opposed to testing the relationship between the same set of predictor variables and different outcome variables with separate models, which would inflate Type I error rate due to multiple testing)
  - Takes into account the relationship between the outcome variables 
  - More power to detect whether groups differ along a combination of dimensions 
  - MANOVA has greater power than ANOVA to detect effects because it takes account of the correlations between the outcome variables (Huberty & Morris, 1989)
  
  
- **Centroids**
  - **Concept**
    - A centre point in a multidimensional geometric space 
  - **Mean centroid**
    - **Concept**
      - A centre point that represents the mean on multiple dimensions
      - A mean centroid is represented by a column vector 
    - **Mathematics (Matrix)**
      - ${\boldsymbol{\mu}} = \begin{pmatrix} \mu_{1} \\ \mu_{2} \\ \mu_{3} \\ \vdots \\ \mu_{d} \end{pmatrix}$
        - ***Where***
          - $d$ - The total number of outcome variables (variates)
        - 
  
- **Hotelling's $t^2$ (Hotelling, 1931)**
  - **Concept**
    - Multivariate version of the t-statistic
    - A test statistic for the difference between 2 mean centroids (or multivariate means; means on multiple outcome variables)
    - Tests whether 2 means differ on the linear combination of multiple outcome variables 
  - **Mathematics (Matrix)**
    - $t^2 = \frac{n_{1}n_{2}}{n_{1}n_{2}}(\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{'}(\mathbf{S})^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})$
      - ***Where***
        - $\mathbf{S}$ - Variance covariance matrix 
        - $\mathbf{Y_{1}}$ - Mean centroid for group 1
        - $\mathbf{Y_{2}}$ - Mean centroid for group 2
  - **The relationship between Mahalanobis $D^2$ and Hotelling's $t^2$**
    - **Concept**
      - The relationship between Mahalanobis $D^2$ and Hotelling's $t^2$ is like the relationship between Cohen's $d$ and Student's $t$
    - **Mathematics**
      - $t^2 = D^2 (\frac{n_{1}n_{2}}{n_{1}+n_{2}})$ 
  - **Distribution of Hotelling's $t^2$**
    - **Distribution of Hotelling's $t^2$ as itself**
      - **Concept**
        - Hotelling's $t^2$ has a Hotelling's T-squared distribution with degrees of freedom 1 of $p$ and degree of freedom 2 of $n_{1} + n_{2} - 2$
      - **Mathematics**
        - $t^2 \sim T^2(p, n_{1} + n_{2} - 2)$
    - **Distribution of Hotelling's $t^2$ as $F$**
      - **Concept**
        - Hotelling's $T^2$ can be converted into an $F$ statistic that has an $F$ distribution with degrees of freedom 1 of $p$ and degrees of freedom 2 of $n_{1} + n_{2} - p -1$
        - Hence, you can convert Hotelling's $T^2$ into an $F$ statistic and significance test with the $F$ distribution
      - **Mathematics**
        - $F = t^2\times\frac{v-p+1}{p(v)}$
      - **Distribution of $F$**
        - $F = t^2\times\frac{v-p+1}{p(v)} \sim F(p, n_{1} + n_{2} - p -1)$
  - **NHST**
    - **Hypotheses**
      - $H_0: \mathbf{Y_1} = \mathbf{Y_2}$ - The 2 mean centroids are the same - The 2 mean centroids are from the same population
      - $H_1: \mathbf{Y_1} â‰  \mathbf{Y_2}$ - The 2 mean centroids are not the same - The 2 mean centroids are from different populations


<p style = "margin-bottom: 0px; font-size: 20px; ">**Assumptions**</p>

- **Assumptions of MANOVA**
  - Uncorrelated errors 
  - Random sampling 
  - Multivariate normality 
  - Homogeneity of Covariance 
  - 
  
- **Homogeneity of Covariance**
  - **Concept**
    - The variance of each outcome variable and the covariance between any outcome variables is equal between all groups
    - The variance-covariance matrices are identical between groups
  - **Mathematics**
    - $\Sigma_1 = \Sigma_2 = \Sigma_3 = \cdots = \Sigma_k$
      - ***Where***
        - $\Sigma_k$ - The variance Covariance matrix for group $k$
        - $k$ - The total number of groups in the MANOVA
  - **Effects of violation of this assumption**
    - Only under this condition that the sampling distribution of $t^2\times\frac{v-p+1}{p(v)}$ has an $F$ distribution with degrees of freedom 1 of $p$ and degrees of freedom 2 of $n_{1} + n_{2} - p -1$
  - **Testing for Homogeneity of Covariance**
    - **Statistical Tests for Homogeneity of Covariance**
      - Box M test
    - **Box M test**
      - **Concept**
        - A test that test the null hypothesis that multiple covariance matrices are equal 
      - **The Box M statistic**
        - **Mathematics (Huberty & Olejnik, 2006, p.41)**   
         - $M = v_{e}\ln{|\mathbf{S_e}| - \sum_{k=1}^{k}{v_{k}\ln{|\mathbf{S_k}|}}}$
          - ***Where***
            - $k$ - Number of groups
            - $v_k$ - Degrees of freedom for group k (which is $n_k - 1$)
            - $v_e$ - Error degrees of freedom (which is $\sum_{k=1}^{k}{v_k}$)
            - $\mathbf{S_k}$ - Covariance matrix for group $k$
            - $\mathbf{S_e}$ - Error covariance matrix (Which is $\frac{\mathbf{E}}{v_e}$)
      - **Distribution of Box M**
        - **Chi-squared transformed Box M**
          - **Concept**
            - Box M can be transformed to a statistic that has a Chi-Squared distribution with degrees of freedom of $\frac{(k-1)(p+1)p}{2}$ 
          - **Transforming Box M**
            - **Mathematics** 
              - **For equal sample sizes**
                - $M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \right]$
              - **For unequal sample sizes**
                - $M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \times \left( \sum_{k=1}^{k}{v_{k}^{-1} - v_{e}^{-1}} \right) \right]$
          - **Distribution of $M_{\chi}$**
            - $M_{\chi} \sim \chi^2\left(\frac{\left(k-1\right)(p+1)p}{2}\right)$
        - **F-transformed Box M**
          - **Concept**   
            - Box M is transformed to a statistic that has an $F$ distribution
            - Details can be found in Rencher (2002, p.255-259)
          - **Assumptions**
            - Uncorrelated errors 
            - Multivariate normality 

- **Robust multivariate test of difference**
  - **Robust multivariate test of differences**
    - Yao Test
  - **Yao Test**
    - **Concept**
      - A robust Hotelling's $t^2$
    - **The Yao statistic**
      - **Mathematics**
        - $T^{2}_{Yao} = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{'}\left(\frac{\mathbf{S_1}}{n_1}+\frac{\mathbf{S_2}}{n_2}\right)^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})$
    - **Distribution of Yao statistic**
      - **F-transformed Yao**
        - **Concept**
          - $T^{2}_{Yao}$ can be transformed to a statistic that has an F distribution with degrees of freedom 1 of $p$ and degrees of freedom of $f - p + 1$
        - **Mathematics**
          - $ T^{2}_{Yao, F} = T^{2}_{Yao} \times \frac{f-p+1}{pf}$
            - ***Where***
              - $f = \sum_{k=1}^{k}{\frac{1}{n_k - 1}\left(\frac{V_k}{T^{2}_{Yao}}\right)^2}$
                - ***Where***
                  - $V_k = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{'} \mathbf{W}^{-1} \mathbf{W}_k \mathbf{W}^{-1} (\mathbf{Y_{1}-\mathbf{Y_2}})$
                    - ***Where***
                      - $\mathbf{W}= \sum_{k=1}^{k}{\mathbf{W_{k}}}$
                      - $\mathbf{W}_k = \frac{\mathbf{S}_k}{n_k}$
        - **Distribution of $T^{2}_{Yao, F}$**
          - $T^{2}_{Yao, F} \sim F \left( p, f-p+1\right)$






<p style = "margin-bottom: 0px; font-size: 20px; ">**Multivariate ANOVA**</p>

- **MANOVA**
  - **Concept**
    - **MANOVA** - Multivariate Analysis of Variance
    - The comparison of multiple group mean centroids 
    - The comparison of multiple group means on multiple outcome variables 
  - **Hypotheses**
    - **Null Hypothesis**
      - Multiple group mean centroids are identical (or are from the same population)
      - $\begin{alignat*}{1} &H_0:  ~~~~~\boldsymbol{\mu}_{1}& &= ~~~~~\boldsymbol{\mu}_{2}& &= ~~~~~\boldsymbol{\mu}_{3}& &= \cdots& &= ~~~~~\boldsymbol{\mu}_{k}& \\ &H_0: \begin{pmatrix} \mu_{1, 1} \\ \mu_{2, 1} \\ \mu_{3, 1} \\ \vdots \\ \mu_{v, 1}\end{pmatrix}&  &= \begin{pmatrix} \mu_{1, 2} \\ \mu_{2, 2} \\ \mu_{3, 2} \\ \vdots \\ \mu_{v, 2}\end{pmatrix}& &= \begin{pmatrix} \mu_{1, 3} \\ \mu_{2, 3} \\ \mu_{3, 3} \\ \vdots \\ \mu_{v, 3}\end{pmatrix}& &= \cdots& &= \begin{pmatrix} \mu_{1, k} \\ \mu_{2, k} \\ \mu_{3, k} \\ \vdots \\ \mu_{v, k}\end{pmatrix}& \end{alignat*}$
        - ***Where***
          - $\mu_{v, k}$ - Mean of group $k$ on outcome $v$ 
          - $k$ - Group 
          - $v$ - Variate

- **Multivariate F-ratio: The $\mathbf{HE}^{-1}$ matrix**
  - **Concept**
    - A matrix that represents the multivariate version of the F-ratio
    - A matrix that represents the ratio of systematic variance in a set of outcome variables (the variance in the linear combination of multiple outcome variables explained by the model) to the unsystematic variance in the set of outcome variables (the variance in the linear combination of multiple outcome variables not explained by the model)
  - **Eigenvectors of the $\mathbf{HE}^{-1}$ matrix**
    - Each of the Eigenvectors of the $\mathbf{HE}^{-1}$ matrix represents each of the underlying variates and the Eigenvalue of each Eigenvector represents the effect of the associated variate in the form of F-ratio (ratio of variance explained to variance unexplained)
  - **Mathematics (Matrix)**
    - $\mathbf{HE}^{-1}$
      - ***Where***
        - $\mathbf{H}$ - Hypothesis SSCP matrix
        - $\mathbf{E}$ - Error SSCP matrix 
        
  - **Eigenvalues of the $\mathbf{HE}^{-1}$ matrix**
    - Each of the eigenvalues of the $\mathbf{HE}^{-1}$ matrix represents the F-ratio of each of the underlying variates 


- **Test statistic for MANOVA**
  - Wilk Lambda
  - Pillai-Bartlett trace
  - Hotelling-Lawley trace
  - Roy's largest root 
  
- **Wilks lambda (Wilks, 1932)**
  - **Wilks lambda statistic**
    - **Introduction**
      - The oldest and perhaps the most commonly used criterion 
    - **Mathematical description**
      - It is the ratio of the determinant of the Error SSCP matrix to the determinant of the Total SSCP matrix 
      - It represents the ratio of error variance to total variance (or the proportion of unexplained variance)
      - Represents the ratio of the variance in all of the variates not explained by the model to the total variance in all of the variates 
    - **Method 1**
      - **Description**
      - **Mathematics**
        - $\displaystyle \Lambda = \cfrac{|\mathbf{E}|}{|\mathbf{H} + \mathbf{E}|} = \cfrac{|\mathbf{E}|}{|\mathbf{T}|}$
    - **Method 2**
      - **Description**
        - The product of the ratio of variance unexplained to total variance for all variates
      - **Mathematics**
        - $\displaystyle \Lambda = \prod\limits_{v=1}^{v}{\frac{1}{1+\lambda_{v}}}$ 
          - ***Where***
            - $v$ - Variate
            - $\lambda_{v}$ - Eigenvalue for variate $v$
              - ***Note***
                - $\frac{1}{1+\lambda_{v}}$ changes the ratio of variance explained to the variance unexplained for variate $v$ ($\lambda_{v}$) to a ratio of variance unexplained to the total variance for variate $v$
                  - $\begin{aligned} \frac{1}{1+\lambda_{v}} &= \frac{1}{1+\frac{SS_{M,v}}{SS_{R,v}}} \\ &= \frac{1}{\frac{SS_{R,v}}{SS_{R,v}}+\frac{SS_{M,v}}{SS_{R,v}}} \\ &= \frac{SS_{R,v}}{SS_{R,v} + SS_{M,v}} \\ &= \frac{SS_{R,v}}{SS_{T,v}} \end{aligned}$
  - **Distribution of Wilks lambda**
    - **F-transformed Lambda**
      - **Description**
        - Wilks lambda is transformed to a test statistic that has an F distribution
      - **F-transformed Lambda for two groups**
        - $\Lambda_{F} = \cfrac{1-\Lambda}{\Lambda}\times \cfrac{v_{e} - p + 1}{p} \sim F\left( p, v_e - p + 1\right)$
      - **F-transformed Lambda for three groups**
          - $\Lambda_{F} = \cfrac{1-\sqrt{\Lambda}}{\sqrt{\Lambda}} \times \cfrac{v_{e} - p + 1}{p} \sim F\left( 2p, 2v_e - 2p + 2\right)$
          

- **Pillai-Bartlett trace**
  - **Concept** 
    - Aka Pillai's trace
    - Represents the ratio of variance in all the variates explained by the predictor variable to the total variance in all the variates $\left( \frac{SS_M}{SS_T} \right)$
    - It is the sum of the ratio of variance explained to the total variance of each of the variates 
  - **Mathematics**
    - $\displaystyle V = \sum_{v=1}^{v}{\frac{\lambda_{v}}{1 + \lambda_v}}$
      - ***Note***
        - $\frac{\lambda_{v}}{1 + \lambda_v}$ 
          - The squared canonical correlation between the grouping variable and the $v^{th}$ linear discriminant function 
          - This transforms $\lambda_v$ to the ratio of variance explained to the total variance 
            - $\begin{aligned} \frac{\lambda_{v}}{1 + \lambda_v} &= \frac{\frac{SS_M}{SS_R}}{1 + \frac{SS_M}{SS_R}} \\ &= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R}{SS_R} + \frac{SS_M}{SS_R}} \\ &= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R + SS_M}{SS_R}} \\ &= \frac{\frac{SS_M}{SS_R}}{\frac{SS_T}{SS_R}} \\ &= \frac{SS_M}{SS_R} \times \frac{SS_R}{SS_T} \\ &= \frac{SS_M}{SS_T} \end{aligned}$
  - **Distribution of Pillai's trace**
    - **F-transformed Pillai's trace**
      - **Concept**
        - Pillai's trace is transformed to a test statistic that has an $F$ distribution with degrees of freedom 1 of $br$ and degrees of freedom of $r\left( df_e - p + r \right)$
      - **Mathematics**
        - $\displaystyle V_F = \frac{V}{r - V} \times \frac{v_e - p + r}{b}$
          - ***Where***
            - $b = \max{\left( p, df_h \right)}$
            - $r = \min{\left( p, df_h \right)}$
      - **Distribution of F-transformed Pillai's trace**
        - $\displaystyle V_F \sim F \left( br, r\left( df_e - p + r \right)\right)$
        
        
- **Hotelling-Lawley trace**
  - **Concept**
    - It represents the ratio of variance explained to the variance unexplained 
    - It is the sum of the eigenvalues for all the variates 
  - **Mathematics**
    - $\displaystyle T = \sum_{v=1}^{v}{\lambda_{v}}$
  - **F-transformed Hotelling-Lawley trace**
    - **Description**
      - $T$ is transformed to a statistic so that it has a central $F$ distribution 
    - **Mathematics**
      - $\displaystyle T_F = T \times \left[ \frac{r\left( df_e - p - 1\right) + 2}{br^2 } \right] $
    - **Distribution of F-transformed Hotelling-Lawley trace**
      - $\displaystyle T_F \sim F \left( br, r(df_e - p -1) + 2 \right)$
        - ***Where***
          - $b = \max \left(p, df_h \right)$
          - $r = \min \left(p, df_h \right)$

- **Roy's largest Root**
  - **Concept**
    - Roy's largest root suggests only to consider the largest eigenvalue (which is this $\lambda_1$, because the first eigenvalue is the largest one) and ignore the rest (if there are multiple eigenvalues)
    - You can interpret it in anyway you like, for example, you can convert this this eigenvalue to a ratio of variance explained to the total variance using this $\frac{\lambda_1}{1+\lambda_1}$, this point is that you only need to consider the largest eigenvalue and do whatever you want with it
  - **Mathematics**
    - $\Theta = \lambda_1$













































