---
title: "Statistics"
author: "Samuel Mak"
date: "`r Sys.Date()`"

site: bookdown::bookdown_site
output: 
  bookdown::html_book:
    theme: darkly
    highlight: monochrome


---

# The Linear Regression Model


<p style = "margin-bottom: 0px; font-size: 20px; ">**Mathematical model**</p>

- **Description**
    - A model that describes the relationship between variables with a mathematical function/expression



<p style = "margin-bottom: 0px; font-size: 20px; ">**Linear Model**</p>

- **Description**
    - A model that describes the relationship between variables with the linear function
- **Mathematics** 
    - $y = mx + c$
- **Assumption of the linear model**
    - **Linearity** - The relationship between the variables of interest is linear or best described by a linear function
    - **Additivity** - The relationship between the variables of interest is best described as a linear combination
    
    
 
<p style = "margin-bottom: 0px; font-size: 20px; ">**Regression**</p>

- **Concept**
    - The process of estimating the relationship between variables of interest


    
<p style = "margin-bottom: 0px; font-size: 20px; ">**Linear Regression**</p>

- **Concept**
  - The process of estimating the relationship between variables of interest with a linear model (fitting a straight line to the data)
  

<p style = "margin-bottom: 0px; font-size: 20px; ">**Curvilinear Regression**</p>

  - **Concept**
    - The process of estimating the relationship between variables of interest with a curvilinear model (fitting a curve to the data)


<p style = "margin-bottom: 0px; font-size: 20px; ">**Simple Linear Regression**</p>

- **Concept** 
  - Linear regression with one predictor 
- **Mathematics**
  - **Population Model**
    - $y_{i} = \beta_{0} + \beta_{1}X_{i,1} + \epsilon_{i}$
      - **Where**
          - $y_{i}$
            - The value of observation *i*
          - $\beta_{p}$
            - Regression coefficients/parameters
          - $\beta_{0}$ 
            - The intercept
            - The predicted value of the outcome when the value of predictor 1 ($X_{1}$) is 0
          - $\beta_{1}$
            - Regression coefficient/weight for predictor 1
            - Represents the magnitude and direction of the relationship between predictor 1 and the outcome as the change in the value of the outcome variable for every unit change in the value of predictor 1 (it is the gradient/slope of the regression line)
          - $X_{i,1}$ 
            - The value of predictor 1 for observation *i*
          - $\epsilon_{i}$
            - Error for observation *i*
            - Error - The absolute deviation/distance/difference between the observed value and the expected/predicted/fitted value for observation *i*
            - This quantifies the error in prediction at the observation level
  - **Sample Model**
    - $y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i,1} + e_{i}$
- **Mathematics (Matrix)**
  - **Population Model**
    - $\begin{aligned}\mathbf{Y} &= \mathbf{X}\boldsymbol{\beta} + \mathbf{E} \\ \begin{bmatrix} y_{i} \\ y_{i} \\ y_{3} \\ \vdots \\ y_{n}\end{bmatrix} &= \begin{bmatrix} 1 & x_{1} \\ 1 & x_{2} \\ 1 & x_{3} \\ \vdots & \vdots \\ 1 & x_{n}\end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \end{bmatrix} + \begin{bmatrix} \epsilon_{1} \\ \epsilon_{2} \\ \epsilon_{3} \\ \vdots \\ \epsilon_{n}\end{bmatrix}\end{aligned}$
      - **Where**
        - $\mathbf{Y}$ 
          - A column vector that contains the values for the outcome variable for each of the observations
        - $\mathbf{X}$
          - The model matrix/design matrix
          - The model matrix contains 
            - Column vector $\mathbf{U}$
              - A column vector of 1s one for each observation. The column of 1s corresponds to $\beta_{0}$ in the $\boldsymbol{\beta}$ matrix, which means that the intercept is the same for every observation.
            - Column vector of *x* values 
              - A column vector that contains the values of predictor variable 1 for each of the observations
        - $\boldsymbol{\beta}$
          - A column vector that contains the regression coefficient/weight for each of regression parameters
        - $\mathbf{E}$
          - A column vector that contains the error of each of the observations
          
          

<p style = "margin-bottom: 0px; font-size: 20px; ">**Methods of Estimation**</p>

- Ordinary Least Squares (OLS)
- Generalised Least Squares (GLS)
- Maximum Likelihood (ML)


<p style = "margin-bottom: 0px; font-size: 20px; ">**Ordinary Least Squares**</p>

- **Concept**
  - Estimates the regression line by fitting a line that minimises the model sum of squared residuals (the sum of the squared difference/distance/deviation between each of the observed values and the corresponding predicted/fitted values)
  - $\min{\sum_{i=1}^{n}{e^{2}_{i}}} = \min{\sum_{i=1}^{n}({y_{i} - \hat\beta_{0} - \hat\beta_{1}x_{i,1}}})^2$
- **Mathematics**
  - $\hat\beta_{0} = \bar{y} - \hat\beta_{1}\bar{x}$
  - $\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})y_{i}}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}$
      - Note
        - I think these two equations are rather boring, looking at how they are derived is much more interesting
- **Derivation**
  - Start off with the linear model with the outcome variable as a function of the weights
    - $y_i = \hat\beta_0 + \hat\beta_{1}x_{i,1} + e_i$
  - Make the residual as the subject (express the residual as a function of the weights)
    - $e_i = y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1}$
  - Square both sides
    - $e_{i}^{2} = (y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2$
  - Sum both sides 
    - $\begin{aligned}\sum_{i=1}^{n}{e_{i}^{2}} &= \sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2} \\ S(\beta_0,\beta_1) &= \sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2}\end{aligned}$
    - ![](image/ols_1.png){width=30%}
  - Find the values of $\hat\beta_{0}$ and $\hat\beta_{1}$ that minimises $S(\hat\beta_{0}, \hat\beta_{1})$
    - There is a point at which the sum of squared residuals is at the minimum 
    - The $S(\hat\beta_{0}, \hat\beta_{1})$ function is a quadratic and it has a minimum that represents the minimal sum of squared residuals
    - The coordinates of the minimum are the values of $\hat\beta_{0}$ and $\hat\beta_{1}$ that give you the least sum of squared residuals (or minimises $S(\hat\beta_{0}, \hat\beta_{1})$)
    - Hence, find the values of the coordinate of the minimum of the quadratic function
    - To find each of the values of the coordinate of the minimum of the function $S(\hat\beta_{0}, \hat\beta_{1})$, the partial derivatives of the function $S(\hat\beta_{0}, \hat\beta_{1})$ with respect to each of the $\hat\beta$s are set to 0 (because the minimum has a slope of 0)
    - Expression for $\hat\beta_0$
      - $\frac{\partial{S}}{\partial{\hat\beta_{0}}}=-2\sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})}$
    - Expression for $\hat\beta_1$
      - $\frac{\partial{S}}{\partial{\hat\beta_{1}}}=-2\sum_{i=1}^{n}{x_{i,1}(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})}$
    - Solve the above simultaneous equations
    - Solving the above simultaneous equations would result in 
      - $\hat\beta_{0} = \frac{\sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{y_{i}} - \sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{x_{i,1}y_{i}}}{n\sum_{i=1}^{n}{x_{i,1}^2}-(\sum_{i=1}^{n}{x_{i,1}})^2}$
      - $\hat\beta_{1} = \frac{n\sum_{i=1}^{n}{x_{i,1}y_i} - \sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{y_{i}}}{n\sum_{i=1}^{n}{x_{i,1}^2}-(\sum_{i=1}^{n}{x_{i,1}})^2}$
    - Solving the simultaneous equations would also result in 
      - For $\hat\beta_{0}$
        - $\hat\beta_{0} = \bar{y} - \hat\beta_{1}\bar{x}$
      - For $\hat\beta_{1}$
        - $\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})(y_{i} - \bar{y})}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}$
        - $\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})y_{i}}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}$
        - $\hat\beta_{1} = \frac{Cov(x,y)}{s_{x}^{2}}$
        - $\hat\beta_{1} = \frac{S_{xx}}{S_{xx}}$
- **Mathematics (Matrix)**
  - $\boldsymbol{\beta} = (\mathbf{X'X)^{-1}X'Y}$
- **Derivation (Matrix)**
  - $S(\boldsymbol{\hat\beta}) = ||\mathbf{u}||^2 = ||\mathbf{Y-\hat{Y}}||^2 = ||\mathbf{Y-X\boldsymbol{\beta}}||^2$
  - *note* - Matrix derivation not complete
  
- Variance of $\beta$
  - **Variance of $\beta_{0}$**
    - **Mathematics** 
      - $\sigma_{\hat\beta_{0}}^{2} = \frac{\sigma^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}$
      - $\text{SE}_{\hat\beta_{0}} = \sigma_{\hat\beta_{0}} = \sqrt{\frac{\sigma^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}}$

































