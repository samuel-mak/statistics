<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Multivariate Analysis | Statistics</title>
  <meta name="description" content="Chapter 6 Multivariate Analysis | Statistics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Multivariate Analysis | Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Multivariate Analysis | Statistics" />
  
  
  

<meta name="author" content="Samuel Mak" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="derivation.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a >Section</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="2" data-path="generalised-least-squares.html"><a href="generalised-least-squares.html"><i class="fa fa-check"></i><b>2</b> Generalised Least Squares</a></li>
<li class="chapter" data-level="3" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>3</b> Mathematics</a></li>
<li class="chapter" data-level="4" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>4</b> ANOVA</a></li>
<li class="chapter" data-level="5" data-path="derivation.html"><a href="derivation.html"><i class="fa fa-check"></i><b>5</b> Derivation</a></li>
<li class="chapter" data-level="6" data-path="multivariate-analysis.html"><a href="multivariate-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Analysis</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-analysis" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Multivariate Analysis<a href="multivariate-analysis.html#multivariate-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Multivariate Analysis</strong>
</p>
<ul>
<li><strong>Multivariate analysis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Any analyses involving more than one outcome variable</li>
<li>The process of estimating the relationship between the linear combination of several outcome variables and one or more predictor variables</li>
</ul></li>
</ul></li>
<li><strong>Strengths of Multivariate Analysis</strong>
<ul>
<li>A multivariate model allows the relationships between the set of predictor variables and multiple outcome variables to be tested in one test, which can maintain the Type I error rate (as opposed to testing the relationship between the same set of predictor variables and different outcome variables with separate models, which would inflate Type I error rate due to multiple testing)</li>
<li>Takes into account the relationship between the outcome variables</li>
<li>More power to detect whether groups differ along a combination of dimensions</li>
<li>MANOVA has greater power than ANOVA to detect effects because it takes account of the correlations between the outcome variables (Huberty &amp; Morris, 1989)</li>
</ul></li>
<li><strong>Centroids</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A centre point in a multidimensional geometric space</li>
</ul></li>
<li><strong>Mean centroid</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A centre point that represents the mean on multiple dimensions</li>
<li>A mean centroid is represented by a column vector</li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\({\boldsymbol{\mu}} = \begin{pmatrix} \mu_{1} \\ \mu_{2} \\ \mu_{3} \\ \vdots \\ \mu_{d} \end{pmatrix}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(d\)</span> - The total number of outcome variables (variates)</li>
</ul></li>
<li></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hotelling’s <span class="math inline">\(t^2\)</span> (Hotelling, 1931)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Multivariate version of the t-statistic</li>
<li>A test statistic for the difference between 2 mean centroids (or multivariate means; means on multiple outcome variables)</li>
<li>Tests whether 2 means differ on the linear combination of multiple outcome variables</li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(t^2 = \frac{n_{1}n_{2}}{n_{1}n_{2}}(\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{&#39;}(\mathbf{S})^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mathbf{S}\)</span> - Variance covariance matrix</li>
<li><span class="math inline">\(\mathbf{Y_{1}}\)</span> - Mean centroid for group 1</li>
<li><span class="math inline">\(\mathbf{Y_{2}}\)</span> - Mean centroid for group 2</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The relationship between Mahalanobis <span class="math inline">\(D^2\)</span> and Hotelling’s <span class="math inline">\(t^2\)</span></strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The relationship between Mahalanobis <span class="math inline">\(D^2\)</span> and Hotelling’s <span class="math inline">\(t^2\)</span> is like the relationship between Cohen’s <span class="math inline">\(d\)</span> and Student’s <span class="math inline">\(t\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(t^2 = D^2 (\frac{n_{1}n_{2}}{n_{1}+n_{2}})\)</span></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Hotelling’s <span class="math inline">\(t^2\)</span></strong>
<ul>
<li><strong>Distribution of Hotelling’s <span class="math inline">\(t^2\)</span> as itself</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Hotelling’s <span class="math inline">\(t^2\)</span> has a Hotelling’s T-squared distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degree of freedom 2 of <span class="math inline">\(n_{1} + n_{2} - 2\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(t^2 \sim T^2(p, n_{1} + n_{2} - 2)\)</span></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Hotelling’s <span class="math inline">\(t^2\)</span> as <span class="math inline">\(F\)</span></strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Hotelling’s <span class="math inline">\(T^2\)</span> can be converted into an <span class="math inline">\(F\)</span> statistic that has an <span class="math inline">\(F\)</span> distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degrees of freedom 2 of <span class="math inline">\(n_{1} + n_{2} - p -1\)</span></li>
<li>Hence, you can convert Hotelling’s <span class="math inline">\(T^2\)</span> into an <span class="math inline">\(F\)</span> statistic and significance test with the <span class="math inline">\(F\)</span> distribution</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = t^2\times\frac{v-p+1}{p(v)}\)</span></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(F\)</span></strong>
<ul>
<li><span class="math inline">\(F = t^2\times\frac{v-p+1}{p(v)} \sim F(p, n_{1} + n_{2} - p -1)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>NHST</strong>
<ul>
<li><strong>Hypotheses</strong>
<ul>
<li><span class="math inline">\(H_0: \mathbf{Y_1} = \mathbf{Y_2}\)</span> - The 2 mean centroids are the same - The 2 mean centroids are from the same population</li>
<li><span class="math inline">\(H_1: \mathbf{Y_1} ≠ \mathbf{Y_2}\)</span> - The 2 mean centroids are not the same - The 2 mean centroids are from different populations</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Assumptions</strong>
</p>
<ul>
<li><strong>Assumptions of MANOVA</strong>
<ul>
<li>Uncorrelated errors</li>
<li>Random sampling</li>
<li>Multivariate normality</li>
<li>Homogeneity of Covariance</li>
<li></li>
</ul></li>
<li><strong>Homogeneity of Covariance</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The variance of each outcome variable and the covariance between any outcome variables is equal between all groups</li>
<li>The variance-covariance matrices are identical between groups</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\Sigma_1 = \Sigma_2 = \Sigma_3 = \cdots = \Sigma_k\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\Sigma_k\)</span> - The variance Covariance matrix for group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(k\)</span> - The total number of groups in the MANOVA</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Effects of violation of this assumption</strong>
<ul>
<li>Only under this condition that the sampling distribution of <span class="math inline">\(t^2\times\frac{v-p+1}{p(v)}\)</span> has an <span class="math inline">\(F\)</span> distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degrees of freedom 2 of <span class="math inline">\(n_{1} + n_{2} - p -1\)</span></li>
</ul></li>
<li><strong>Testing for Homogeneity of Covariance</strong>
<ul>
<li><strong>Statistical Tests for Homogeneity of Covariance</strong>
<ul>
<li>Box M test</li>
</ul></li>
<li><strong>Box M test</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A test that test the null hypothesis that multiple covariance matrices are equal</li>
</ul></li>
<li><strong>The Box M statistic</strong>
<ul>
<li><strong>Mathematics (Huberty &amp; Olejnik, 2006, p.41)</strong><br />
</li>
<li><span class="math inline">\(M = v_{e}\ln{|\mathbf{S_e}| - \sum_{k=1}^{k}{v_{k}\ln{|\mathbf{S_k}|}}}\)</span></li>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> - Number of groups</li>
<li><span class="math inline">\(v_k\)</span> - Degrees of freedom for group k (which is <span class="math inline">\(n_k - 1\)</span>)</li>
<li><span class="math inline">\(v_e\)</span> - Error degrees of freedom (which is <span class="math inline">\(\sum_{k=1}^{k}{v_k}\)</span>)</li>
<li><span class="math inline">\(\mathbf{S_k}\)</span> - Covariance matrix for group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\mathbf{S_e}\)</span> - Error covariance matrix (Which is <span class="math inline">\(\frac{\mathbf{E}}{v_e}\)</span>)</li>
</ul></li>
</ul></li>
<li><strong>Distribution of Box M</strong>
<ul>
<li><strong>Chi-squared transformed Box M</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Box M can be transformed to a statistic that has a Chi-Squared distribution with degrees of freedom of <span class="math inline">\(\frac{(k-1)(p+1)p}{2}\)</span></li>
</ul></li>
<li><strong>Transforming Box M</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><strong>For equal sample sizes</strong>
<ul>
<li><span class="math inline">\(M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \right]\)</span></li>
</ul></li>
<li><strong>For unequal sample sizes</strong>
<ul>
<li><span class="math inline">\(M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \times \left( \sum_{k=1}^{k}{v_{k}^{-1} - v_{e}^{-1}} \right) \right]\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(M_{\chi}\)</span></strong>
<ul>
<li><span class="math inline">\(M_{\chi} \sim \chi^2\left(\frac{\left(k-1\right)(p+1)p}{2}\right)\)</span></li>
</ul></li>
</ul></li>
<li><strong>F-transformed Box M</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Box M is transformed to a statistic that has an <span class="math inline">\(F\)</span> distribution</li>
<li>Details can be found in Rencher (2002, p.255-259)</li>
</ul></li>
<li><strong>Assumptions</strong>
<ul>
<li>Uncorrelated errors</li>
<li>Multivariate normality</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Robust multivariate test of difference</strong>
<ul>
<li><strong>Robust multivariate test of differences</strong>
<ul>
<li>Yao Test</li>
</ul></li>
<li><strong>Yao Test</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A robust Hotelling’s <span class="math inline">\(t^2\)</span></li>
</ul></li>
<li><strong>The Yao statistic</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(T^{2}_{Yao} = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{&#39;}\left(\frac{\mathbf{S_1}}{n_1}+\frac{\mathbf{S_2}}{n_2}\right)^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})\)</span></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Yao statistic</strong>
<ul>
<li><strong>F-transformed Yao</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(T^{2}_{Yao}\)</span> can be transformed to a statistic that has an F distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degrees of freedom of <span class="math inline">\(f - p + 1\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li>$ T^{2}<em>{Yao, F} = T^{2}</em>{Yao} $
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(f = \sum_{k=1}^{k}{\frac{1}{n_k - 1}\left(\frac{V_k}{T^{2}_{Yao}}\right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(V_k = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{&#39;} \mathbf{W}^{-1} \mathbf{W}_k \mathbf{W}^{-1} (\mathbf{Y_{1}-\mathbf{Y_2}})\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mathbf{W}= \sum_{k=1}^{k}{\mathbf{W_{k}}}\)</span></li>
<li><span class="math inline">\(\mathbf{W}_k = \frac{\mathbf{S}_k}{n_k}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(T^{2}_{Yao, F}\)</span></strong>
<ul>
<li><span class="math inline">\(T^{2}_{Yao, F} \sim F \left( p, f-p+1\right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Multivariate ANOVA</strong>
</p>
<ul>
<li><strong>MANOVA</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><strong>MANOVA</strong> - Multivariate Analysis of Variance</li>
<li>The comparison of multiple group mean centroids</li>
<li>The comparison of multiple group means on multiple outcome variables</li>
</ul></li>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null Hypothesis</strong>
<ul>
<li>Multiple group mean centroids are identical (or are from the same population)</li>
<li><span class="math inline">\(\begin{alignat*}{1} &amp;H_0: ~~~~~\boldsymbol{\mu}_{1}&amp; &amp;= ~~~~~\boldsymbol{\mu}_{2}&amp; &amp;= ~~~~~\boldsymbol{\mu}_{3}&amp; &amp;= \cdots&amp; &amp;= ~~~~~\boldsymbol{\mu}_{k}&amp; \\ &amp;H_0: \begin{pmatrix} \mu_{1, 1} \\ \mu_{2, 1} \\ \mu_{3, 1} \\ \vdots \\ \mu_{v, 1}\end{pmatrix}&amp; &amp;= \begin{pmatrix} \mu_{1, 2} \\ \mu_{2, 2} \\ \mu_{3, 2} \\ \vdots \\ \mu_{v, 2}\end{pmatrix}&amp; &amp;= \begin{pmatrix} \mu_{1, 3} \\ \mu_{2, 3} \\ \mu_{3, 3} \\ \vdots \\ \mu_{v, 3}\end{pmatrix}&amp; &amp;= \cdots&amp; &amp;= \begin{pmatrix} \mu_{1, k} \\ \mu_{2, k} \\ \mu_{3, k} \\ \vdots \\ \mu_{v, k}\end{pmatrix}&amp; \end{alignat*}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mu_{v, k}\)</span> - Mean of group <span class="math inline">\(k\)</span> on outcome <span class="math inline">\(v\)</span></li>
<li><span class="math inline">\(k\)</span> - Group</li>
<li><span class="math inline">\(v\)</span> - Variate</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Multivariate F-ratio: The <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A matrix that represents the multivariate version of the F-ratio</li>
<li>A matrix that represents the ratio of systematic variance in a set of outcome variables (the variance in the linear combination of multiple outcome variables explained by the model) to the unsystematic variance in the set of outcome variables (the variance in the linear combination of multiple outcome variables not explained by the model)</li>
</ul></li>
<li><strong>Eigenvectors of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix</strong>
<ul>
<li>Each of the Eigenvectors of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix represents each of the underlying variates and the Eigenvalue of each Eigenvector represents the effect of the associated variate in the form of F-ratio (ratio of variance explained to variance unexplained)</li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(\mathbf{HE}^{-1}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span> - Hypothesis SSCP matrix</li>
<li><span class="math inline">\(\mathbf{E}\)</span> - Error SSCP matrix</li>
</ul></li>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\textbf{H}\)</span> and <span class="math inline">\(\textbf{E}^{-1}\)</span> are both symmetric, hence, the order in which they are multiplied does not matter, which means that <span class="math inline">\(\mathbf{HE}^{-1} = \mathbf{E}^{-1}\mathbf{H}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Eigenvalues of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix</strong>
<ul>
<li>Each of the eigenvalues of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix represents the F-ratio of each of the underlying variates</li>
</ul></li>
</ul></li>
<li><strong>Test statistic for MANOVA</strong>
<ul>
<li>Wilk Lambda</li>
<li>Pillai-Bartlett trace</li>
<li>Hotelling-Lawley trace</li>
<li>Roy’s largest root</li>
</ul></li>
<li><strong>Wilks lambda (Wilks, 1932)</strong>
<ul>
<li><strong>Wilks lambda statistic</strong>
<ul>
<li><strong>Introduction</strong>
<ul>
<li>The oldest and perhaps the most commonly used criterion</li>
</ul></li>
<li><strong>Mathematical description</strong>
<ul>
<li>It is the ratio of the determinant of the Error SSCP matrix to the determinant of the Total SSCP matrix</li>
<li>It represents the ratio of error variance to total variance (or the proportion of unexplained variance)</li>
<li>Represents the ratio of the variance in all of the outcome variables not explained by the model to the total variance in all of the outcome variables</li>
</ul></li>
<li><strong>Method 1</strong>
<ul>
<li><strong>Description</strong></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \Lambda = \cfrac{|\mathbf{E}|}{|\mathbf{H} + \mathbf{E}|} = \cfrac{|\mathbf{E}|}{|\mathbf{T}|}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Method 2</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The product of the ratio of variance unexplained to total variance for all variates</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \Lambda = \prod\limits_{v=1}^{v}{\frac{1}{1+\lambda_{v}}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(v\)</span> - Variate</li>
<li><span class="math inline">\(\lambda_{v}\)</span> - Eigenvalue for variate <span class="math inline">\(v\)</span>
<ul>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\frac{1}{1+\lambda_{v}}\)</span> changes the ratio of variance explained to the variance unexplained for variate <span class="math inline">\(v\)</span> (<span class="math inline">\(\lambda_{v}\)</span>) to a ratio of variance unexplained to the total variance for variate <span class="math inline">\(v\)</span>
<ul>
<li><span class="math inline">\(\begin{aligned} \frac{1}{1+\lambda_{v}} &amp;= \frac{1}{1+\frac{SS_{M,v}}{SS_{R,v}}} \\ &amp;= \frac{1}{\frac{SS_{R,v}}{SS_{R,v}}+\frac{SS_{M,v}}{SS_{R,v}}} \\ &amp;= \frac{SS_{R,v}}{SS_{R,v} + SS_{M,v}} \\ &amp;= \frac{SS_{R,v}}{SS_{T,v}} \end{aligned}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Wilks lambda</strong>
<ul>
<li><strong>F-transformed Lambda</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Wilks lambda is transformed to a test statistic that has an F distribution</li>
</ul></li>
<li><strong>F-transformed Lambda for two groups</strong>
<ul>
<li><span class="math inline">\(\Lambda_{F} = \cfrac{1-\Lambda}{\Lambda}\times \cfrac{v_{e} - p + 1}{p} \sim F\left( p, v_e - p + 1\right)\)</span></li>
</ul></li>
<li><strong>F-transformed Lambda for three groups</strong>
<ul>
<li><span class="math inline">\(\Lambda_{F} = \cfrac{1-\sqrt{\Lambda}}{\sqrt{\Lambda}} \times \cfrac{v_{e} - p + 1}{p} \sim F\left( 2p, 2v_e - 2p + 2\right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Pillai-Bartlett trace</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka Pillai’s trace</li>
<li>Represents the ratio of variance in all the variates explained by the predictor variable to the total variance in all the variates <span class="math inline">\(\left( \frac{SS_M}{SS_T} \right)\)</span></li>
<li>It is the sum of the ratio of variance explained to the total variance of each of the variates</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle V = \sum_{v=1}^{v}{\frac{\lambda_{v}}{1 + \lambda_v}}\)</span>
<ul>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\frac{\lambda_{v}}{1 + \lambda_v}\)</span>
<ul>
<li>The squared canonical correlation between the grouping variable and the <span class="math inline">\(v^{th}\)</span> linear discriminant function</li>
<li>This transforms <span class="math inline">\(\lambda_v\)</span> to the ratio of variance explained to the total variance
<ul>
<li><span class="math inline">\(\begin{aligned} \frac{\lambda_{v}}{1 + \lambda_v} &amp;= \frac{\frac{SS_M}{SS_R}}{1 + \frac{SS_M}{SS_R}} \\ &amp;= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R}{SS_R} + \frac{SS_M}{SS_R}} \\ &amp;= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R + SS_M}{SS_R}} \\ &amp;= \frac{\frac{SS_M}{SS_R}}{\frac{SS_T}{SS_R}} \\ &amp;= \frac{SS_M}{SS_R} \times \frac{SS_R}{SS_T} \\ &amp;= \frac{SS_M}{SS_T} \end{aligned}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Pillai’s trace</strong>
<ul>
<li><strong>F-transformed Pillai’s trace</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Pillai’s trace is transformed to a test statistic that has an <span class="math inline">\(F\)</span> distribution with degrees of freedom 1 of <span class="math inline">\(br\)</span> and degrees of freedom of <span class="math inline">\(r\left( df_e - p + r \right)\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle V_F = \frac{V}{r - V} \times \frac{v_e - p + r}{b}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(b = \max{\left( p, df_h \right)}\)</span></li>
<li><span class="math inline">\(r = \min{\left( p, df_h \right)}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of F-transformed Pillai’s trace</strong>
<ul>
<li><span class="math inline">\(\displaystyle V_F \sim F \left( br, r\left( df_e - p + r \right)\right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hotelling-Lawley trace</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It represents the ratio of variance explained to the variance unexplained</li>
<li>It is the sum of the eigenvalues for all the variates</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle T = \sum_{v=1}^{v}{\lambda_{v}}\)</span></li>
</ul></li>
<li><strong>F-transformed Hotelling-Lawley trace</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li><span class="math inline">\(T\)</span> is transformed to a statistic so that it has a central <span class="math inline">\(F\)</span> distribution</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li>$T_F = T $</li>
</ul></li>
<li><strong>Distribution of F-transformed Hotelling-Lawley trace</strong>
<ul>
<li><span class="math inline">\(\displaystyle T_F \sim F \left( br, r(df_e - p -1) + 2 \right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(b = \max \left(p, df_h \right)\)</span></li>
<li><span class="math inline">\(r = \min \left(p, df_h \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Roy’s largest Root</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Roy’s largest root suggests only to consider the largest eigenvalue (which is this <span class="math inline">\(\lambda_1\)</span>, because the first eigenvalue is the largest one) and ignore the rest (if there are multiple eigenvalues)</li>
<li>You can interpret it in anyway you like, for example, you can convert this this eigenvalue to a ratio of variance explained to the total variance using this <span class="math inline">\(\frac{\lambda_1}{1+\lambda_1}\)</span>, the point is that you only need to consider the largest eigenvalue and do whatever you want with it</li>
<li>This method tends to be advocated</li>
<li>Because it has the largest ratio of variance explained to variance unexplained, it has the the most power</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\Theta = \lambda_1\)</span></li>
</ul></li>
<li><strong>F-transformed Roy’s largest Root</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Roy’s largest root can be transformed to a statistic that has an <span class="math inline">\(F\)</span> distribution</li>
<li>Roy’s largest root is transformed to this statistic for NHST</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\Theta_F = \Theta \times \frac{N-b-1}{b}\)</span></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(\Theta_F\)</span></strong>
<ul>
<li><span class="math inline">\(\Theta_F \sim F \left( b, N - b - 1\right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(b = \max{\left( p, df_h\right)}\)</span></li>
</ul></li>
<li><strong><em>Notes</em></strong>
<ul>
<li>Rencher (2002) suggest that <span class="math inline">\(df_e\)</span> might be used instead of <span class="math inline">\(N\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Central vs noncentral distribution</strong>
<ul>
<li><strong>Central distribution</strong> - Distributions that assume the null hypothesis is true</li>
<li><strong>Noncentral distribution</strong> - Distributions that assume the alterative hypothesis is true</li>
</ul></li>
<li><strong>MANOVA effect sizes</strong>
<ul>
<li><strong>MANOVA effect sizes</strong>
<ul>
<li>Multivariate Eta-squared</li>
<li>Multivariate Omega-squared</li>
<li>Tau-squared</li>
<li>Xi-squared</li>
<li>Zeta-squared</li>
</ul></li>
<li><strong>Multivariate Eta-squared</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The original Eta-squared generalized to multivariate contexts</li>
<li>Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (proportion of variance explained) (Regardless of the number of constructs underlying the outcome variable)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><strong>Huberty and Olejnik (2006)</strong>
<ul>
<li><span class="math inline">\(\eta^{2}_{Multivariate}= 1 - \Lambda\)</span></li>
</ul></li>
<li><a href="https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize?action=AttachFile&do=view&target=mvwl.doc#:~:text=Attachment%20%27mvwl.doc%27-,Download,-Current%20configuration%20does" target="_blank"><strong>Cambridge</strong></a>
<ul>
<li><span class="math inline">\(\eta^{2}_{Multivariate}= 1 - \Lambda^{\frac{1}{r}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(r = \min\left(k - 1, y\right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> - The number of levels of the factor</li>
<li><span class="math inline">\(y\)</span> - The number of outcome variables</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Multivariate Eta-squared</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The multivariate extension of the univariate Omega-squared</li>
<li>Suggested by Tatsuoka (1970)</li>
<li>Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (regardless of the number of constructs underlying the outcome variables)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\omega^{2}_{multivariate} = 1 - \frac{N\Lambda}{\left( N - df_H - 1 \right) + \Lambda}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Tau-squared (Cramer &amp; Nicewander, 1979)</strong>
<ul>
<li><strong>Concept</strong></li>
<li><strong>Mathematics</strong>
<ul>
<li><strong>Method 1</strong>
<ul>
<li><span class="math inline">\(\displaystyle \tau^2 = 1 - \left( \prod \limits_{v=1}^{v}{1 - \frac{\lambda_v}{1+\lambda_v}} \right)^{\frac{1}{r}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(r = \min{\left(y, df_M \right)}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> - The number of outcome variables</li>
<li><span class="math inline">\(df_M\)</span> - Model degrees of freedom</li>
</ul></li>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\left( \prod \limits_{v=1}^{v}{1 - \frac{\lambda_v}{1+\lambda_v}} \right)^{\frac{1}{r}}\)</span> - This is the geometric mean proportion of variation in all of the variates that is not explained by the model</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Xi squared</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>This is the mean squared canonical correlation (mean proportion of variance in all the variates that is explained by the model; the variance explained per variate; it takes into account the number of variates estimated)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\xi^2 = \frac{V}{s}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(s\)</span> - The number of variates estimated</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Adjustment for positive bias of variance-based effect size</strong>
<ul>
<li><strong>Serlin’s Adjustment (Serlin, 1982)</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li>$ ^2_{adj} = 1- ( 1 - ^2 )$</li>
</ul></li>
<li><strong>Evaluation</strong>
<ul>
<li>Kim and Olejnik (2005) provided simulation support for Serlin’s adjustment</li>
<li>It can also be used with <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\(\zeta^2\)</span> but they work best when there are only 2 levels and when <span class="math inline">\(df_M ≥ 2\)</span> if the sample is large</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hotelling-Lawley Effect size (Zeta-squared)</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>This effect size measure is associated with the Hotelling-Lawley test statistic</li>
<li>It takes into account the number of variates estimated, it represents the variance explained per variate (the average variance explained by each variate)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \zeta^2 = \frac{V}{s + V}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(s = \min \left( y, df_M\right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> - The number of variates estimated</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Canonical Correlation</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The correlation between the linear combination of predictor variables and the linear combination of outcome variables</li>
<li>In other words, it is the correlation between the set of predictor variables and <span class="math inline">\(v^{th}\)</span> variate</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(r_c = \sqrt{V}\)</span></li>
</ul></li>
</ul></li>
</ul>
<p>ans<span class="math inline">\(adj.r.squared &lt;- 1 - (1 - ans\)</span>r.squared) * ((n -
df.int)/rdf)</p>
<ul>
<li><strong>Positive bias of proportion of variance explained</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The sample proportion of variance explained is a positively-based estimator of the population proportion of variance explained</li>
<li>There are methods for adjusting for this bias</li>
</ul></li>
<li><strong>Moderators of this bias</strong>
<ul>
<li>The degree of positive bias increases as
<ul>
<li>The number of levels of the predictor increases</li>
<li>The number of outcome variables increases</li>
<li>The group sample size decreases</li>
</ul></li>
</ul></li>
<li><strong>Adjustments for <span class="math inline">\(R^2\)</span></strong>
<ul>
<li><strong>The Smith formula (Ezekiel, 1928)</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle R_{adj}^{2} = 1 - \left( 1 - R^2\right) \times \frac{n}{n-p}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(p\)</span> - The number of predictors</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The Wherry formula-1</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle R_{adj}^{2} = 1 - \left( 1 - R^2\right) \times \frac{n-1}{n-p-1}\)</span></li>
</ul></li>
</ul></li>
<li><strong>The Wherry formula-2</strong>
<ul>
<li><strong>Introduction</strong>
<ul>
<li>This is implemented by many statistical packages (e.g. SAS, SPSS, R)</li>
<li><code>lm()</code> uses this</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle R_{adj}^{2} = 1 - \left( 1 - R^2\right) \times \frac{n-1}{n-p}\)</span></li>
</ul></li>
</ul></li>
<li><strong>The Olkin and Pratt formula (Olkin &amp; Pratt, 1958)</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle R_{adj}^{2} = 1 - \frac{\left( N-3 \right) ( 1-R^2 )}{N - p - 1} \times \left[ 1 + \frac{2 (1 - R^2)}{N - p + 1}\right]\)</span></li>
</ul></li>
</ul></li>
<li><strong>The Pratt formula</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle R^2 = 1 - \frac{(N - 3)(1 - R^2)}{N - p - 1} \times \left[ 1 + \frac{2 (1 - R^2)}{N - p - 2.3}\right]\)</span></li>
</ul></li>
</ul></li>
<li><strong>The Claudy formula-3</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle R_{adj}^{2} = 1 - \frac{(N - 4)(1 - R^2)}{N - p - 1} \times \left[ 1 + \frac{2 (1 - R^2)}{N - p + 1}\right]\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Which is the best??</strong>
<ul>
<li>Yin and Fan (2001) conducted a simulation study in which they compared the 6 methods, they found that when the <span class="math inline">\(\frac{N}{p}\)</span> is large, almost all of those 6 methods were unbiased, and the Pratt formula better than all the others especially when the <span class="math inline">\(\frac{N}{p}\)</span> ratio is small. So Pratt’s method is probably the way to go, but most statistical packages don’t use it… (Also see <a href="https://stats.stackexchange.com/questions/48703/what-is-the-adjusted-r-squared-formula-in-lm-in-r-and-how-should-it-be-interpret" target="_blank">Stackexchange</a>)</li>
</ul></li>
</ul></li>
<li><strong>Contrasts</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Contrasts are operationalised as a linear combination in which each term is the product of a group mean and its weight</li>
<li>A specific contrast is defined by the specific weights set for each mean such that it compares a specific pair of means</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{aligned} \psi =&amp; \sum_{k=1}^{k}{w_{k}\bar{x}_{k}} \\ \psi =&amp; w_{1}\bar{x}_{1} + w_{2}\bar{x}_{2} + w_{3}\bar{x}_{3} + \cdots + w_{k}\bar{x}_{k} \end{aligned}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> - The number of levels of the categorical predictor</li>
<li><span class="math inline">\(w_k\)</span> - Weight for the mean of group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\bar{x}_k\)</span> - The mean of group <span class="math inline">\(k\)</span></li>
</ul></li>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\displaystyle \sum_{k = 1}^{k}{w_k} = 0\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Example</strong>
<ul>
<li><strong>A 3-group example</strong>
<ul>
<li><span class="math inline">\(\psi = w_{1}\bar{x}_{1} + w_{2}\bar{x}_{2} + w_{3}\bar{x}_{3}\)</span></li>
<li><strong>A comparison of group 1 and 3</strong>
<ul>
<li><span class="math inline">\(\psi = 1\bar{x}_{1} + 0\bar{x}_{2} - 1\bar{x}_{3}\)</span></li>
</ul></li>
<li><strong>A comparison of group 2 and 1</strong>
<ul>
<li><span class="math inline">\(\psi = -1\bar{x}_{1} + 1\bar{x}_{2} + 0\bar{x}_{3}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Sampling variance of a contrast</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle s_{\psi}^{2} = s_{e}^{2}\sum_{k=1}^{k}{\frac{w_k}{n_k}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(s_{e}^{2}\)</span> - Error variance - The error variance is based on all <span class="math inline">\(k\)</span> groups in the study - This is the error variance across all the groups</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Standard error of a contrast</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle s_{\psi} = \sqrt{s_{e}^{2}\sum_{k=1}^{k}{\frac{w_k}{n_k}}}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Student’s <span class="math inline">\(t\)</span> of a contrast</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle t = \frac{\psi}{s_{\psi}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Multivariate contrasts</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A comparison between any two group centroids</li>
<li>A specific multivariate contrast is defined by the specific set of weights for the mean centroids</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\begin{aligned}\displaystyle \boldsymbol{\psi} &amp;= \sum_{k=1}^{k}{w_{k}\mathbf{y}_{k}} \\ &amp;= w_{1}\mathbf{y}_{1} + w_{2}\mathbf{y}_{2} + w_{3}\mathbf{y}_{3} + \cdots + w_{k}\mathbf{y}_{k} \end{aligned}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\boldsymbol{\psi}\)</span> - A column vector representing a contrast</li>
<li><span class="math inline">\(w_k\)</span> - A scalar or weight for group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\mathbf{y}_k\)</span> - Mean centroid of group <span class="math inline">\(k\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Test statistics for multivariate contrasts</strong>
<ul>
<li><strong>Hotelling’s <span class="math inline">\(T^2\)</span></strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle T^{2}_{\psi} = \boldsymbol{\psi}&#39; \left( \sum_{k = 1}^{k}{\textbf{S}_e \frac{w_{k}^{2}}{n_k} } \right)^{-1} \boldsymbol{\psi}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\textbf{S}_e\)</span> - Error variance-covariance matrix</li>
<li><span class="math inline">\(w_k\)</span> - Weight of group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(n_k\)</span> - Sample size of group <span class="math inline">\(k\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Multivariate <span class="math inline">\(F\)</span></strong>
<ul>
<li><strong>The multivariate <span class="math inline">\(F\)</span></strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\mathbf{H_{\psi}}\mathbf{E}^{-1}\)</span></li>
</ul></li>
</ul></li>
<li><strong>The multivariate F test statistic</strong>
<ul>
<li>Wilks lambda</li>
<li>Pillai’s trace</li>
<li>Roy’s largest root</li>
<li>Hotelling-Lawley</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>SSCP matrix for contrasts</strong>
</p>
<ul>
<li><strong>Hypothesis SSCP for a contrast</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{aligned}\mathbf{H_{\psi}} &amp;= \frac{1}{\sum_{k=1}^{k}{\frac{w_{k}^{2}}{n_k}}} \left(\boldsymbol{\psi \psi&#39;} \right) \\ &amp;= \begin{bmatrix} SS_{\psi, y_{1}} &amp; CP_{\psi, y_{1}, y_{2}} &amp; CP_{\psi, y_{1}, y_{3}} &amp; \cdots &amp; CP_{\psi, y_{1}, y_{y}} \\ CP_{\psi, y_{2}, y_{1}} &amp; SS_{\psi, y_{2}, y_{2}} &amp; CP_{\psi, y_{2}, y_{3}} &amp; \cdots &amp; CP_{\psi, y_{2}, y_{y}} \\ CP_{\psi, y_{3}, y_{1}} &amp; SS_{\psi, y_{3}, y_{2}} &amp; SS_{\psi, y_{3}} &amp; \cdots &amp; CP_{\psi, y_{3}, y_{y}} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ CP_{\psi, y_{y}y_{1}} &amp; CP_{\psi, y_{y}, y_{2}} &amp; CP_{\psi, y_{y}, y_{3}} &amp; \cdots &amp; SS_{\psi, y_{y}}\end{bmatrix} \end{aligned}\)</span>
<ul>
<li><strong><em>Notes</em></strong>
<ul>
<li>When sample sizes are equal, it is simplified to
<ul>
<li><span class="math inline">\(\displaystyle \mathbf{H_{\psi}} = \frac{n}{\sum_{k=1}^{k}{w_{k}^{2}}} \left(\boldsymbol{\psi \psi&#39;} \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Contrasts</strong>
</p>
<ul>
<li><strong>Complex contrasts</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Comparing 2 sets of group means (not just 2 group means)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\psi_{ab} = w_{a}\left(\sum_{k=1}^{k_a}{\bar{x}_k} \right) - w_{b}\left(\sum_{k=1}^{k_b}{\bar{x}_k} \right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\psi_{ab}\)</span> - Contrast for set <span class="math inline">\(a\)</span> set <span class="math inline">\(b\)</span></li>
<li><span class="math inline">\(w_a\)</span> - The weight for the <span class="math inline">\(a\)</span> set of group means</li>
<li><span class="math inline">\(w_b\)</span> - The weight for the <span class="math inline">\(b\)</span> set of group means</li>
<li><span class="math inline">\((\sum_{k=1}^{k_a}{\bar{x}_k} \right)\)</span> - The sum of means in the <span class="math inline">\(a\)</span> set</li>
<li><span class="math inline">\((\sum_{k=1}^{k_b}{\bar{x}_k} \right)\)</span> - The sum of means in the <span class="math inline">\(b\)</span> set</li>
</ul></li>
<li><strong><em>Notes</em></strong>
<ul>
<li><strong>Weights are summed to 0</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The sum of the weights must be 0</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(w_{a} + w_{b} = 0\)</span></li>
</ul></li>
<li><strong>Ensuring weights sum to 0</strong>
<ul>
<li><span class="math inline">\(\displaystyle w_a = \frac{k_b}{k}\)</span></li>
<li><span class="math inline">\(\displaystyle w_b = \frac{k_a}{k}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k_b\)</span> - The number of groups/levels in set <span class="math inline">\(b\)</span></li>
<li><span class="math inline">\(k_a\)</span> - The number of groups/levels in set <span class="math inline">\(a\)</span></li>
<li><span class="math inline">\(k\)</span> - The total number of groups/levels of the categorical predictor variable</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Example</strong>
<ul>
<li><strong>A 5-group example</strong>
<ul>
<li><strong>A complex contrast comparing group 1 and 2 with group 3, 4, and 5</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{aligned}\psi_{ab} &amp;= w_{a}\left(\bar{x}_{1} + \bar{x}_{2}\right) - w_{b}\left(\bar{x}_{3} + \bar{x}_{4} + \bar{x}_{5} \right) \\ &amp;= \frac{3}{5}\left(\bar{x}_{1} + \bar{x}_{2}\right) - \frac{2}{5}\left(\bar{x}_{3} + \bar{x}_{4} + \bar{x}_{5} \right) \\ &amp;= \frac{3}{5}\bar{x}_{1} + \frac{3}{5}\bar{x}_{2}- \frac{2}{5}\bar{x}_{3} - \frac{2}{5}\bar{x}_{4} - \frac{2}{5}\bar{x}_{5} \end{aligned}\)</span></li>
<li></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Discriminant Function Analysis</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of finding a variate that can best separate groups (maximising the <span class="math inline">\(F\)</span> ratio for the groups) so that it can best discriminate between groups</li>
<li>The groups are transformed in a way that their <span class="math inline">\(F\)</span> ratio are maximised</li>
<li>The first variate is calculated by maximising the <span class="math inline">\(F\)</span> ratio, each of the subsequent variates is calculated after all preceding variates are partialled out. This means that the variates are orthogonal to each other</li>
</ul></li>
<li><strong>Variate</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A linear combination of outcome variables</li>
<li>A variate is also called a linear discriminant function to reflect its goal of discriminating groups</li>
<li>In a discriminant function analysis, different variates are identified and they are defined by the specific set of weights for the outcome variables (outcome variables are combined differently)</li>
<li>Variates are latent variables (aka underlying dimensions/constructs of the outcome variables)</li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{aligned} V &amp;= \sum_{p=1}^{p}{\beta_{p}y_{p}} \\ &amp;= \beta_{1}y_{1} + \beta_{2}y_{2} + \beta_{3}y_{3} + \cdots + \beta_{p}y_{p}\end{aligned}\)</span>
<ul>
<li><h2 id="where" class="hasAnchor"><strong><em>Where</em></strong><a href="multivariate-analysis.html#where" class="anchor-section" aria-label="Anchor link to header"></a></h2></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The eigenvectors and eigenvalues of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Each of the eigenvectors of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix represents a variate and the components in an eigenvector are the set of weights for the outcome variables in the variate and the eigenvalue associated with the eigenvector represents the ratio of variance in the variate explained by the predictor variable(s) to the variance in that variate unexplained by the predictor variable(s)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{aligned}\lambda_v &amp;= \frac{SS_{M_{v}}}{SS_{R_{v}}} \\ &amp;= \frac{\sum_{k = 1}^{k}{n_k (\bar{x}_{k} - \bar{x}_{grand})}}{\sum_{n=1, k = 1}^{n, k}{(x_{i, k} - \bar{x}_{k})}} \end{aligned}\)</span></li>
</ul></li>
<li><strong>Example</strong>
<ul>
<li><span class="math inline">\(\lambda_1\)</span> is the ratio of the variance on LDF 1 explained by the predictor variable(s) to the variance on LDF 1 not explained by the predictor variable(s)</li>
</ul></li>
</ul></li>
<li><strong>Factor loading</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka structure r’s</li>
<li>The relationship between each component of a dimension/construct and the dimension/construct itself</li>
</ul></li>
</ul></li>
<li><strong>Factor loading in discriminant analysis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The relationship between each outcome variable in the variate and the variate itself (sometimes called the LDF-Variable correlations)</li>
<li>That is how much each outcome variable in the variate contribute to the variate</li>
</ul></li>
<li><strong>Assessing factor loading</strong>
<ul>
<li><strong>Methods</strong>
<ul>
<li>Total group structure correlations</li>
<li>Between-group structure correlation</li>
<li>Within-group structure correlation</li>
</ul></li>
<li><strong>Which is the best one??</strong>
<ul>
<li>Huberty and Olejnik (2006) advocates within-group structure correlation as it takes into account the group differences of mean vectors (which is meaningful)</li>
</ul></li>
</ul></li>
<li><strong>Interpreting variates</strong>
<ul>
<li><strong>Label of a variate</strong>
<ul>
<li>The label of a variate should be based on the outcome variables with higher factor loading because these outcome variables are strongly contributing to the variate</li>
</ul></li>
<li><strong>Dropping variates</strong>
<ul>
<li>You may choose to drop the variates that are less meaningful (e.g. those with low eigenvalues)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The LDF plot</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A plot in which each of the axes represents each of the LDFs/variates and the scores represent scores of the original data that are transformed into variate scores (by putting the original scores into the LDFs)</li>
</ul></li>
<li><strong>Types of LDF plot</strong>
<ul>
<li>The mean LDF plot</li>
</ul></li>
<li><strong>The mean LDF plot</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>An LDF plot which plots the group LDF mean centroids (the group mean centroid but transformed into variate score)</li>
<li>It can be used to determine the number of LDFs to retain for interpretation</li>
</ul></li>
<li><strong>Constructing the mean LDF plot</strong>
<ul>
<li>For group g on variate v, put the original group means on the discriminant variables into the corresponding places in the vth linear discriminant function, the result of that is the mean for group g on variate v</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Dropping/retaining variates</strong>
<ul>
<li><strong>Methods for dropping/retaining variates</strong>
<ul>
<li>Proportion of variance based</li>
</ul></li>
</ul></li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="derivation.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/samuel-mak/statistics/edit/master/06-multivariate_analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/samuel-mak/statistics/blob/master/06-multivariate_analysis.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub", "bookdownproj.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
