<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Multivariate Analysis | Statistics</title>
  <meta name="description" content="Chapter 6 Multivariate Analysis | Statistics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Multivariate Analysis | Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Multivariate Analysis | Statistics" />
  
  
  

<meta name="author" content="Samuel Mak" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="derivation.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a >Section</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="2" data-path="generalised-least-squares.html"><a href="generalised-least-squares.html"><i class="fa fa-check"></i><b>2</b> Generalised Least Squares</a></li>
<li class="chapter" data-level="3" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>3</b> Mathematics</a></li>
<li class="chapter" data-level="4" data-path="a-very-statistical-day.html"><a href="a-very-statistical-day.html"><i class="fa fa-check"></i><b>4</b> A very statistical day</a></li>
<li class="chapter" data-level="5" data-path="derivation.html"><a href="derivation.html"><i class="fa fa-check"></i><b>5</b> Derivation</a></li>
<li class="chapter" data-level="6" data-path="multivariate-analysis.html"><a href="multivariate-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Analysis</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-analysis" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Multivariate Analysis<a href="multivariate-analysis.html#multivariate-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Multivariate Analysis</strong>
</p>
<ul>
<li><strong>Multivariate analysis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Any analyses involving more than one outcome variable</li>
<li>The process of estimating the relationship between the linear combination of several outcome variables and one or more predictor variables</li>
</ul></li>
</ul></li>
<li><strong>Strengths of Multivariate Analysis</strong>
<ul>
<li>A multivariate model allows the relationships between the set of predictor variables and multiple outcome variables to be tested in one test, which can maintain the Type I error rate (as opposed to testing the relationship between the same set of predictor variables and different outcome variables with separate models, which would inflate Type I error rate due to multiple testing)</li>
<li>Takes into account the relationship between the outcome variables</li>
<li>More power to detect whether groups differ along a combination of dimensions</li>
<li>MANOVA has greater power than ANOVA to detect effects because it takes account of the correlations between the outcome variables (Huberty &amp; Morris, 1989)</li>
</ul></li>
<li><strong>Centroids</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A centre point in a multidimensional geometric space</li>
</ul></li>
<li><strong>Mean centroid</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A centre point that represents the mean on multiple dimensions</li>
<li>A mean centroid is represented by a column vector</li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\({\boldsymbol{\mu}} = \begin{pmatrix} \mu_{1} \\ \mu_{2} \\ \mu_{3} \\ \vdots \\ \mu_{d} \end{pmatrix}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(d\)</span> - The total number of outcome variables (variates)</li>
</ul></li>
<li></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hotelling’s <span class="math inline">\(t^2\)</span> (Hotelling, 1931)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Multivariate version of the t-statistic</li>
<li>A test statistic for the difference between 2 mean centroids (or multivariate means; means on multiple outcome variables)</li>
<li>Tests whether 2 means differ on the linear combination of multiple outcome variables</li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(t^2 = \frac{n_{1}n_{2}}{n_{1}n_{2}}(\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{&#39;}(\mathbf{S})^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mathbf{S}\)</span> - Variance covariance matrix</li>
<li><span class="math inline">\(\mathbf{Y_{1}}\)</span> - Mean centroid for group 1</li>
<li><span class="math inline">\(\mathbf{Y_{2}}\)</span> - Mean centroid for group 2</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The relationship between Mahalanobis <span class="math inline">\(D^2\)</span> and Hotelling’s <span class="math inline">\(t^2\)</span></strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The relationship between Mahalanobis <span class="math inline">\(D^2\)</span> and Hotelling’s <span class="math inline">\(t^2\)</span> is like the relationship between Cohen’s <span class="math inline">\(d\)</span> and Student’s <span class="math inline">\(t\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(t^2 = D^2 (\frac{n_{1}n_{2}}{n_{1}+n_{2}})\)</span></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Hotelling’s <span class="math inline">\(t^2\)</span></strong>
<ul>
<li><strong>Distribution of Hotelling’s <span class="math inline">\(t^2\)</span> as itself</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Hotelling’s <span class="math inline">\(t^2\)</span> has a Hotelling’s T-squared distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degree of freedom 2 of <span class="math inline">\(n_{1} + n_{2} - 2\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(t^2 \sim T^2(p, n_{1} + n_{2} - 2)\)</span></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Hotelling’s <span class="math inline">\(t^2\)</span> as <span class="math inline">\(F\)</span></strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Hotelling’s <span class="math inline">\(T^2\)</span> can be converted into an <span class="math inline">\(F\)</span> statistic that has an <span class="math inline">\(F\)</span> distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degrees of freedom 2 of <span class="math inline">\(n_{1} + n_{2} - p -1\)</span></li>
<li>Hence, you can convert Hotelling’s <span class="math inline">\(T^2\)</span> into an <span class="math inline">\(F\)</span> statistic and significance test with the <span class="math inline">\(F\)</span> distribution</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = t^2\times\frac{v-p+1}{p(v)}\)</span></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(F\)</span></strong>
<ul>
<li><span class="math inline">\(F = t^2\times\frac{v-p+1}{p(v)} \sim F(p, n_{1} + n_{2} - p -1)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>NHST</strong>
<ul>
<li><strong>Hypotheses</strong>
<ul>
<li><span class="math inline">\(H_0: \mathbf{Y_1} = \mathbf{Y_2}\)</span> - The 2 mean centroids are the same - The 2 mean centroids are from the same population</li>
<li><span class="math inline">\(H_1: \mathbf{Y_1} ≠ \mathbf{Y_2}\)</span> - The 2 mean centroids are not the same - The 2 mean centroids are from different populations</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Assumptions</strong>
</p>
<ul>
<li><strong>Assumptions of MANOVA</strong>
<ul>
<li>Uncorrelated errors</li>
<li>Random sampling</li>
<li>Multivariate normality</li>
<li>Homogeneity of Covariance</li>
<li></li>
</ul></li>
<li><strong>Homogeneity of Covariance</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The variance of each outcome variable and the covariance between any outcome variables is equal between all groups</li>
<li>The variance-covariance matrices are identical between groups</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\Sigma_1 = \Sigma_2 = \Sigma_3 = \cdots = \Sigma_k\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\Sigma_k\)</span> - The variance Covariance matrix for group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(k\)</span> - The total number of groups in the MANOVA</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Effects of violation of this assumption</strong>
<ul>
<li>Only under this condition that the sampling distribution of <span class="math inline">\(t^2\times\frac{v-p+1}{p(v)}\)</span> has an <span class="math inline">\(F\)</span> distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degrees of freedom 2 of <span class="math inline">\(n_{1} + n_{2} - p -1\)</span></li>
</ul></li>
<li><strong>Testing for Homogeneity of Covariance</strong>
<ul>
<li><strong>Statistical Tests for Homogeneity of Covariance</strong>
<ul>
<li>Box M test</li>
</ul></li>
<li><strong>Box M test</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A test that test the null hypothesis that multiple covariance matrices are equal</li>
</ul></li>
<li><strong>The Box M statistic</strong>
<ul>
<li><strong>Mathematics (Huberty &amp; Olejnik, 2006, p.41)</strong><br />
</li>
<li><span class="math inline">\(M = v_{e}\ln{|\mathbf{S_e}| - \sum_{k=1}^{k}{v_{k}\ln{|\mathbf{S_k}|}}}\)</span></li>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> - Number of groups</li>
<li><span class="math inline">\(v_k\)</span> - Degrees of freedom for group k (which is <span class="math inline">\(n_k - 1\)</span>)</li>
<li><span class="math inline">\(v_e\)</span> - Error degrees of freedom (which is <span class="math inline">\(\sum_{k=1}^{k}{v_k}\)</span>)</li>
<li><span class="math inline">\(\mathbf{S_k}\)</span> - Covariance matrix for group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\mathbf{S_e}\)</span> - Error covariance matrix (Which is <span class="math inline">\(\frac{\mathbf{E}}{v_e}\)</span>)</li>
</ul></li>
</ul></li>
<li><strong>Distribution of Box M</strong>
<ul>
<li><strong>Chi-squared transformed Box M</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Box M can be transformed to a statistic that has a Chi-Squared distribution with degrees of freedom of <span class="math inline">\(\frac{(k-1)(p+1)p}{2}\)</span></li>
</ul></li>
<li><strong>Transforming Box M</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><strong>For equal sample sizes</strong>
<ul>
<li><span class="math inline">\(M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \right]\)</span></li>
</ul></li>
<li><strong>For unequal sample sizes</strong>
<ul>
<li><span class="math inline">\(M_{\chi} = M \times \left[ 1-\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \times \left( \sum_{k=1}^{k}{v_{k}^{-1} - v_{e}^{-1}} \right) \right]\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(M_{\chi}\)</span></strong>
<ul>
<li><span class="math inline">\(M_{\chi} \sim \chi^2\left(\frac{\left(k-1\right)(p+1)p}{2}\right)\)</span></li>
</ul></li>
</ul></li>
<li><strong>F-transformed Box M</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Box M is transformed to a statistic that has an <span class="math inline">\(F\)</span> distribution</li>
<li>Details can be found in Rencher (2002, p.255-259)</li>
</ul></li>
<li><strong>Assumptions</strong>
<ul>
<li>Uncorrelated errors</li>
<li>Multivariate normality</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Robust multivariate test of difference</strong>
<ul>
<li><strong>Robust multivariate test of differences</strong>
<ul>
<li>Yao Test</li>
</ul></li>
<li><strong>Yao Test</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A robust Hotelling’s <span class="math inline">\(t^2\)</span></li>
</ul></li>
<li><strong>The Yao statistic</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(T^{2}_{Yao} = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{&#39;}\left(\frac{\mathbf{S_1}}{n_1}+\frac{\mathbf{S_2}}{n_2}\right)^{-1}(\mathbf{Y_{1}-\mathbf{Y_2}})\)</span></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Yao statistic</strong>
<ul>
<li><strong>F-transformed Yao</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(T^{2}_{Yao}\)</span> can be transformed to a statistic that has an F distribution with degrees of freedom 1 of <span class="math inline">\(p\)</span> and degrees of freedom of <span class="math inline">\(f - p + 1\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li>$ T^{2}<em>{Yao, F} = T^{2}</em>{Yao} $
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(f = \sum_{k=1}^{k}{\frac{1}{n_k - 1}\left(\frac{V_k}{T^{2}_{Yao}}\right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(V_k = (\mathbf{Y_{1}-\mathbf{Y_2}})\mathbf{&#39;} \mathbf{W}^{-1} \mathbf{W}_k \mathbf{W}^{-1} (\mathbf{Y_{1}-\mathbf{Y_2}})\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mathbf{W}= \sum_{k=1}^{k}{\mathbf{W_{k}}}\)</span></li>
<li><span class="math inline">\(\mathbf{W}_k = \frac{\mathbf{S}_k}{n_k}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(T^{2}_{Yao, F}\)</span></strong>
<ul>
<li><span class="math inline">\(T^{2}_{Yao, F} \sim F \left( p, f-p+1\right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Multivariate ANOVA</strong>
</p>
<ul>
<li><strong>MANOVA</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><strong>MANOVA</strong> - Multivariate Analysis of Variance</li>
<li>The comparison of multiple group mean centroids</li>
<li>The comparison of multiple group means on multiple outcome variables</li>
</ul></li>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null Hypothesis</strong>
<ul>
<li>Multiple group mean centroids are identical (or are from the same population)</li>
<li><span class="math inline">\(\begin{alignat*}{1} &amp;H_0: ~~~~~\boldsymbol{\mu}_{1}&amp; &amp;= ~~~~~\boldsymbol{\mu}_{2}&amp; &amp;= ~~~~~\boldsymbol{\mu}_{3}&amp; &amp;= \cdots&amp; &amp;= ~~~~~\boldsymbol{\mu}_{k}&amp; \\ &amp;H_0: \begin{pmatrix} \mu_{1, 1} \\ \mu_{2, 1} \\ \mu_{3, 1} \\ \vdots \\ \mu_{v, 1}\end{pmatrix}&amp; &amp;= \begin{pmatrix} \mu_{1, 2} \\ \mu_{2, 2} \\ \mu_{3, 2} \\ \vdots \\ \mu_{v, 2}\end{pmatrix}&amp; &amp;= \begin{pmatrix} \mu_{1, 3} \\ \mu_{2, 3} \\ \mu_{3, 3} \\ \vdots \\ \mu_{v, 3}\end{pmatrix}&amp; &amp;= \cdots&amp; &amp;= \begin{pmatrix} \mu_{1, k} \\ \mu_{2, k} \\ \mu_{3, k} \\ \vdots \\ \mu_{v, k}\end{pmatrix}&amp; \end{alignat*}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mu_{v, k}\)</span> - Mean of group <span class="math inline">\(k\)</span> on outcome <span class="math inline">\(v\)</span></li>
<li><span class="math inline">\(k\)</span> - Group</li>
<li><span class="math inline">\(v\)</span> - Variate</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Multivariate F-ratio: The <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A matrix that represents the multivariate version of the F-ratio</li>
<li>A matrix that represents the ratio of systematic variance in a set of outcome variables (the variance in the linear combination of multiple outcome variables explained by the model) to the unsystematic variance in the set of outcome variables (the variance in the linear combination of multiple outcome variables not explained by the model)</li>
</ul></li>
<li><strong>Eigenvectors of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix</strong>
<ul>
<li>Each of the Eigenvectors of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix represents each of the underlying variates and the Eigenvalue of each Eigenvector represents the effect of the associated variate in the form of F-ratio (ratio of variance explained to variance unexplained)</li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(\mathbf{HE}^{-1}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span> - Hypothesis SSCP matrix</li>
<li><span class="math inline">\(\mathbf{E}\)</span> - Error SSCP matrix</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Eigenvalues of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix</strong>
<ul>
<li>Each of the eigenvalues of the <span class="math inline">\(\mathbf{HE}^{-1}\)</span> matrix represents the F-ratio of each of the underlying variates</li>
</ul></li>
</ul></li>
<li><strong>Test statistic for MANOVA</strong>
<ul>
<li>Wilk Lambda</li>
<li>Pillai-Bartlett trace</li>
<li>Hotelling-Lawley trace</li>
<li>Roy’s largest root</li>
</ul></li>
<li><strong>Wilks lambda (Wilks, 1932)</strong>
<ul>
<li><strong>Wilks lambda statistic</strong>
<ul>
<li><strong>Introduction</strong>
<ul>
<li>The oldest and perhaps the most commonly used criterion</li>
</ul></li>
<li><strong>Mathematical description</strong>
<ul>
<li>It is the ratio of the determinant of the Error SSCP matrix to the determinant of the Total SSCP matrix</li>
<li>It represents the ratio of error variance to total variance (or the proportion of unexplained variance)</li>
<li>Represents the ratio of the variance in all of the outcome variables not explained by the model to the total variance in all of the outcome variables</li>
</ul></li>
<li><strong>Method 1</strong>
<ul>
<li><strong>Description</strong></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \Lambda = \cfrac{|\mathbf{E}|}{|\mathbf{H} + \mathbf{E}|} = \cfrac{|\mathbf{E}|}{|\mathbf{T}|}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Method 2</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The product of the ratio of variance unexplained to total variance for all variates</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \Lambda = \prod\limits_{v=1}^{v}{\frac{1}{1+\lambda_{v}}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(v\)</span> - Variate</li>
<li><span class="math inline">\(\lambda_{v}\)</span> - Eigenvalue for variate <span class="math inline">\(v\)</span>
<ul>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\frac{1}{1+\lambda_{v}}\)</span> changes the ratio of variance explained to the variance unexplained for variate <span class="math inline">\(v\)</span> (<span class="math inline">\(\lambda_{v}\)</span>) to a ratio of variance unexplained to the total variance for variate <span class="math inline">\(v\)</span>
<ul>
<li><span class="math inline">\(\begin{aligned} \frac{1}{1+\lambda_{v}} &amp;= \frac{1}{1+\frac{SS_{M,v}}{SS_{R,v}}} \\ &amp;= \frac{1}{\frac{SS_{R,v}}{SS_{R,v}}+\frac{SS_{M,v}}{SS_{R,v}}} \\ &amp;= \frac{SS_{R,v}}{SS_{R,v} + SS_{M,v}} \\ &amp;= \frac{SS_{R,v}}{SS_{T,v}} \end{aligned}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Wilks lambda</strong>
<ul>
<li><strong>F-transformed Lambda</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Wilks lambda is transformed to a test statistic that has an F distribution</li>
</ul></li>
<li><strong>F-transformed Lambda for two groups</strong>
<ul>
<li><span class="math inline">\(\Lambda_{F} = \cfrac{1-\Lambda}{\Lambda}\times \cfrac{v_{e} - p + 1}{p} \sim F\left( p, v_e - p + 1\right)\)</span></li>
</ul></li>
<li><strong>F-transformed Lambda for three groups</strong>
<ul>
<li><span class="math inline">\(\Lambda_{F} = \cfrac{1-\sqrt{\Lambda}}{\sqrt{\Lambda}} \times \cfrac{v_{e} - p + 1}{p} \sim F\left( 2p, 2v_e - 2p + 2\right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Pillai-Bartlett trace</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka Pillai’s trace</li>
<li>Represents the ratio of variance in all the variates explained by the predictor variable to the total variance in all the variates <span class="math inline">\(\left( \frac{SS_M}{SS_T} \right)\)</span></li>
<li>It is the sum of the ratio of variance explained to the total variance of each of the variates</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle V = \sum_{v=1}^{v}{\frac{\lambda_{v}}{1 + \lambda_v}}\)</span>
<ul>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\frac{\lambda_{v}}{1 + \lambda_v}\)</span>
<ul>
<li>The squared canonical correlation between the grouping variable and the <span class="math inline">\(v^{th}\)</span> linear discriminant function</li>
<li>This transforms <span class="math inline">\(\lambda_v\)</span> to the ratio of variance explained to the total variance
<ul>
<li><span class="math inline">\(\begin{aligned} \frac{\lambda_{v}}{1 + \lambda_v} &amp;= \frac{\frac{SS_M}{SS_R}}{1 + \frac{SS_M}{SS_R}} \\ &amp;= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R}{SS_R} + \frac{SS_M}{SS_R}} \\ &amp;= \frac{\frac{SS_M}{SS_R}}{\frac{SS_R + SS_M}{SS_R}} \\ &amp;= \frac{\frac{SS_M}{SS_R}}{\frac{SS_T}{SS_R}} \\ &amp;= \frac{SS_M}{SS_R} \times \frac{SS_R}{SS_T} \\ &amp;= \frac{SS_M}{SS_T} \end{aligned}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of Pillai’s trace</strong>
<ul>
<li><strong>F-transformed Pillai’s trace</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Pillai’s trace is transformed to a test statistic that has an <span class="math inline">\(F\)</span> distribution with degrees of freedom 1 of <span class="math inline">\(br\)</span> and degrees of freedom of <span class="math inline">\(r\left( df_e - p + r \right)\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle V_F = \frac{V}{r - V} \times \frac{v_e - p + r}{b}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(b = \max{\left( p, df_h \right)}\)</span></li>
<li><span class="math inline">\(r = \min{\left( p, df_h \right)}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of F-transformed Pillai’s trace</strong>
<ul>
<li><span class="math inline">\(\displaystyle V_F \sim F \left( br, r\left( df_e - p + r \right)\right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hotelling-Lawley trace</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It represents the ratio of variance explained to the variance unexplained</li>
<li>It is the sum of the eigenvalues for all the variates</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle T = \sum_{v=1}^{v}{\lambda_{v}}\)</span></li>
</ul></li>
<li><strong>F-transformed Hotelling-Lawley trace</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li><span class="math inline">\(T\)</span> is transformed to a statistic so that it has a central <span class="math inline">\(F\)</span> distribution</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li>$T_F = T $</li>
</ul></li>
<li><strong>Distribution of F-transformed Hotelling-Lawley trace</strong>
<ul>
<li><span class="math inline">\(\displaystyle T_F \sim F \left( br, r(df_e - p -1) + 2 \right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(b = \max \left(p, df_h \right)\)</span></li>
<li><span class="math inline">\(r = \min \left(p, df_h \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Roy’s largest Root</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Roy’s largest root suggests only to consider the largest eigenvalue (which is this <span class="math inline">\(\lambda_1\)</span>, because the first eigenvalue is the largest one) and ignore the rest (if there are multiple eigenvalues)</li>
<li>You can interpret it in anyway you like, for example, you can convert this this eigenvalue to a ratio of variance explained to the total variance using this <span class="math inline">\(\frac{\lambda_1}{1+\lambda_1}\)</span>, the point is that you only need to consider the largest eigenvalue and do whatever you want with it</li>
<li>This method tends to be advocated</li>
<li>Because it has the largest ratio of variance explained to variance unexplained, it has the the most power</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\Theta = \lambda_1\)</span></li>
</ul></li>
<li><strong>F-transformed Roy’s largest Root</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Roy’s largest root can be transformed to a statistic that has an <span class="math inline">\(F\)</span> distribution</li>
<li>Roy’s largest root is transformed to this statistic for NHST</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\Theta_F = \Theta \times \frac{N-b-1}{b}\)</span></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(\Theta_F\)</span></strong>
<ul>
<li><span class="math inline">\(\Theta_F \sim F \left( b, N - b - 1\right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(b = \max{\left( p, df_h\right)}\)</span></li>
</ul></li>
<li><strong><em>Notes</em></strong>
<ul>
<li>Rencher (2002) suggest that <span class="math inline">\(df_e\)</span> might be used instead of <span class="math inline">\(N\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Central vs noncentral distribution</strong>
<ul>
<li><strong>Central distribution</strong> - Distributions that assume the null hypothesis is true</li>
<li><strong>Noncentral distribution</strong> - Distributions that assume the alterative hypothesis is true</li>
</ul></li>
<li><strong>MANOVA effect sizes</strong>
<ul>
<li><strong>MANOVA effect sizes</strong>
<ul>
<li>Multivariate Eta-squared</li>
<li>Multivariate Omega-squared</li>
<li>Tau-squared</li>
<li>Xi-squared</li>
<li>Zeta-squared</li>
</ul></li>
<li><strong>Multivariate Eta-squared</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The original Eta-squared generalized to multivariate contexts</li>
<li>Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (proportion of variance explained) (Regardless of the number of constructs underlying the outcome variable)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><strong>Huberty and Olejnik (2006)</strong>
<ul>
<li><span class="math inline">\(\eta^{2}_{Multivariate}= 1 - \Lambda\)</span></li>
</ul></li>
<li><strong><a href="https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize?action=AttachFile&amp;do=view&amp;target=mvwl.doc#:~:text=Attachment%20%27mvwl.doc%27-,Download,-Current%20configuration%20does">Cambridge</a></strong></li>
<li><a href="https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize?action=AttachFile&do=view&target=mvwl.doc#:~:text=Attachment%20%27mvwl.doc%27-,Download,-Current%20configuration%20does" target="_blank">Cambridge</a>
<ul>
<li><span class="math inline">\(\eta^{2}_{Multivariate}= 1 - \Lambda^{\frac{1}{r}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(r = \min\left(k - 1, y\right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> - The number of levels of the factor</li>
<li><span class="math inline">\(y\)</span> - The number of outcome variables</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Multivariate Eta-squared</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The multivariate extension of the univariate Omega-squared</li>
<li>Suggested by Tatsuoka (1970)</li>
<li>Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (regardless of the number of constructs underlying the outcome variables)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\omega^{2}_{multivariate} = 1 - \frac{N\Lambda}{\left( N - df_H - 1 \right) + \Lambda}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Tau-squared (Cramer &amp; Nicewander, 1979)</strong>
<ul>
<li><strong>Concept</strong></li>
<li><strong>Mathematics</strong>
<ul>
<li><strong>Method 1</strong>
<ul>
<li><span class="math inline">\(\displaystyle \tau^2 = 1 - \left( \prod \limits_{v=1}^{v}{1 - \frac{\lambda_v}{1+\lambda_v}} \right)^{\frac{1}{r}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(r = \min{\left(y, df_M \right)}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> - The number of outcome variables</li>
<li><span class="math inline">\(df_M\)</span> - Model degrees of freedom</li>
</ul></li>
<li><strong><em>Note</em></strong>
<ul>
<li><span class="math inline">\(\left( \prod \limits_{v=1}^{v}{1 - \frac{\lambda_v}{1+\lambda_v}} \right)^{\frac{1}{r}}\)</span> - This is the geometric mean proportion of variation in all of the variates that is not explained by the model</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Xi squared</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>This is the mean squared canonical correlation (mean proportion of variance in all the variates that is explained by the model; the variance explained per variate; it takes into account the number of variates estimated)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\xi^2 = \frac{V}{s}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(s\)</span> - The number of variates estimated</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Adjustment for positive bias of variance-based effect size</strong>
<ul>
<li><strong>Serlin’s Adjustment (Serlin, 1982)</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li>$ ^2_{adj} = 1- ( 1 - ^2 )$</li>
</ul></li>
<li><strong>Evaluation</strong>
<ul>
<li>Kim and Olejnik (2005) provided simulation support for Serlin’s adjustment</li>
<li>It can also be used with <span class="math inline">\(\tau^2\)</span> and <span class="math inline">\(\zeta^2\)</span> but they work best when there are only 2 levels and when <span class="math inline">\(df_M ≥ 2\)</span> if the sample is large</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hotelling-Lawley Effect size (Zeta-squared)</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>This effect size measure is associated with the Hotelling-Lawley test statistic</li>
<li>It takes into account the number of variates estimated, it represents the variance explained per variate (the average variance explained by each variate)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \zeta^2 = \frac{V}{s + V}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(s = \min \left( y, df_M\right)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> - The number of variates estimated</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Canonical Correlation</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The correlation between the linear combination of predictor variables and the linear combination of outcome variables</li>
<li>In other words, it is the correlation between the set of predictor variables and <span class="math inline">\(v^{th}\)</span> variate</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(r_c = \sqrt{V}\)</span></li>
</ul></li>
</ul></li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="derivation.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/samuel-mak/statistics/edit/master/06-multivariate_analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/samuel-mak/statistics/blob/master/06-multivariate_analysis.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub", "bookdownproj.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
