[["index.html", "Statistics Chapter 1 The Linear Regression Model", " Statistics Samuel Mak Last Updated: 25-Sep-2022 | 21:26:08 Chapter 1 The Linear Regression Model Mathematical model Description A model that describes the relationship between variables with a mathematical function/expression Linear Model Description A model that describes the relationship between variables with the linear function Mathematics \\(y = mx + c\\) Assumption of the linear model Linearity - The relationship between the variables of interest is linear or best described by a linear function Additivity - The relationship between the variables of interest is best described as a linear combination Regression Concept The process of estimating the relationship between variables of interest Linear Regression Concept The process of estimating the relationship between variables of interest with a linear model (fitting a straight line to the data) Curvilinear Regression Concept The process of estimating the relationship between variables of interest with a curvilinear model (fitting a curve to the data) Simple Linear Regression Concept Linear regression with one predictor Mathematics Population Model \\(y_{i} = \\beta_{0} + \\beta_{1}X_{i,1} + \\epsilon_{i}\\) Where \\(y_{i}\\) The value of observation i \\(\\beta_{p}\\) Regression coefficients/parameters \\(\\beta_{0}\\) The intercept The predicted value of the outcome when the value of predictor 1 (\\(X_{1}\\)) is 0 \\(\\beta_{1}\\) Regression coefficient/weight for predictor 1 Represents the magnitude and direction of the relationship between predictor 1 and the outcome as the change in the value of the outcome variable for every unit change in the value of predictor 1 (it is the gradient/slope of the regression line) \\(X_{i,1}\\) The value of predictor 1 for observation i \\(\\epsilon_{i}\\) Error for observation i Error - The absolute deviation/distance/difference between the observed value and the expected/predicted/fitted value for observation i This quantifies the error in prediction at the observation level Sample Model \\(y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i,1} + e_{i}\\) Mathematics (Matrix) Population Model \\(\\begin{aligned}\\mathbf{Y} &amp;= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{E} \\\\ \\begin{bmatrix} y_{i} \\\\ y_{i} \\\\ y_{3} \\\\ \\vdots \\\\ y_{n}\\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ 1 &amp; x_{3} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n}\\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_{1} \\\\ \\epsilon_{2} \\\\ \\epsilon_{3} \\\\ \\vdots \\\\ \\epsilon_{n}\\end{bmatrix}\\end{aligned}\\) Where \\(\\mathbf{Y}\\) A column vector that contains the values for the outcome variable for each of the observations \\(\\mathbf{X}\\) The model matrix/design matrix The model matrix contains Column vector \\(\\mathbf{U}\\) A column vector of 1s one for each observation. The column of 1s corresponds to \\(\\beta_{0}\\) in the \\(\\boldsymbol{\\beta}\\) matrix, which means that the intercept is the same for every observation. Column vector of x values A column vector that contains the values of predictor variable 1 for each of the observations \\(\\boldsymbol{\\beta}\\) A column vector that contains the regression coefficient/weight for each of regression parameters \\(\\mathbf{E}\\) A column vector that contains the error of each of the observations Methods of Estimation Ordinary Least Squares (OLS) Generalised Least Squares (GLS) Maximum Likelihood (ML) Ordinary Least Squares Concept Estimates the regression line by fitting a line that minimises the model sum of squared residuals (the sum of the squared difference/distance/deviation between each of the observed values and the corresponding predicted/fitted values) \\(\\min{\\sum_{i=1}^{n}{e^{2}_{i}}} = \\min{\\sum_{i=1}^{n}({y_{i} - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1}}})^2\\) Mathematics \\(\\hat\\beta_{0} = \\bar{y} - \\hat\\beta_{1}\\bar{x}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})y_{i}}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) Note I think these two equations are rather boring, looking at how they are derived is much more interesting Derivation Start off with the linear model with the outcome variable as a function of the weights \\(y_i = \\hat\\beta_0 + \\hat\\beta_{1}x_{i,1} + e_i\\) Make the residual as the subject (express the residual as a function of the weights) \\(e_i = y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1}\\) Square both sides \\(e_{i}^{2} = (y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2\\) Sum both sides \\(\\begin{aligned}\\sum_{i=1}^{n}{e_{i}^{2}} &amp;= \\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2} \\\\ S(\\beta_0,\\beta_1) &amp;= \\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2}\\end{aligned}\\) Find the values of \\(\\hat\\beta_{0}\\) and \\(\\hat\\beta_{1}\\) that minimises \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) There is a point at which the sum of squared residuals is at the minimum The \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) function is a quadratic and it has a minimum that represents the minimal sum of squared residuals The coordinates of the minimum are the values of \\(\\hat\\beta_{0}\\) and \\(\\hat\\beta_{1}\\) that give you the least sum of squared residuals (or minimises \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\)) Hence, find the values of the coordinate of the minimum of the quadratic function To find each of the values of the coordinate of the minimum of the function \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\), the partial derivatives of the function \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) with respect to each of the \\(\\hat\\beta\\)s are set to 0 (because the minimum has a slope of 0) Expression for \\(\\hat\\beta_0\\) \\(\\frac{\\partial{S}}{\\partial{\\hat\\beta_{0}}}=-2\\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})}\\) Expression for \\(\\hat\\beta_1\\) \\(\\frac{\\partial{S}}{\\partial{\\hat\\beta_{1}}}=-2\\sum_{i=1}^{n}{x_{i,1}(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})}\\) Solve the above simultaneous equations Solving the above simultaneous equations would result in \\(\\hat\\beta_{0} = \\frac{\\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{y_{i}} - \\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{x_{i,1}y_{i}}}{n\\sum_{i=1}^{n}{x_{i,1}^2}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\hat\\beta_{1} = \\frac{n\\sum_{i=1}^{n}{x_{i,1}y_i} - \\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{y_{i}}}{n\\sum_{i=1}^{n}{x_{i,1}^2}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) Solving the simultaneous equations would also result in For \\(\\hat\\beta_{0}\\) \\(\\hat\\beta_{0} = \\bar{y} - \\hat\\beta_{1}\\bar{x}\\) For \\(\\hat\\beta_{1}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})(y_{i} - \\bar{y})}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})y_{i}}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\hat\\beta_{1} = \\frac{Cov(x,y)}{s_{x}^{2}}\\) \\(\\hat\\beta_{1} = \\frac{S_{xx}}{S_{xx}}\\) Mathematics (Matrix) \\(\\boldsymbol{\\beta} = (\\mathbf{X&#39;X)^{-1}X&#39;Y}\\) Derivation (Matrix) \\(S(\\boldsymbol{\\hat\\beta}) = ||\\mathbf{u}||^2 = ||\\mathbf{Y-\\hat{Y}}||^2 = ||\\mathbf{Y-X\\boldsymbol{\\beta}}||^2\\) note - Matrix derivation not complete Variance of \\(\\beta\\) Variance of \\(\\beta\\) Variance of \\(\\beta_{0}\\) Mathematics \\(\\text{Var}(\\hat\\beta_{0}) = \\sigma_{\\hat\\beta_{0}}^{2} = \\frac{\\sigma^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\text{SE}_{\\hat\\beta_{0}} = \\sigma_{\\hat\\beta_{0}} = \\sqrt{\\frac{\\sigma^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}}\\) Estimation from the sample Description \\(\\sigma^{2}\\) is unknown Since \\(\\sigma^{2}\\) is the variance of the population model (the average model sum of squared errors), it is reasonable to use the variance of the sample model (\\(s^2\\)) as the estimator It has been shown that \\(s^2\\) is an unbiased estimator of \\(\\sigma^{2}\\) Mathematics \\(\\hat{\\text{Var}(\\hat\\beta_{0})} = \\hat\\sigma_{\\hat\\beta_{0}}^{2} = \\frac{s^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\hat{\\text{SE}}_{\\hat\\beta_{0}} = \\hat\\sigma_{\\hat\\beta_{0}} = \\sqrt{\\frac{s^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}}\\) Variance of \\(\\beta_{1}\\) Population Mathematics \\(\\sigma_{\\hat\\beta_{1}^{2}} = \\frac{n\\sigma^{2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}- (\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\text{Var}(\\hat\\beta_{1}) = \\sigma_{\\hat\\beta_{1}^{2}} = \\frac{n\\sigma^{2}}{n\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\text{Var}(\\hat\\beta_{1}) = \\sigma_{\\hat\\beta_{1}}^{2} = \\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\text{SE}_{\\hat\\beta_{1}} = \\sigma_{\\hat\\beta_{1}} = \\sqrt{\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}}\\) Estimation from the sample Mathematics \\(\\hat{\\text{Var}(\\hat\\beta_{1})} = \\hat\\sigma_{\\hat\\beta_{1}}^{2} = \\frac{s^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\hat{\\text{SE}}_{\\hat\\beta_{1}} = \\hat\\sigma_{\\hat\\beta_{1}} = \\sqrt{\\frac{s^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}}\\) Assumptions of the Linear Models Linearity Additivity Normality of the sampling distribution Uncorrelated errors Heteroscedasticity Linearity Concept The relationship between each of the predictor variables and the outcome is linear (or best described as linear) This is the most important assumption of linear models because even if all other assumptions are met, the model is invalid because the description of the process you want to model is wrong Treatment Transformation Segmented regression Fit non-linear models Additivity Concept The combined effect of multiple predictors in a multiple regression is best described as a linear combination of the predictors This, along with linearity, is the most important assumptions of linear models Treatment Transformation Segmented regression Fit non-linear models Uncorrelated Errors Concept The errors in the population model are uncorrelated with each other In other words, there is no autocorrelation/serial correlation The distribution of errors at given value of x is uncorrelated with the distribution of errors at another value of x It is important to note that the errors are not individual errors, they are theoretical distribution of errors given x, in other words, for each vale of x, there is a distribution of errors, and the observed residuals given x you have are sampled from the distribution of errors given the same x (see this short description on CrossValidated) Mathematics \\(\\text{Cov}(e_{i}, e_{j}) = 0\\) Mathematics \\(\\boldsymbol\\Phi = \\Sigma_{e_{i}e_{j}} = \\sigma^{2}\\mathbf{I} = \\begin{bmatrix} \\sigma_{1,1}^{2} &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_{2,2}^{2} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3,3}^{2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_{i,j}^{2} \\end{bmatrix}\\) Note All the off-diagonals are 0s, indicating that all errors are uncorrelated with each other The variances are unconstrained, hence, they can vary Effects of correlated errors on OLS estimation Biased SE The standard mathematical expression of the estimate of the variance of the sampling distribution (\\(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\)) is a biased estimate of the true variance of the sampling distribution If errors are positively correlated, the standard mathematical expression of the estimate of the variance of the sampling distribution (\\(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\)) will underestimate the true value (a negative bias), this means that the true sampling variance and SE are larger than estimated, this means that OLS is less efficient/optimal than other estimators, this follows that all statistical inferences involving the SE will be biased (e.g. underestimating the confidence intervals, overestimating the p-value in significance tests, higher Type I error etc.) Assessing Autocorrelation See the autocorrelation section Treatment Autocorrelation consistent standard errors Cluster robust standard errors Use other estimators Normality of the Sampling Distribution Concept The sampling distribution of the sample estimates is normally distributed A normally distributed sampling distribution is desirable because it can be modelled easily with the normal distribution curve and inferential statistics can be easily derived (which may be easier than if it has a non-normal distribution, e.g. Pearson’s r) Effects of violation of the assumption All the standard mathematical expressions and procedures for estimating the inferential statistics and uncertainty of the sample estimate are based on the assumption that the sampling distribution is normal or at least approximately normal. If the true sampling distribution is not normal, then all the inferential statistics and estimates of uncertainty of the sample estimate are incorrect Assessing Normality of the sampling distribution Distribution of model residuals Description Assess whether the model residuals are normally distributed Explanation In general, in a linear combination of variables, if the variables in the linear combination are random, independent, and normally distributed (their sampling distribution), then the linear combination itself is also normally distributed (the sampling distribution of the linear combination is normal) Each of the regression parameters can be expressed as a linear combination \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x} )y_i}}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}} = \\sum_{i=1}^{n}{\\frac{x_i - \\bar{x}}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}}y_i}\\) Where \\(\\frac{x_i - \\bar{x}}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}}\\) is treated as a constant and \\(y_i\\) is treated a variable Hence, for the linear combination (\\(\\hat\\beta_1\\)) to be normally distributed, the random variable \\(y_i\\) has to be independent and normally distributed Since independence and normality of the errors imply independence and normality of the random variable \\(y_i\\), normality of errors can be used to indicate that the linear combination (\\(\\hat\\beta_1\\)) is normally distributed And since we don’t have the errors in hand, we use the model residuals Assessing normality See the ‘Assessing Normality’ section The Central Limit Theorem and the assumption of normality The distribution of the residuals is not important if sample size is “sufficiently” large according to the Central Limit Theorem. (The CLT states that the sampling distribution is approximately normal if sample size is large and can be assumed to be normal if sample size is sufficiently large (n &gt; 30) regardless of the shape of the population distribution) Hence, there is much less to worry about the non-normality of the sampling distribution if the sample size is “sufficiently” large Just some side notes \\(y_i\\) also needs to be normally distributed, if y is not normally distributed, then the scores of y need to be transformed such that it’s sampling distribution is normal or approximately normal (I am still not sure about this part) Treatment Use robust/non-parametric methods Transformation Heteroscedasticity Description Aka - Homogeneity of variance or constant variance The variance of the distribution of errors given x is the identical between values of x in the population model The variance of the distribution of errors given x is a constant - It is a fixed value and does not vary across values of x in the population model Some sources say that the variance of errors given x does not depend on x (e.g. Rice, 2006; Field, 2023) Visualisation Mathematics \\(\\text{Var}(\\epsilon_{i}) = \\sigma^2\\) Mathematics (Matrix) \\(\\boldsymbol\\Phi = E(\\epsilon\\epsilon&#39;|X) = \\Sigma_{\\epsilon\\epsilon} = \\begin{bmatrix} \\sigma^{2} &amp; \\phi_{1,2} &amp; \\phi_{1,3} &amp; \\cdots &amp; \\phi_{1,j} \\\\ \\phi_{2,1} &amp; \\sigma^{2} &amp; \\phi_{2,3} &amp; \\cdots &amp; \\phi_{2,j} \\\\ \\phi_{3,1} &amp; \\phi_{3,2} &amp; \\sigma^{2} &amp; \\cdots &amp; \\phi_{3,j} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\phi_{i,1} &amp; \\phi_{i,2} &amp; \\phi_{i,3} &amp;\\cdots &amp; \\sigma^{2} \\end{bmatrix}\\) Notes The variances in the diagonals are all \\(\\sigma^2\\) The covariances in the off-diagonals can vary (the covariances are not the condition here) Effects of Homoscedasticity Biased estimation of sampling variance The mathematical expression for the estimate of the sampling variance assumes homoscedasticity (see derivation) (or rather, requires homoscedasticity to be unbiased). If there is heteroscedasticity, then the sampling variance estimated using the same standard mathematical expression will be biased, in that the estimated value is not the same as the true value, this in turn means that the parameter estimates are not optimal due to increased sampling variation and there are other estimators that will be mor efficient than OLS, the biased sampling variance estimate will affect the SE estimate, and will affect all statistical inferences using the sampling variance (e.g. confidence intervals, test statistic, p-values etc.) Confidence intervals can be “extremely inaccurate” when ignoring heteroscedasticity (Wilcox, 2010) But this assumption matters only if group sizes are unequal Assessing Homo/Heteroscedasticity Description The assumption applies to the population model, it is best to assess heteroscedasticity in the population through heteroscedasticity in the sample model Ways to assess heteroscedasticity Plots Statistical tests Plots Residual Plots Residuals vs fitted plot Residuals vs fitted plot Description The residuals are plotted against the fitted values A plot that shows the vertical distribution of residuals in the y-axis for each of the fitted values in the x-axis Basically it is the scatter plot but detrended It can be based on raw or standardised scores Visualisation ## Base R model &nbsp plot(model, which = 1) Statistical Tests Statistical Tests Levene’s test Brown-Forsythe test Hartley’s Fmax Levene’s tests Description Tests the null hypothesis that the variances in different groups are equal It is a one-way ANOVA with absolute deviation as the outcome variable and the groups/values of x as the predictor variable It tests whether the absolute deviation varies between groups/values of x Original Levene’s test Mathematics \\(\\textit{W} = \\frac{\\sum_{i=1}^{n}{n_{g}(|\\bar{d}|_{g} - |\\bar{d}|_{G})^2}}{\\sum_{i=1}^{n}{n_{g}(|{d}|_{i} - |\\bar{d}|_{g})^2}} \\times \\frac{n-k}{k-1}\\) Where \\(g\\) - Denotes group \\(G\\) - Denotes grand \\(d_i = y_i - \\bar{y}_{g}\\) - A deviation means the difference between a score and mean of the group in which it belongs Note Levene’s test statistic is expressed as such but it can be interpreted as the F-statistic Brown-Forsythe test Description Similar to Levene’s test but uses the median in the absolute deviation instead (\\(d_i = y_i - \\tilde{y}_{g}\\)) Levene’s test using trimmed mean Description Similar to Levene’s test but uses the trimmed mean instead of the mean Comparing the tests Trimmed mean performed best when the underlying data followed a Cauchy distribution and the median performed best when the data followed a chi-squared distribution with 4 degrees of freedom (sharply skewed distribution) (Brown &amp; Forsythe, 1974; extracted from Wiki) Hartley’s Fmax (Pearson &amp; Harley, 1954) Description It is the ratio of the variances between the group with the biggest variance and the group with the smallest variance It then conducts a null hypothesis significance test for the ratio The Fmax isn’t used very often, so it is harder to find a critical value table Mathematics \\(\\textit{F}_{max} = \\frac{\\max{s_{g}^{2}}}{\\min{s_{g}^{2}}}\\) Where \\(\\max{s_{g}^{2}}\\) - The largest group variance among all the groups \\(\\min{s_{g}^{2}}\\) - The smallest group variance among all the groups Cochrane’s C test Description It tests whether the variance of a particular group is exceptionally larger than the rest of the group (it is an upper tail test) The test statistic is the ratio of the variane of a group of interest and the overall variance (of all of the groups) It then conducts a null hypothesis significance test on the ratio It is used to test for variance outlier Mathematics \\(\\textit{C}_{g} = \\frac{s_{g}^{2}}{\\sum_{i=1}^{n}{s_{g}^{2}}}\\) Where \\(s_{g}^{2}\\) - The variance of a particular group \\(\\sum_{i=1}^{n}{s_{g}^{2}}\\) - The sum of all the variances (the total variance) Null Hypothesis Significance Test Hypothesis \\(\\textit{H}_{0}:\\) All variances are equal \\(\\textit{H}_{1}:\\) At least one group variance is significantly larger than the other variances Don’t use these tests Sample size In large samples, even trivial heteroscedasticity can produce significant Levene’s result In small samples, Levene’s test has inadequate power to detect heteroscedasticity Group size equality Homoscedasticity tests work best when group sample sizes are equal (when heteroscedasticity does not matter as mentioned before) and does not work well when sample sizes are unequal (when heteroscedasticity matters) Adjust for it instead… There are robust methods for heteroscedasticity, hence, no need to drill on testing heteroscedasticity Read Zimmerman (2004) about the problems with heteroscedasticity tests… Remedies Heteroscedasticity-Consistent Standard Errors Weighted Least Squares Heteroscedasticity-Consistent Standard Error Description Aka Heteroscedasticity-Robust Standard Errors Robust Standard Errors Eicker-Huber-White standard errors Huber-White Standard Errors White Standard Errors Sandwich Estimator (because the matrix expression looks like a sandwich) Versions HC1 HC2 HC3 HC4 HC0 Description The first version of the HC standard error Mathematical desciption In the mathematical expression of the estimate of the sampling variance of the beta parameters, the variance of errors given x is not treated as fixed/a constant, rather, it relaxes the assumption of homoscedasticity and treat the variance of errors given x are different at different values of x Mathematics Population Model \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=1}^{n}{(x_i - \\bar{x})^2})^2} \\times \\sum_{i=1}^{n}{(x_i - \\bar{x})^2 \\textit{Var}(\\epsilon_i|X)}\\) Sample Estimatte \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=1}^{n}{(x_i - \\bar{x})^2})^2} \\times \\sum_{i=1}^{n}{(x_i - \\bar{x})^2 e_{i}^{2}}\\) Where \\(e_{i}^{2}\\) - Squared residual for observation i (it is basically the variance of residual given x, but we usually only have 1 observation given x) Mathematics (Matrix) \\(\\boldsymbol{\\hat\\beta_1} = (\\mathbf{X&#39;X})^{-1}(\\sum_{i=1}^{n}{e_{i}^{2}x&#39;_{i}x_{i}})(\\mathbf{X&#39;X})^{-1}\\) Evaluation Simulations show that it is better than the normal estimation when sample size is large but worse when sample size is small Which version is the best? HC3 is better than HC0 and HC2 (Long &amp; Ervin, 2000) HC4 appears to be more robust than HC3 when there are influential cases and non-normal errors (See Hayes and Cai, 2007 for a review) Heteroscedastic-Autocorrelation-consistent variance estimation Description Variance estimation robust to heteroscedasticity and autocorrelation They are extensions of the HC standard error Types Newey and West (1987) Cluster Robust Standard Errors Description Standard Errors estimation that accounts for heteroscedasticity and correlated errors Based on the concept that data points can be clustered in a certain way Mathematics \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{i=1}^{n}{\\textit{Var}(\\epsilon_{i}(x_{i} - \\bar{x}))} + 2\\sum_{j=1}^{n}{\\sum_{i=1}^{n}{(x_{i} - \\bar{x})(x_{j} - \\bar{x})(\\epsilon{}_{i}\\epsilon{}_{j})}}\\) \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{j=1}^{n}{\\sum_{i=1}^{n}{\\textit{Cov}((x_{i} - \\bar{x})\\epsilon{}_{i}, (x_{j} - \\bar{x})\\epsilon{}_{j})}}\\) \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{j=1}^{n}{\\sum_{i=1}^{n}{(x_{i} - \\bar{x})(x_{j} - \\bar{x})(\\epsilon{}_{i}\\epsilon{}_{j})}}\\) \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{j=1}^{n}{\\sum_{i=1}^{n}{(x_{i} - \\bar{x})(x_{j} - \\bar{x})(\\epsilon{}_{i}\\epsilon{}_{j})}}1[A]\\) Where \\(1[A]\\) \\(A\\) is an event in which i and j are in the same cluster \\(1[A]\\) - A indicator function that indicates whether 2 errors belong to the same cluster The indicator function \\(1[A]\\) equals 1 if event A happens (i and j are in the same cluster) The indicator function \\(1[A]\\) equals 0 if event A does not happen (i ad j are not in the same cluster) This results in a block matrix where the diagonal blocks are the error variance-covariance submatrices and the off-diagonal blocks are all 0s (assuming that the clusters are uncorrelated with each other) - For this reason, it is robust for arbitrary within cluster error correlation but not between cluster error correlation Mathematics (Matrix) \\(\\textbf{V} = (\\textbf{X}&#39;\\textbf{X})^{-1} \\textbf{X}&#39;\\boldsymbol\\Phi \\textbf{X}(\\textbf{X}&#39;\\textbf{X})^{-1}\\) Where \\(\\boldsymbol\\Phi\\) - Error Variance-Covariance matrix The Bootstrap (Efron &amp; Tibshirani, 1993) Description Estimates the properties of the sampling distribution empirically from the sample data Autocorrelation Description Aka serial correlation Errors given x are correlated with errors in other values of x Autocorrelation is often described with respect to time because autocorrelation usually occurs in time series data (correlated across time) Reasons for autocorrelation Temporal autocorrelation Description Autocorrelation across time time Examples Repeated measures designs Time series Longitudinal designs Growth models Spatial autocorrelation Description Autocorrelation due to space Examples Clustered data Ignoring covariates If a covariate is not included in the model, the errors will capture the effect of that covariate The error is not random - There are systematic process in the errors that are mistreated as random or ignored Model misspecification E.g. ignoring linearity Measurement error in the independent variable Types of serial correlation First-order serial correlation First-order serial correlation Description The correlation between errors at any one point of x (\\(e_{x}\\)) and errors at the point of x that is 1 unit less (\\(e_{x-1}\\)) Note that t is often used instead of x, but I am keeping things general here Visualisation Positive first-order serial correlation Negative first-order serial correlation Mathematical concept Tests testing for first-order serial correlation usually assess whether errors given \\(x\\) can be predicted by errors a unit of x that is 1 unit smaller (\\(x - 1\\)) Mathematics \\(e_{t} = \\rho{}e_{x-1} + u_x\\) Where \\(\\rho{}\\) - First-order serial correlation coefficient The process for the error term is called the first-order autoregressive process (AR1) Tests for first-order serial correlation Durbin-Watson test Breusch-Godfrey test Durbin-Watson test The Durbin-Watson statistic Mathematics \\(d = \\frac{\\sum_{t=2}^{n}{(e_{t} - e_{t-1})^2}}{\\sum_{t=1}^{n}{e_{t}^{2}}}\\) Interpretation \\(0 ≤ d ≤ 4\\) \\(d = 2\\) - No serial correlation \\(d &lt; 2\\) - Positive serial correlation (\\(d &lt; 1\\) is interpreted as serious positive serial correlation and thus concerning) \\(d &gt; 2\\) - Negative serial correlation Significance testing Statistical significance of the Durbin-Watson statistic us often sort through assessing whether the observed Durbin-Watson statistic is inside or outside the critical boundary in the sampling distribution under the null hypothesis The critical boundary is a function of sample size and the number of variables Breusch-Godfrey test (Breusch &amp; Godfrey, 1978) Concept Aka auxiliary regression (A regression for supplementary purposes) Tests the extent to which the errors given x can be predicted by the predictor in the model and the model error at a value of x that is 1 less, model error at a value of x that is 2 less, model error at a value of x that is 3 less, and so on (these are the lagged variables, and the number of lagged variables is defined by the researcher, but some people may include lagged variables until the errors in that auxiliary regression is no longer autocorrelated) Mathematics \\(e_{t} = \\beta_{0} + \\beta_{1}x_i + \\beta_{2}e_{t-1} + \\beta_{3}e_{t-2} + \\beta_{4}e_{t-3} + \\cdots + \\beta_{p+1}e_{t-p} + u_i\\) Null Hypothesis Significance Test The Breusch-Godfrey test statistic Mathematics (wiki) \\(nR^2\\) Distribution of the Breusch-Godfrey test statistic \\(nR^2 \\sim{} \\chi_{p}^{2}\\) Warning A source (zed statistics) said it is \\((n-p)R^2 \\sim{} \\chi_{p}^{2}\\) But they are very similar, so I think it’s fine Hypotheses \\(H_0:\\) There is no autocorrelation \\(H_1:\\) There is autocorrelation in the lagged variables in the model Assessing Normality Ways to assess normality Plots Statistical Tests Plots Types of plots Frequency Distribution plot (e.g. histogram) Theoretical vs Observed distribution plots Theoretical vs Observed Distribution plots Description A plot with the distributional points of a particular distribution of interest and the distributional points of the data It visualises how well the distributional point of each of the data points fit their corresponding distributional point a particular distribution of interest (e.g. a normal distribution) The corresponding distributional point is the expected value that the observed score should have in a particular distribution of interest Types P-P plot Q-Q plot P-P plot Description A plot with the probability of each of the points of a particular distribution of interest against the probability of each of the data points in the data (relative to the whole data) P-P plot for the normal distribution Concept A plot with the probability of each of the points of a normal distribution against the probability of each of the data points in the data (relative to the whole data) Q-Q plot Description A plot with the quantile or z-score of each of the points of a particular distribution of interest against the quantile or z-score of each of the data points in the data (relative to the whole data) Detrending (Thode, 2002) Concept Detrending - Transforming the plots such that the line showing the theoretical distributional values is horizontal and not diagonal The diagonal arrangement of Q-Q and P-P plots can lead to visual bias, detrending the plots can reduce visual bias Visualisation Q-Q plot for the normal distribution Concept A plot with the quantile or z-score of each of the points of a normal distribution against the quantile or z-score of each of the data points in the data (relative to the whole data) Visualisation # Q-Q plot (general) ggplot2::ggplot(data = iris, aes(sample = Sepal.Length)) + &nbsp&nbsp qqplotr::stat_qq_point(size = 1, alpha = 0.6, detrend = FALSE) + &nbsp&nbsp qqplotr::stat_qq_line(colour = \"#008898\", detrend = FALSE) + &nbsp&nbsp qqplotr::stat_qq_band(fill = \"#008898\", alpha = 0.3, detrend = FALSE) + &nbsp&nbsp labs(x = \"Theoretical Quantile\", y = \"Observed Quantile\") + &nbsp&nbsp theme_minimal() &nbsp # Q-Q plot for Model residual plot(model_object, which = 2) Statistical Tests Statistical Tests Kolmogorov-Smirnov test Shapiro-Wilk test Kolmogorov-Smirnov test Description Tests the null hypothesis that the scores are normally distributed It compares the scores to a normally distributed set of scores with the same mean and standard deviation The problem with statistical tests Statistical significance does not necessarily mean practical importance Statistical significance tells how real an effect is but does not tell the practical importance/size of the effect It doesn’t work when you need it, but works when you don’t need it (that’s what she said) The smaller the sample size, the lower the power of statistical tests, the less likely normality problems are detected by these statistical tests, however, this is when the assumption of normality is more of a concern The larger the sample size, the higher the power of statistical tests, the more likely normality problems are detected by these statistical tests, however, this is when the assumption of normality is less of a concern due to the CLT Robust methods are the way to go Violation of the normality assumption can be adjusted, hence, we don’t have to drill on statistical tests (comparing non-robust result with robust result as a sensitivity analysis may be a better option) Robust Techniques Robust Techniques Trimming Winsorizing The Bootstrap Robust Estimators Robust Estimators M-estimators L-estimators S-estimators R-estimators Extremum estimator (\\(\\hat\\theta\\)) Description Extremum Estimator - A class of estimators in which the parameters are estimated through maximisation or minimisation of a certain obective function that depends on the data The theory of extremum estimators was developed by Amemiya (1985) M-estimators Description M-estimators - Extremum estimators for which the obejct function is a sample average History These are called M-estimators because Maximum likelihood is one of the first of these estimators, hence, Huber (1981, p. 43) called these estimators as M-estimators to denote that these are “maximum likelihood-type” estimators Examples Least squares - Because the estimator is defined as a minimum of the sum of the squared errors/residuals Maximum Likelihood- Because it maximises the likelihood function over the parameter space Trimming Description Deleting scores from the extremes Robust methods tend to use trimming (e.g. trimmed mean, the median, M-estimators, etc.) Trimming can be baesd on percentage or standard deviation of the scores relative to the data Standard deviation-based trimming is a bad idea (Field, 2023) Percentage-based trimming Description Deleting certain percentage of data points from each of the extremes before any statistic is calculated 10% trimming Deleting 10% of the data points from each of the extremes If the sample size is 100, then 10 of the data points at the higher end (because 0% of 100 is 10) and 10 data points at the lower end will be deleted The median is a trimmed mean The median is the mean when all but the middle data point are trimmed A trimmed statistic A statistic that is calculated on the trimmed data (e.g. trimmed mean) Winsorizing Description Arranging the data in ascending order and replacing values outside a threshold boundary (outside of which is considered extremes of the data) woth the value at the threshold boundary Defining the threshold boundary Percentage-based threshold boundary Description The threshold boundary is determined by a percentage of the data Example The data 9, 19, 22, 4, 8, 21, 9, 10, 0, 0, 1, 17, 4, 19, 13, 11, 0, 5, 3, 4 Arrange in ascending order 0, 0, 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 21, 22 Winsorize 5% Winsorization The bottom 5% and upper 5% of the data is replaced 5% of a data with n = 20 is 1 Hence, the bottom first data point is replaced with the value after it, and the upper first data point is replaced with the value before it 0, 0, 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 21, 21 10% Winsorization The bottom 10% and upper 10% of the data is replaced 10% of a data with n = 20 is 2 Hence, the bottom 2 data points are replaced with the value after the second data point, the upper 2 data points are replaced with the value before the boundary 0, 0 , 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 19, 19 20% Winsorization The data points in the bottom 20% and those in the upper 20% of the data is replaced 20% of the data of N = 20 is 4 Hence the bottom 4 data points are replace with the value at the lower boundary, the upper 4 data poitns are replaced with the value at the upper boundaryy 3, 3, 3, 3 , 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 17, 17, 17, 17 Standard-deviation based threshold boundary The threshold boundary is determined by the standard deviation of the data The scores at the are replaced by the unstandardised score that has a standardised score at the boundary (not repeating the actual score at the boundary as in trimming) Theil-Sen Estimator Description Robust regarding outliers among the predictor variables Mathematics \\(\\hat\\beta_{p} = \\tilde{\\beta_{p,ij}}\\) (need to double check) "],["generalised-least-squares.html", "Chapter 2 Generalised Least Squares", " Chapter 2 Generalised Least Squares Slope Estimation Description Mathematics Mathematics (Matrix) \\(\\hat{\\mathbf{\\beta}} = (\\mathbf{X&#39;V^{-1}X})^{-1}\\mathbf{X&#39;V^{-1}Y}\\) Where \\((\\mathbf{X&#39;V^{-1}X})^{-1}\\) - The sampling error variance-covariance "],["mathematics.html", "Chapter 3 Mathematics", " Chapter 3 Mathematics Mapping Concept Mapping is the process of pairing elements from the input set with elements from the output set based on certain criteria Types of mapping One-to-One One element from the input set is mapped onto one element from the output set One input has one output One value of x has one value of y Many-to-One Multiple elements from the input set is mapped onto one element from the output set Multiple inputs have one output Multiple values of x have one value of y One-to-Many The element from the input set is mapped onto multiple elements of the output set One input has multiple outputs The value of x has multiple values of y Many-to-Many Multiple elements of the input set is mapped onto multiple elements of the output set Multiple inputs have multiple outputs Multiple values of x have multiple values of y Testing the type of mapping of a mathematical relational expression Testing for one-to-many The vertical line test Place a vertical line on the graph of the mathematical expression and see if they intersect with each other more than once If they intersect with each other more than once, then the mathematical expression has a one-to-many mapping Testing for many-to-one The horizontal line test Place a horizontal line on the graph of the mathematical expression and see if they intersect with each other more than once If they intersect with each other more once, then the mathematical expression has a many-to-one mapping Testing for many-to-many The horizontal and vertical line test Basically using both the horizontal and vertical line tests Functions Concept A mathematical relational expression that has a one-to-one or many-to-one mapping (that maps one or more values of a set of inputs to a single output) Mathematical relational expressions that have a one-to-many or many-to-many are not functions Components of a function Variables Concept Aka indeterminates Variable - Anything that can vary in quantity A function consists of variables of which the output is a function (in other words, a function consists of variables that varies with the outcome variable) Coefficient Concept Coefficients - Constants that act as multipliers for the variables The intercept Concept The intercept is the coefficient for the variable with an exponent of 0 What the intercept tells us?? In general sense The intercept defines the change on the y-axis from the x-axis in addition to the terms with variables in the function. In terms of graphical transformation, the intercept defines how the graph of the function is shifted on the y-axis from the x-axis In terms of the intercept The intercept defines the y value when x = 0 (or the value of \\(f(0)\\)). In terms of graphical transformation, the intercept defines the value of y in the coordinate of the graph that touches the y-axis Related concepts Domain of a function Range of a function Root(s) of a function Inverse of a function Extrema of a function Domain of a function The set of possible inputs for a function or the the set of possible values for the variables in the function Range of a function The set of possible outputs of a function Root of a function The values of x for which the output is 0 (\\(f(x) = 0\\)) Graphically, it is the x-coordinate of the point(s) of the graph of the function that intersect(s) with the x-axis Intercept of a function The values of y for which the input is 0 (\\(f(0)\\)) Graphically, it is the y-coordinate of the point of the graph of the function that intersects with the y-axis Inverse of a function - A function that is a reversal of another function (it reverses the subject) Extrema of a function Concept The most extreme value of the output of a function (plural is extrema) Types of Extremum Maximum vs Minimum Maximum The highest possible value of the output of a function Minimum The smallest possible value of the output of a function Global vs Local Local extrema The extrema of the function within a certain range of the domain (the variables) Global extrema The extrema of the function on the entire domain of the function Visualsation Some Useful Functions General Exponential Log Sigmoid Reciprocal Polynomial Functions Linear Quadratic Cubic Quartic Quintic Trigonometric Functions Ordinary Trigonometric Functions Sine Cosine Tangent Hyperbolic Functions Hyperbolic Sine Hyperbolic Cosine Hyperbolic Tangent Probability/Distributional Functions (Probability and Cumulative) Probability Density Function Normal Distribution Chi-squared Distribution Log-Normal Distribution Exponential Distribution F Distribution Probability Mass Function Binomial Distribution Poisson Distribution Bernoulli Distribution Rademacher Distribution Polynomial Functions Concept Polynomial functions - Functions that are made up of the sum of one or more algebraic terms each of which is the product of a coefficient and one or more variables each of which has a non-negative integer exponent The Leading Coefficient - The coefficient of the variable term with the highest degree or the largest exponent in a polynomial function - The leading coefficient in a polynomial function must not be 0 Degree of a polynomial The highest possible degree of the polynomial’s monomial (individual terms) with non-zero coefficients (the degree of a term is the exponent of the variable) Types of Polynomial Function Polynomials by degree A constant (degree of 0) Linear (degree of 1) Quadratic (degree of 2) Cubic (degree of 3) Quartic (degree of 4) Quintic (degree of 5) and so on… The Linear Function The Linear Function Concept The linear function describes the output as a linear function of one variable (variable x) Aka a polynomial with a degree of 1 Mathematics \\(f(x) = ax + b\\) The Linear Equation Concept The univariate univariable linear equation is an equation that describes a variable (variable y) as a linear function of another variable (variable x) Mathematics \\(y = ax + b\\) Where \\(y\\) Variable y (A variable represented on the y-axis) This variable is usually the output or the variable of interest, which is also called the outcome variable or the dependent variable (DV) in mathematical modelling \\(x\\) Variable x (A variable represented on the x-axis) This variable is usually the input, which is also called the predictor variable or the independent variable (IV) in mathematical modelling \\(a\\) The coefficient for \\(x\\) It is the weight of the x variable on the y variable It defines the magnitude of the linear relationship x has on y Graphically, it defines the slope of the line \\(b\\) The intercept The Quadratic Function The Quadratic Function Concept The univariable quadratic function describes the output as a quadratic function of one variable A function with a non-zero coefficient for any variables with an exponent of 2 In the factored form, it is the product of 2 linear factors Mathematics \\(f(x) = ax^{2} + bx + c\\) The Quadratic Equation Concept The univariate univariable quadratic describes a variable (variable y) as a quadratic function of one variable (variable x) Mathematics \\(y = ax^{2} + bx + c\\) Where \\(a\\) Coefficient for \\(x^{2}\\) It is the weight of variable x on variable y It defines the magnitude of the quadratic relationship x has on y Graphically, it defines the slope of the quadratic curve \\(b\\) Coefficient for \\(x\\) \\(c\\) Intercept The Quadratic Graph Real life examples Distance Travelled (\\(s = ut + \\frac{1}{2}at^{2}\\)) Yerkes and Dodson Law (performance is a negative quadratic function of arousal) Bridges Sydney Barbour Bridge (Negative quadratic) Clifton Suspension Bridge (Positive quadratic) The Cubic Function The Cubic Function Concept The univariable cubic function describes the output as a cubic function of one variable (variable x) The cubic function has a non-zero coefficient for any variables with an exponent of 3 In the factored form, it is the product of 3 linear factors or the product of a linear factor and a quadratic factor Mathematics \\(f(x)= ax^{3} + bx^{2} + cx + d\\) The Cubic Equation Concept The univariate univariable cubic equation describes a variable (variable y) as a cubic function of one variable (variable x) Mathematics \\(y = ax^{3} + bx^{2} + cx + d\\) Where \\(a\\) - Coefficient for \\(x^{3}\\) \\(b\\) - Coefficient for \\(x^{2}\\) \\(c\\) - Coefficient for \\(x\\) \\(d\\) - Intercept The Cubic Curve The Quartic Function The Quartic Function Concept The univariable univariate quartic function describes the output as a quartic function of one variable (variable x) The leading coefficient must be non-zero, in quartic functions, it means that the coefficient for any variables with an exponent of 4 must be non-zero In the factored form, it can be the product of 4 linear factors 2 quadratic factors 1 linear factor and 1 cubic factor Mathematics \\(f(x)= ax^{4} + bx^{3} + cx^{2} + dx + e\\) The Quartic Equation Concept The univariate univariable quartic equation describes a variable (variable y) as a quartic function of one variable (variable x) Where \\(a\\) - Coefficient of \\(x^4\\) \\(b\\) - Coefficient of \\(x^3\\) \\(c\\) - Coefficient of \\(x^2\\) \\(d\\) - Coefficient of \\(x\\) \\(e\\) - Intercept The Quartic graph The Quintic Function The Quintic Function Concept The univariable univariate quintic function describes the output as a quintic function of one variable (variable x) The leading coefficient must be non-zero, in quintic functions, this means that the coefficient for any variables with an exponent of 5 must be non-zero In the factored form, it can be the product of 5 linear factors 3 linear factors and 1 quadratic factor 2 linear factors and 1 cubic factor 1 linear factor and 2 quadratic factors 1 linear factor and 1 quartic factors 1 quadratic factor and 1 cubic factor Mathematics \\(f(x) = ax^{5} + bx^{4} + cx^{3} + dx^{2} + ex + f\\) The Quintic Equation Concept The univariate univariable quintic equation describes a variable (variable y) as a quintic function of one variable (variable x) Mathematics \\(y = ax^{5} + bx^{4} + cx^{3} + dx^{2} + ex + f\\) Where \\(a\\) - Coefficient of \\(x^5\\) \\(b\\) - Coefficient of \\(x^4\\) \\(c\\) - Coefficient of \\(x^3\\) \\(d\\) - Coefficient of \\(x^2\\) \\(e\\) - Coefficient of \\(x\\) \\(f\\) - Intercept The Quintic Graph Finding the Intercept of the function Finding the Root(s) of a function Finding the slope of the tangent line of a point of a function Methods Drawing a triangle Differentiation Finding the Extremum of a function Transforming functions ## Scale for &#39;y&#39; is already present. Adding another scale for ## &#39;y&#39;, which will replace the existing scale. ## Scale for &#39;x&#39; is already present. Adding another scale for ## &#39;x&#39;, which will replace the existing scale. ## Warning: Removed 1 rows containing missing values (geom_segment). ## Removed 1 rows containing missing values (geom_segment). ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 Vectors Vectors A vector is a quantity that quantifies a movement (in other words, it is a mathematical representation of a movement) A vector has a magnitude and a direction A vector can be represented by a directed line segment Modulus of a vector Concept The magnitude of a vector The modulus of vector \\(\\mathbf{v}\\) is usually denoted as \\(|\\mathbf{v}|\\) The modulus of a vector is the length of that vector Finding the length/modulus of a vector Concept The length of a vector with more than 1 dimension can be found by using Pythagoras’ theorem Examples Finding the length of a 2d vector Given that \\(\\mathbf{v}= \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\) \\(|\\mathbf{v}| = \\sqrt{x^{2} + y^{2}}\\) Finding the length of a 3d vector Given that \\(\\mathbf{v}= \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\\) \\(|\\mathbf{v}| = \\sqrt{x^{2} + y^{2} + z^{2}}\\) Components of a vector Concept The vectors that are parallel to the coordinate axes when the vector is drawn on a Cartesian grid that make up the vector of interest Components of a vector are linearly independent to each other They are basically the \\(x\\), \\(y\\), and \\(z\\) in the vector \\(\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\\) Visualisation Zero vector Concept A vector that has zero magnitude and no direction Denoted as \\(\\mathbf{0}\\) Unit vector Concept A vector that has a magnitude of 1 A unit vector is denoted by a hat on the vector (e.g. \\(\\hat{\\mathbf{v}}\\)) Position vectors Concept Vectors that stem from the origin Vectors that represent the position of a point in relation to the origin \\(O\\) A vector that represent the position of a point \\(P\\) in relation to the origin \\(O\\) would be denoted as \\(\\overrightarrow{OP}\\) ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d Resultant vector - A vector that summaries a set of vectors - It is the vector that is the combined result of multiple vectors - - The triangle law for vector addition - Concept - When 3 vectors are joined with the end of each vector linking with the end of another vector such that a closed triangle is formed, any one of those vectors is the combination of the other 2 vectors - Hence, the resultant vector of 2 vectors that are joined end-to-end (or successively put together) is the vector that would form a triangle when it is linked up with those two vectors - Mathematics - \\(\\overrightarrow{AB} + \\overrightarrow{BC} = \\overrightarrow{AC}\\) - Where - \\(\\overrightarrow{AC}\\) - The resultant vector - Graph - The polygon law of vector addition Concept The resultant vector of 3 or more vectors is a vector that would form a closed polygon when it is linked up with the vectors that have been successively put together You are basically applying the triangle law multiple times in succession Mathematics \\(\\overrightarrow{AB} + \\overrightarrow{BC} + \\overrightarrow{CD}= \\overrightarrow{AD}\\) Visualisation Explanation \\(\\mathbf{r}\\) is the resultant vector of vector \\(\\mathbf{k}\\) and \\(\\mathbf{c}\\) \\(\\mathbf{r} = \\mathbf{k} + \\mathbf{c}\\) \\(\\mathbf{k}\\) in turn is the resultant vector of vector \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) \\(\\mathbf{k} = \\mathbf{a} + \\mathbf{b}\\) Hence \\(\\mathbf{r} = \\mathbf{k} + \\mathbf{c}\\) \\(\\mathbf{r} = (\\mathbf{a} + \\mathbf{b}) + \\mathbf{c}\\) \\(\\mathbf{r} = \\mathbf{a} + \\mathbf{b} + \\mathbf{c}\\) Scalars Scalar Concept Any numbers that are used to multiply another thing (e.g. variables or vectors) It is called scalar because it is used to multiply or “scale’ other things (e.g. variables or vectors) It can also refer to any quantities that only has a magnitude or can be fully described using a magnitude (e.g. speed, mass) Unit vectors parallel to the coordinate axes Concept Unit vectors that are parallel to the coordinate axes \\(\\mathbf{i} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\) is a unit vector that is parallel to the x-axis \\(\\mathbf{j} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) is a unit vector and is parallel to the y-axis \\(\\mathbf{k} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\) is a unit vector and is parallel to the z-axis Hence, a 2-D vector \\(\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\) can be written as \\(x\\mathbf{i} + y\\mathbf{j}\\) and a 3-D vector \\(\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\\) can be written as \\(x\\mathbf{i} + y\\mathbf{j} + z\\mathbf{k}\\) Finding a unit vector parallel to a vector of interest Concept A unit vector parallel to a vector of interest is the vector per the length of the vector Basically, you are scaling that vector such that it has a length of 1 (this is done by standardising the original vector per unit) Mathematics A unit vector parallel to vector \\(\\mathbf{v}\\) is \\(\\frac{\\mathbf{v}}{\\mathbf{|v|}}\\) Example Consider a 2D vector \\(\\mathbf{v} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}\\) The unit vector parallel to vector \\(\\mathbf{v}\\) is \\(\\frac{\\mathbf{v}}{\\mathbf{|v|}} = \\frac{\\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}}{5} = \\begin{pmatrix} \\frac{4}{5} \\\\ \\frac{3}{5} \\end{pmatrix}\\) Distance between 2 points Distance between 2 3-D points Mathematics \\(d_{1,2} = \\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}+(z_{1}-z_{2})^{2}}\\) Explanation Imagine that we need to find the distance between the 2 black dots (or the black vector) To find the distance of the black vector, you need to first find the length of the blue and red vectors Finding the length of the blue vector Since the blue vector is parallel to the y-axis, the length of the blue vector can be found by the difference between the y-coordinates of the black dots \\(length~of~blue~vector = y_{1} - y_{2}\\) Finding the length of the red vector Since the red vector is not parallel to any of the axes, to find the length of the red vector, we need to first find the length of the yellow and green vectors so that the length of the red vector can be found by using the Pythagoras’ theorem Finding the length of the yellow vector Since the yellow vector is parallel to the x-axis, it can be found by the difference between the x-coordinates of the black dots \\(length~of~yellow~vector = x_{1} - x_{2}\\) Finding the length of the green vector Since the green vector is parallel to the z-axis, it can be found by the difference between the z-coordinates of the black dots \\(length~of~green~vector = z_{1} - z_{2}\\) Now that we know the length of the yellow and green vectors, we can apply the Pythagoras’ theorem to find the length of the red vector \\(length~of~red~vector = \\sqrt{(length~of~yellow~line)^2+(length~of~green~vector)^2}\\) \\(length~of~red~vector = \\sqrt{(x_{1} - x_{2})^2+(z_{1} - z_{2})^2}\\) Now that we know the length of the red vector and the blue vector we found earlier, we can apply the Pythagoras’ theorem to find the black vector \\(length~of~black~vector = \\sqrt{(length~of~red~line)^2+(length~of~blue~vector)^2}\\) \\(length~of~black~vector = \\sqrt{(\\sqrt{(x_{1} - x_{2})^2+(z_{1} - z_{2})^2})^2+(y_{1} - y_{2})^2}\\) \\(length~of~black~vector = \\sqrt{(x_{1} - x_{2})^2+(z_{1} - z_{2})^2+(y_{1} - y_{2})^2}\\) \\(length~of~black~vector = \\sqrt{(x_{1} - x_{2})^2+(y_{1} - y_{2})^2+(z_{1} - z_{2})^2}\\) Other concepts Two vectors are equal if have the same magnitude and direction One vector is the negative of another vector if they have the same magnitude but opposite direction One vector is a scalar multiple of another vector if they have the same direction but different magnitude (they are scaled versions of each other) Collinear points are points that line on the same straight line ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) ## of type &#39;expression&#39; ## quartz_off_screen ## 2 Dot product of 2 vectors Concept The dot product basically quantifies the extent to which 2 vectors interact with each other (or how much each of the two vectors reinforce each other in their magnitude) When the vectors are orthogonal to each other, in other words, they don’t interact with each other, then they have a dot product of \\(0\\) (because when 2 vectors are orthogonal, \\(\\theta = 90\\), and \\(\\cos{90} = 0\\), therefore, the dot product is \\(0\\) ) When 2 vectors are parallel to each other, in other words, they full interact with each other, then they have a dot product of \\(|\\mathbf{a}||\\mathbf{b}|\\) Mathematics The usual form \\(\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}||\\mathbf{b}|\\cos{\\theta}\\) The component form \\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{d}{a_{d}b_{d}}\\) \\(\\mathbf{a} \\cdot \\mathbf{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + \\cdots + a_{d}b_{d}\\) Where \\(a_{d}\\) - The magnitude of vector \\(\\mathbf{a}\\) in the direction of the d axis \\(b_{d}\\) - The magnitude of vector \\(\\mathbf{b}\\) in the direction of the d axis \\(a_{d}b_{d}\\) - The combined magnitude of vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of the d-axis \\(d\\) - The total number of dimensions Explanation Given 2 2-D vectors, vector \\(\\mathbf{a}\\) and vector \\(\\mathbf{b}\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{b}\\) The magnitude of \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{b}\\) (\\(\\mathbf{a}_{\\mathbf{b}}\\)) can be found using the ratio of \\(\\cos{}\\) \\(\\mathbf{a}_{\\mathbf{b}} = |\\mathbf{a}|\\cos{\\theta}\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{b}\\) is the multiplicative effect between the magnitude of \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{b}\\) (or \\(\\mathbf{a}_{\\mathbf{b}}\\)) and the magnitude of \\(\\mathbf{b}\\) \\(\\mathbf{a}\\cdot\\mathbf{b} = |\\mathbf{a}|\\cos{\\theta} |\\mathbf{b}|\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\) The magnitude of \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\) (\\(\\mathbf{b}_{\\mathbf{a}}\\)) can be found using the ratio of \\(\\cos{}\\) \\(\\mathbf{b}_{\\mathbf{a}} = |\\mathbf{b}|\\cos{\\theta}\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\) is multiplicative effect between the magnitude of \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{a}\\) (or \\(\\mathbf{b}_{\\mathbf{a}}\\)) and the magnitude of \\(\\mathbf{a}\\) \\(\\mathbf{a}\\cdot\\mathbf{b} = |\\mathbf{b}|\\cos{\\theta}|\\mathbf{a}|\\) Duality We can see that the combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{b}\\) is exactly the same as the combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\). Hence, the order does not matter (\\(\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{b}\\cdot\\mathbf{a}\\)) Alternative thinking \\(|\\mathbf{a}||\\mathbf{b}|\\) is the maximum possible amount of interaction between the 2 vectors \\(\\cos{\\theta}\\) is like a quantity that quantifies how much the 2 vectors project to the same direction as each other. This quantity ranges from -1 to +1. +1 means that the 2 vectors are fully in the same direction. When the 2 vectors are entirely in the same direction, then \\(\\theta = 0\\) and \\(\\cos{90} = 1\\) When the 2 vectors are entirely in the opposite direction, then \\(\\theta = 180\\) and \\(\\cos{180} = -1\\) When the 2 vectors are orthogonal to each other, then \\(\\theta = 90\\) and \\(\\cos{90} = 0\\) \\(|\\mathbf{a}||\\mathbf{b}| \\cos{\\theta}\\) is like scaling \\(|\\mathbf{a}||\\mathbf{b}|\\) by the factor \\(\\cos{\\theta}\\) such that the result reflects the amount of interaction between the 2 vectors When \\(\\cos{90} = 1\\), then you get the maximum possible amount of interaction When \\(\\cos{180} = -1\\), then you get maximum possible amount of interaction When \\(\\cos{0} = 0\\), then you 0 percent of that interaction Other tips \\(\\mathbf{a} \\cdot (\\mathbf{b} + \\mathbf{c}) = \\mathbf{a} \\cdot\\mathbf{b} + \\mathbf{a} \\cdot\\mathbf{c}\\) ## Rendering [--------------------------] at 3.2 fps ~ eta: 31s ## Rendering [&gt;-------------------------] at 3.1 fps ~ eta: 31s ## Rendering [&gt;-------------------------] at 3.1 fps ~ eta: 30s ## Rendering [=&gt;------------------------] at 3.1 fps ~ eta: 30s ## Rendering [=&gt;------------------------] at 3.1 fps ~ eta: 29s ## Rendering [==&gt;-----------------------] at 3.1 fps ~ eta: 29s ## Rendering [==&gt;-----------------------] at 3.1 fps ~ eta: 28s ## Rendering [===&gt;----------------------] at 3.1 fps ~ eta: 27s ## Rendering [===&gt;----------------------] at 3.1 fps ~ eta: 26s ## Rendering [====&gt;---------------------] at 3.1 fps ~ eta: 26s ## Rendering [====&gt;---------------------] at 3.1 fps ~ eta: 25s ## Rendering [=====&gt;--------------------] at 3.1 fps ~ eta: 25s ## Rendering [=====&gt;--------------------] at 3.1 fps ~ eta: 24s ## Rendering [======&gt;-------------------] at 3.1 fps ~ eta: 24s ## Rendering [======&gt;-------------------] at 3.1 fps ~ eta: 23s ## Rendering [=======&gt;------------------] at 3.1 fps ~ eta: 23s ## Rendering [=======&gt;------------------] at 3.1 fps ~ eta: 22s ## Rendering [========&gt;-----------------] at 3.1 fps ~ eta: 22s ## Rendering [========&gt;-----------------] at 3.1 fps ~ eta: 21s ## Rendering [=========&gt;----------------] at 3.1 fps ~ eta: 20s ## Rendering [=========&gt;----------------] at 3.1 fps ~ eta: 19s ## Rendering [==========&gt;---------------] at 3.1 fps ~ eta: 19s ## Rendering [==========&gt;---------------] at 3.1 fps ~ eta: 18s ## Rendering [===========&gt;--------------] at 3.1 fps ~ eta: 18s ## Rendering [===========&gt;--------------] at 3.1 fps ~ eta: 17s ## Rendering [============&gt;-------------] at 3.1 fps ~ eta: 17s ## Rendering [============&gt;-------------] at 3.1 fps ~ eta: 16s ## Rendering [=============&gt;------------] at 3.1 fps ~ eta: 16s ## Rendering [=============&gt;------------] at 3.1 fps ~ eta: 15s ## Rendering [==============&gt;-----------] at 3.1 fps ~ eta: 14s ## Rendering [==============&gt;-----------] at 3.1 fps ~ eta: 13s ## Rendering [===============&gt;----------] at 3.1 fps ~ eta: 13s ## Rendering [===============&gt;----------] at 3.1 fps ~ eta: 12s ## Rendering [================&gt;---------] at 3.1 fps ~ eta: 12s ## Rendering [=================&gt;----------] at 3 fps ~ eta: 11s ## Rendering [==================&gt;---------] at 3 fps ~ eta: 11s ## Rendering [==================&gt;---------] at 3 fps ~ eta: 10s ## Rendering [===================&gt;--------] at 3 fps ~ eta: 10s ## Rendering [===================&gt;--------] at 3 fps ~ eta: 9s ## Rendering [====================&gt;-------] at 3 fps ~ eta: 9s ## Rendering [====================&gt;-------] at 3 fps ~ eta: 8s ## Rendering [=====================&gt;------] at 3 fps ~ eta: 8s ## Rendering [=====================&gt;------] at 3 fps ~ eta: 7s ## Rendering [======================&gt;-----] at 3 fps ~ eta: 6s ## Rendering [=======================&gt;----] at 3 fps ~ eta: 5s ## Rendering [=======================&gt;----] at 3 fps ~ eta: 4s ## Rendering [========================&gt;---] at 3 fps ~ eta: 4s ## Rendering [========================&gt;---] at 3 fps ~ eta: 3s ## Rendering [=========================&gt;--] at 3 fps ~ eta: 3s ## Rendering [=========================&gt;--] at 3 fps ~ eta: 2s ## Rendering [==========================&gt;-] at 3 fps ~ eta: 2s ## Rendering [==========================&gt;-] at 3 fps ~ eta: 1s ## Rendering [===========================&gt;] at 3 fps ~ eta: 0s ## Rendering [============================] at 3 fps ~ eta: 0s "],["a-very-statistical-day.html", "Chapter 4 A very statistical day", " Chapter 4 A very statistical day Lets begin with some matrices. Matrices are a good way to express a system of linear equations in a more compact form. They may help the reader to manipulate a system of linear equations and spot insights easily. "],["derivation.html", "Chapter 5 Derivation", " Chapter 5 Derivation "],["multivariate-analysis.html", "Chapter 6 Multivariate Analysis", " Chapter 6 Multivariate Analysis MANOVA Multivariate analysis Concept Any analyses involving more than one outcome variable The process of estimating the relationship between the linear combination of several outcome variables and one or more predictor variables Strengths of Multivariate Analysis A multivariate model allows the relationships between the set of predictor variables and multiple outcome variables to be tested in one test, which can maintain the Type I error rate (as opposed to testing the relationship between the same set of predictor variables and different outcome variables with separate models, which would inflate Type I error rate due to multiple testing) Takes into account the relationship between the outcome variables More power to detect whether groups differ along a combination of dimensions MANOVA has greater power than ANOVA to detect effects because it takes account of the correlations between the outcome variables (Huberty &amp; Morris, 1989) Centroids Concept A centre point in a multidimensional geometric space Mean centroid Concept A centre point that represents the mean on multiple dimensions A mean centroid is represented by a column vector Mathematics (Matrix) \\({\\boldsymbol{\\mu}} = \\begin{pmatrix} \\mu_{1} \\\\ \\mu_{2} \\\\ \\mu_{3} \\\\ \\vdots \\\\ \\mu_{d} \\end{pmatrix}\\) Where \\(d\\) - The total number of outcome variables (variates) Hotelling’s \\(t^2\\) (Hotelling, 1931) Concept Multivariate version of the t-statistic A test statistic for the difference between 2 mean centroids (or multivariate means; means on multiple outcome variables) Tests whether 2 means differ on the linear combination of multiple outcome variables Mathematics (Matrix) \\(t^2 = \\frac{n_{1}n_{2}}{n_{1}n_{2}}(\\mathbf{Y_{1}-\\mathbf{Y_2}})\\mathbf{&#39;}(\\mathbf{S})^{-1}(\\mathbf{Y_{1}-\\mathbf{Y_2}})\\) Where \\(\\mathbf{S}\\) - Variance covariance matrix \\(\\mathbf{Y_{1}}\\) - Mean centroid for group 1 \\(\\mathbf{Y_{2}}\\) - Mean centroid for group 2 The relationship between Mahalanobis \\(D^2\\) and Hotelling’s \\(t^2\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
