[["index.html", "Statistics Chapter 1 The Linear Regression Model", " Statistics Samuel Mak 2022-09-10 Chapter 1 The Linear Regression Model Mathematical model Description A model that describes the relationship between variables with a mathematical function/expression Linear Model Description A model that describes the relationship between variables with the linear function Mathematics \\(y = mx + c\\) Assumption of the linear model Linearity - The relationship between the variables of interest is linear or best described by a linear function Additivity - The relationship between the variables of interest is best described as a linear combination Regression Concept The process of estimating the relationship between variables of interest Linear Regression Concept The process of estimating the relationship between variables of interest with a linear model (fitting a straight line to the data) Curvilinear Regression Concept The process of estimating the relationship between variables of interest with a curvilinear model (fitting a curve to the data) Simple Linear Regression Concept Linear regression with one predictor Mathematics Population Model \\(y_{i} = \\beta_{0} + \\beta_{1}X_{i,1} + \\epsilon_{i}\\) Where \\(y_{i}\\) The value of observation i \\(\\beta_{p}\\) Regression coefficients/parameters \\(\\beta_{0}\\) The intercept The predicted value of the outcome when the value of predictor 1 (\\(X_{1}\\)) is 0 \\(\\beta_{1}\\) Regression coefficient/weight for predictor 1 Represents the magnitude and direction of the relationship between predictor 1 and the outcome as the change in the value of the outcome variable for every unit change in the value of predictor 1 (it is the gradient/slope of the regression line) \\(X_{i,1}\\) The value of predictor 1 for observation i \\(\\epsilon_{i}\\) Error for observation i Error - The absolute deviation/distance/difference between the observed value and the expected/predicted/fitted value for observation i This quantifies the error in prediction at the observation level Sample Model \\(y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i,1} + e_{i}\\) Mathematics (Matrix) Population Model \\(\\begin{aligned}\\mathbf{Y} &amp;= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{E} \\\\ \\begin{bmatrix} y_{i} \\\\ y_{i} \\\\ y_{3} \\\\ \\vdots \\\\ y_{n}\\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ 1 &amp; x_{3} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n}\\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_{1} \\\\ \\epsilon_{2} \\\\ \\epsilon_{3} \\\\ \\vdots \\\\ \\epsilon_{n}\\end{bmatrix}\\end{aligned}\\) Where \\(\\mathbf{Y}\\) A column vector that contains the values for the outcome variable for each of the observations \\(\\mathbf{X}\\) The model matrix/design matrix The model matrix contains Column vector \\(\\mathbf{U}\\) A column vector of 1s one for each observation. The column of 1s corresponds to \\(\\beta_{0}\\) in the \\(\\boldsymbol{\\beta}\\) matrix, which means that the intercept is the same for every observation. Column vector of x values A column vector that contains the values of predictor variable 1 for each of the observations \\(\\boldsymbol{\\beta}\\) A column vector that contains the regression coefficient/weight for each of regression parameters \\(\\mathbf{E}\\) A column vector that contains the error of each of the observations Methods of Estimation Ordinary Least Squares (OLS) Generalised Least Squares (GLS) Maximum Likelihood (ML) Ordinary Least Squares Concept Estimates the regression line by fitting a line that minimises the model sum of squared residuals (the sum of the squared difference/distance/deviation between each of the observed values and the corresponding predicted/fitted values) \\(\\min{\\sum_{i=1}^{n}{e^{2}_{i}}} = \\min{\\sum_{i=1}^{n}({y_{i} - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1}}})^2\\) Mathematics Derivation Start off with the linear model with the outcome variable as a function of the weights \\(y_i = \\hat\\beta_0 + \\hat\\beta_{1}x_{i,1} + e_i\\) Make the residual as the subject (express the residual as a function of the weights) \\(e_i = y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1}\\) Square both sides \\(e_{i}^{2} = (y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2\\) Sum both sides \\(\\begin{aligned}\\sum_{i=1}^{n}{e_{i}^{2}} &amp;= \\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2} \\\\ S(\\beta_0,\\beta_1) &amp;= \\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2}\\end{aligned}\\) Find the values of \\(\\hat\\beta_{0}\\) and \\(\\hat\\beta_{1}\\) that minimises \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) There is a point at which the sum of squared residuals is at the minimum The \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) function is a quadratic and it has a minimum that represents the minimal sum of squared residuals The coordinates of the minimum are the values of \\(\\hat\\beta_{0}\\) and \\(\\hat\\beta_{1}\\) that give you the least sum of squared residuals (or minimises \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\)) Hence, find the values of the coordinate of the minimum of the quadratic function To find each of the values of the coordinate of the minimum of the function \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\), the partial derivatives of the function \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) with respect to each of the \\(\\hat\\beta\\)s are set to 0 (because the minimum has a slope of 0) Expression for \\(\\hat\\beta_0\\) \\(\\frac{\\partial{S}}{\\partial{\\hat\\beta_{0}}}=-2\\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})}\\) Expression for \\(\\hat\\beta_1\\) \\(\\frac{\\partial{S}}{\\partial{\\hat\\beta_{1}}}=-2\\sum_{i=1}^{n}{x_{i,1}(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})}\\) Solve the above simultaneous equations Solving the above simultaneous equations would result in \\(\\hat\\beta_{0} = \\frac{\\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{y_{i}} - \\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{x_{i,1}y_{i}}}{n\\sum_{i=1}^{n}{x_{i,1}^2}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\hat\\beta_{1} = \\frac{n\\sum_{i=1}^{n}{x_{i,1}y_i} - \\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{y_{i}}}{n\\sum_{i=1}^{n}{x_{i,1}^2}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) Solving the simultaneous equations would also result in For \\(\\hat\\beta_{0}\\) \\(\\hat\\beta_{0} = \\bar{y} - \\hat\\beta_{1}\\bar{x}\\) For \\(\\hat\\beta_{1}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})(y_{i} - \\bar{y})}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\sum_{i=1}^{n}{x_{i,1}}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})y_{i}}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\sum_{i=1}^{n}{x_{i,1}}\\) \\(\\hat\\beta_{1} = \\frac{Cov(x,y)}{s_{x}^{2}}\\) \\(\\hat\\beta_{1} = \\frac{S_{xx}}{S_{xx}}\\) "],["the-pool-of-tears.html", "Chapter 2 The pool of tears", " Chapter 2 The pool of tears "],["a-caucus-race-and-a-long-tale.html", "Chapter 3 A caucus-race and a long tale", " Chapter 3 A caucus-race and a long tale This is chapter 3, and I will talk about robust methods in this chapter. Robust statistics is a huge topic and it really needs a whole library to contain all of the knowledge in this topic. In this chapter, we will look at Guitars Music Statistics "],["a-very-statistical-day.html", "Chapter 4 A very statistical day", " Chapter 4 A very statistical day Lets begin with some matrices. Matrices are a good way to express a system of linear equations in a more compact form. Itâ€™s benefits include helping the reader to manipulate a system of linear equations easily and spot insights easily. \\(\\begin{array}{ccc} {\\sigma}^{2} &amp; 1\\\\ 1 &amp; {\\sigma}^{2} \\end{array}\\) \\(\\small X = \\begin{bmatrix} 1 &amp; 2\\\\ 1 &amp; 2\\\\ 1 &amp; 2 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2\\\\ 1 &amp; 2\\\\ 1 &amp; 2 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2\\\\ 1 &amp; 2\\\\ 1 &amp; 2 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2\\\\ 1 &amp; 2\\\\ 1 &amp; 2 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2\\\\ 1 &amp; 2\\\\ 1 &amp; 2 \\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2\\\\ 1 &amp; 2\\\\ 1 &amp; 2 \\end{bmatrix} + \\begin{bmatrix}1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\\\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\\\end{bmatrix}\\) Note This is a matrix This is the first point This is the seccond point \\(y = mx + c + d + e + f + g + h + i + j + k + l + m + n + o + p\\) library(ggplot2) carData::Bfox ## partic tfr menwage womwage debt ## 1946 25.3 3748 25.35 14.05 18.18 ## 1947 24.4 3996 26.14 14.61 28.33 ## 1948 24.2 3725 25.11 14.23 30.55 ## 1949 24.2 3750 25.45 14.61 35.81 ## 1950 23.7 3669 26.79 15.26 38.39 ## 1951 24.2 3682 26.33 14.58 26.52 ## 1952 24.1 3845 27.89 15.66 45.65 ## 1953 23.8 3905 29.15 16.30 52.99 ## 1954 23.6 4047 29.52 16.57 54.84 ## 1955 24.3 4043 32.05 17.99 65.53 ## 1956 25.1 4092 32.98 18.33 72.56 ## 1957 26.2 4168 32.25 17.64 69.49 ## 1958 26.6 4073 32.52 18.16 71.71 ## 1959 26.9 4100 33.95 18.58 78.89 ## 1960 27.9 4119 34.63 18.95 84.99 ## 1961 29.1 4159 35.14 18.78 87.71 ## 1962 29.9 4134 34.49 18.74 95.31 ## 1963 29.8 4017 35.99 19.71 104.40 ## 1964 30.9 3886 36.68 20.06 116.80 ## 1965 32.1 3467 37.96 20.94 130.99 ## 1966 33.2 3150 38.68 21.20 135.25 ## 1967 34.5 2879 39.65 21.95 142.93 ## 1968 35.1 2681 41.20 22.68 155.47 ## 1969 36.1 2563 42.44 23.75 165.04 ## 1970 36.9 2571 42.02 25.63 164.53 ## 1971 37.0 2503 45.32 26.79 169.63 ## 1972 37.9 2302 45.61 27.51 190.62 ## 1973 40.1 2931 45.59 27.35 209.60 ## 1974 40.6 1875 48.06 29.64 216.66 ## 1975 42.2 1866 46.12 29.33 224.34 ## parttime ## 1946 10.28 ## 1947 9.28 ## 1948 9.51 ## 1949 8.87 ## 1950 8.54 ## 1951 8.84 ## 1952 8.60 ## 1953 5.49 ## 1954 6.67 ## 1955 6.25 ## 1956 6.32 ## 1957 7.30 ## 1958 8.65 ## 1959 8.80 ## 1960 9.39 ## 1961 10.23 ## 1962 10.77 ## 1963 10.84 ## 1964 11.70 ## 1965 12.33 ## 1966 12.18 ## 1967 13.67 ## 1968 13.82 ## 1969 14.91 ## 1970 15.52 ## 1971 15.47 ## 1972 15.85 ## 1973 15.40 ## 1974 16.23 ## 1975 16.71 hist_plot &lt;- ggplot(data = carData::Bfox, aes(x = debt)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
