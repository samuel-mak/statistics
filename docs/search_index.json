[["index.html", "Statistics Chapter 1 The Linear Regression Model", " Statistics Samuel Mak Last Updated: 31-Dec-2022 | 14:30:41 Chapter 1 The Linear Regression Model Mathematical model Description A model that describes the relationship between variables with a mathematical function/expression Linear Model Description A model that describes the relationship between variables with the linear function Mathematics \\(y = mx + c\\) Assumption of the linear model Linearity - The relationship between the variables of interest is linear or best described by a linear function Additivity - The relationship between the variables of interest is best described as a linear combination Regression Concept The process of estimating the relationship between variables of interest Linear Regression Concept The process of estimating the relationship between variables of interest with a linear model (fitting a straight line to the data) Curvilinear Regression Concept The process of estimating the relationship between variables of interest with a curvilinear model (fitting a curve to the data) Simple Linear Regression Concept Linear regression with one predictor Mathematics Population Model \\(y_{i} = \\beta_{0} + \\beta_{1}X_{i,1} + \\epsilon_{i}\\) Where \\(y_{i}\\) The value of observation i \\(\\beta_{p}\\) Regression coefficients/parameters \\(\\beta_{0}\\) The intercept The predicted value of the outcome when the value of predictor 1 (\\(X_{1}\\)) is 0 \\(\\beta_{1}\\) Regression coefficient/weight for predictor 1 Represents the magnitude and direction of the relationship between predictor 1 and the outcome as the change in the value of the outcome variable for every unit change in the value of predictor 1 (it is the gradient/slope of the regression line) \\(X_{i,1}\\) The value of predictor 1 for observation i \\(\\epsilon_{i}\\) Error for observation i Error - The absolute deviation/distance/difference between the observed value and the expected/predicted/fitted value for observation i This quantifies the error in prediction at the observation level Sample Model \\(y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i,1} + e_{i}\\) Mathematics (Matrix) Population Model \\(\\begin{aligned}\\mathbf{Y} &amp;= \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{E} \\\\ \\begin{bmatrix} y_{i} \\\\ y_{i} \\\\ y_{3} \\\\ \\vdots \\\\ y_{n}\\end{bmatrix} &amp;= \\begin{bmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ 1 &amp; x_{3} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n}\\end{bmatrix} \\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\end{bmatrix} + \\begin{bmatrix} \\epsilon_{1} \\\\ \\epsilon_{2} \\\\ \\epsilon_{3} \\\\ \\vdots \\\\ \\epsilon_{n}\\end{bmatrix}\\end{aligned}\\) Where \\(\\mathbf{Y}\\) A column vector that contains the values for the outcome variable for each of the observations \\(\\mathbf{X}\\) The model matrix/design matrix The model matrix contains Column vector \\(\\mathbf{U}\\) A column vector of 1s one for each observation. The column of 1s corresponds to \\(\\beta_{0}\\) in the \\(\\boldsymbol{\\beta}\\) matrix, which means that the intercept is the same for every observation. Column vector of x values A column vector that contains the values of predictor variable 1 for each of the observations \\(\\boldsymbol{\\beta}\\) A column vector that contains the regression coefficient/weight for each of regression parameters \\(\\mathbf{E}\\) A column vector that contains the error of each of the observations Methods of Estimation Ordinary Least Squares (OLS) Generalised Least Squares (GLS) Maximum Likelihood (ML) Ordinary Least Squares Concept Estimates the regression line by fitting a line that minimises the model sum of squared residuals (the sum of the squared difference/distance/deviation between each of the observed values and the corresponding predicted/fitted values) \\(\\min{\\sum_{i=1}^{n}{e^{2}_{i}}} = \\min{\\sum_{i=1}^{n}({y_{i} - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1}}})^2\\) Mathematics \\(\\hat\\beta_{0} = \\bar{y} - \\hat\\beta_{1}\\bar{x}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})y_{i}}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) Note I think these two equations are rather boring, looking at how they are derived is much more interesting Derivation Start off with the linear model with the outcome variable as a function of the weights \\(y_i = \\hat\\beta_0 + \\hat\\beta_{1}x_{i,1} + e_i\\) Make the residual as the subject (express the residual as a function of the weights) \\(e_i = y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1}\\) Square both sides \\(e_{i}^{2} = (y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2\\) Sum both sides \\(\\begin{aligned}\\sum_{i=1}^{n}{e_{i}^{2}} &amp;= \\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2} \\\\ S(\\beta_0,\\beta_1) &amp;= \\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})^2}\\end{aligned}\\) Find the values of \\(\\hat\\beta_{0}\\) and \\(\\hat\\beta_{1}\\) that minimises \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) There is a point at which the sum of squared residuals is at the minimum The \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) function is a quadratic and it has a minimum that represents the minimal sum of squared residuals The coordinates of the minimum are the values of \\(\\hat\\beta_{0}\\) and \\(\\hat\\beta_{1}\\) that give you the least sum of squared residuals (or minimises \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\)) Hence, find the values of the coordinate of the minimum of the quadratic function To find each of the values of the coordinate of the minimum of the function \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\), the partial derivatives of the function \\(S(\\hat\\beta_{0}, \\hat\\beta_{1})\\) with respect to each of the \\(\\hat\\beta\\)s are set to 0 (because the minimum has a slope of 0) Expression for \\(\\hat\\beta_0\\) \\(\\frac{\\partial{S}}{\\partial{\\hat\\beta_{0}}}=-2\\sum_{i=1}^{n}{(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})}\\) Expression for \\(\\hat\\beta_1\\) \\(\\frac{\\partial{S}}{\\partial{\\hat\\beta_{1}}}=-2\\sum_{i=1}^{n}{x_{i,1}(y_i - \\hat\\beta_{0} - \\hat\\beta_{1}x_{i,1})}\\) Solve the above simultaneous equations Solving the above simultaneous equations would result in \\(\\hat\\beta_{0} = \\frac{\\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{y_{i}} - \\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{x_{i,1}y_{i}}}{n\\sum_{i=1}^{n}{x_{i,1}^2}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\hat\\beta_{1} = \\frac{n\\sum_{i=1}^{n}{x_{i,1}y_i} - \\sum_{i=1}^{n}{x_{i,1}}\\sum_{i=1}^{n}{y_{i}}}{n\\sum_{i=1}^{n}{x_{i,1}^2}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) Solving the simultaneous equations would also result in For \\(\\hat\\beta_{0}\\) \\(\\hat\\beta_{0} = \\bar{y} - \\hat\\beta_{1}\\bar{x}\\) For \\(\\hat\\beta_{1}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})(y_{i} - \\bar{y})}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})y_{i}}}{\\sum_{i=n}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\hat\\beta_{1} = \\frac{Cov(x,y)}{s_{x}^{2}}\\) \\(\\hat\\beta_{1} = \\frac{S_{xx}}{S_{xx}}\\) Mathematics (Matrix) \\(\\boldsymbol{\\beta} = (\\mathbf{X&#39;X)^{-1}X&#39;Y}\\) Derivation (Matrix) \\(S(\\boldsymbol{\\hat\\beta}) = ||\\mathbf{u}||^2 = ||\\mathbf{Y-\\hat{Y}}||^2 = ||\\mathbf{Y-X\\boldsymbol{\\beta}}||^2\\) note - Matrix derivation not complete Variance of \\(\\beta\\) Variance of \\(\\beta\\) Variance of \\(\\beta_{0}\\) Mathematics \\(\\text{Var}(\\hat\\beta_{0}) = \\sigma_{\\hat\\beta_{0}}^{2} = \\frac{\\sigma^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\text{SE}_{\\hat\\beta_{0}} = \\sigma_{\\hat\\beta_{0}} = \\sqrt{\\frac{\\sigma^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}}\\) Estimation from the sample Description \\(\\sigma^{2}\\) is unknown Since \\(\\sigma^{2}\\) is the variance of the population model (the average model sum of squared errors), it is reasonable to use the variance of the sample model (\\(s^2\\)) as the estimator It has been shown that \\(s^2\\) is an unbiased estimator of \\(\\sigma^{2}\\) Mathematics \\(\\hat{\\text{Var}(\\hat\\beta_{0})} = \\hat\\sigma_{\\hat\\beta_{0}}^{2} = \\frac{s^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\hat{\\text{SE}}_{\\hat\\beta_{0}} = \\hat\\sigma_{\\hat\\beta_{0}} = \\sqrt{\\frac{s^{2}\\sum_{i=1}^{n}{x_{i,1}^2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}-(\\sum_{i=1}^{n}{x_{i,1}})^2}}\\) Variance of \\(\\beta_{1}\\) Population Mathematics \\(\\sigma_{\\hat\\beta_{1}^{2}} = \\frac{n\\sigma^{2}}{n\\sum_{i=1}^{n}{x^{2}_{i,1}}- (\\sum_{i=1}^{n}{x_{i,1}})^2}\\) \\(\\text{Var}(\\hat\\beta_{1}) = \\sigma_{\\hat\\beta_{1}^{2}} = \\frac{n\\sigma^{2}}{n\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\text{Var}(\\hat\\beta_{1}) = \\sigma_{\\hat\\beta_{1}}^{2} = \\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\text{SE}_{\\hat\\beta_{1}} = \\sigma_{\\hat\\beta_{1}} = \\sqrt{\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}}\\) Estimation from the sample Mathematics \\(\\hat{\\text{Var}(\\hat\\beta_{1})} = \\hat\\sigma_{\\hat\\beta_{1}}^{2} = \\frac{s^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\) \\(\\hat{\\text{SE}}_{\\hat\\beta_{1}} = \\hat\\sigma_{\\hat\\beta_{1}} = \\sqrt{\\frac{s^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}}\\) Assumptions of the Linear Models Linearity Additivity Normality of the sampling distribution Uncorrelated errors Heteroscedasticity Linearity Concept The relationship between each of the predictor variables and the outcome is linear (or best described as linear) This is the most important assumption of linear models because even if all other assumptions are met, the model is invalid because the description of the process you want to model is wrong Treatment Transformation Segmented regression Fit non-linear models Additivity Concept The combined effect of multiple predictors in a multiple regression is best described as a linear combination of the predictors This, along with linearity, is the most important assumptions of linear models Treatment Transformation Segmented regression Fit non-linear models Uncorrelated Errors Concept The errors in the population model are uncorrelated with each other In other words, there is no autocorrelation/serial correlation The distribution of errors at given value of x is uncorrelated with the distribution of errors at another value of x It is important to note that the errors are not individual errors, they are theoretical distribution of errors given x, in other words, for each vale of x, there is a distribution of errors, and the observed residuals given x you have are sampled from the distribution of errors given the same x (see this short description on CrossValidated) Mathematics \\(\\text{Cov}(e_{i}, e_{j}) = 0\\) Mathematics \\(\\boldsymbol\\Phi = \\Sigma_{e_{i}e_{j}} = \\sigma^{2}\\mathbf{I} = \\begin{bmatrix} \\sigma_{1,1}^{2} &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma_{2,2}^{2} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{3,3}^{2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_{i,j}^{2} \\end{bmatrix}\\) Note All the off-diagonals are 0s, indicating that all errors are uncorrelated with each other The variances are unconstrained, hence, they can vary Effects of correlated errors on OLS estimation Biased SE The standard mathematical expression of the estimate of the variance of the sampling distribution (\\(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\)) is a biased estimate of the true variance of the sampling distribution If errors are positively correlated, the standard mathematical expression of the estimate of the variance of the sampling distribution (\\(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}{(x_{i,1} - \\bar{x})^2}}\\)) will underestimate the true value (a negative bias), this means that the true sampling variance and SE are larger than estimated, this means that OLS is less efficient/optimal than other estimators, this follows that all statistical inferences involving the SE will be biased (e.g. underestimating the confidence intervals, overestimating the p-value in significance tests, higher Type I error etc.) Assessing Autocorrelation See the autocorrelation section Treatment Autocorrelation consistent standard errors Cluster robust standard errors Use other estimators Normality of the Sampling Distribution Concept The sampling distribution of the sample estimates is normally distributed A normally distributed sampling distribution is desirable because it can be modelled easily with the normal distribution curve and inferential statistics can be easily derived (which may be easier than if it has a non-normal distribution, e.g. Pearson’s r) Effects of violation of the assumption All the standard mathematical expressions and procedures for estimating the inferential statistics and uncertainty of the sample estimate are based on the assumption that the sampling distribution is normal or at least approximately normal. If the true sampling distribution is not normal, then all the inferential statistics and estimates of uncertainty of the sample estimate are incorrect Assessing Normality of the sampling distribution Distribution of model residuals Description Assess whether the model residuals are normally distributed Explanation In general, in a linear combination of variables, if the variables in the linear combination are random, independent, and normally distributed (their sampling distribution), then the linear combination itself is also normally distributed (the sampling distribution of the linear combination is normal) Each of the regression parameters can be expressed as a linear combination \\(\\hat\\beta_{1} = \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x} )y_i}}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}} = \\sum_{i=1}^{n}{\\frac{x_i - \\bar{x}}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}}y_i}\\) Where \\(\\frac{x_i - \\bar{x}}{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}}\\) is treated as a constant and \\(y_i\\) is treated a variable Hence, for the linear combination (\\(\\hat\\beta_1\\)) to be normally distributed, the random variable \\(y_i\\) has to be independent and normally distributed Since independence and normality of the errors imply independence and normality of the random variable \\(y_i\\), normality of errors can be used to indicate that the linear combination (\\(\\hat\\beta_1\\)) is normally distributed And since we don’t have the errors in hand, we use the model residuals Assessing normality See the ‘Assessing Normality’ section The Central Limit Theorem and the assumption of normality The distribution of the residuals is not important if sample size is “sufficiently” large according to the Central Limit Theorem. (The CLT states that the sampling distribution is approximately normal if sample size is large and can be assumed to be normal if sample size is sufficiently large (n &gt; 30) regardless of the shape of the population distribution) Hence, there is much less to worry about the non-normality of the sampling distribution if the sample size is “sufficiently” large Just some side notes \\(y_i\\) also needs to be normally distributed, if y is not normally distributed, then the scores of y need to be transformed such that it’s sampling distribution is normal or approximately normal (I am still not sure about this part) Treatment Use robust/non-parametric methods Transformation Heteroscedasticity Description Aka - Homogeneity of variance or constant variance The variance of the distribution of errors given x is the identical between values of x in the population model The variance of the distribution of errors given x is a constant - It is a fixed value and does not vary across values of x in the population model Some sources say that the variance of errors given x does not depend on x (e.g. Rice, 2006; Field, 2023) Visualisation Mathematics \\(\\text{Var}(\\epsilon_{i}) = \\sigma^2\\) Mathematics (Matrix) \\(\\boldsymbol\\Phi = E(\\epsilon\\epsilon&#39;|X) = \\Sigma_{\\epsilon\\epsilon} = \\begin{bmatrix} \\sigma^{2} &amp; \\phi_{1,2} &amp; \\phi_{1,3} &amp; \\cdots &amp; \\phi_{1,j} \\\\ \\phi_{2,1} &amp; \\sigma^{2} &amp; \\phi_{2,3} &amp; \\cdots &amp; \\phi_{2,j} \\\\ \\phi_{3,1} &amp; \\phi_{3,2} &amp; \\sigma^{2} &amp; \\cdots &amp; \\phi_{3,j} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\phi_{i,1} &amp; \\phi_{i,2} &amp; \\phi_{i,3} &amp;\\cdots &amp; \\sigma^{2} \\end{bmatrix}\\) Notes The variances in the diagonals are all \\(\\sigma^2\\) The covariances in the off-diagonals can vary (the covariances are not the condition here) Effects of Homoscedasticity Biased estimation of sampling variance The mathematical expression for the estimate of the sampling variance assumes homoscedasticity (see derivation) (or rather, requires homoscedasticity to be unbiased). If there is heteroscedasticity, then the sampling variance estimated using the same standard mathematical expression will be biased, in that the estimated value is not the same as the true value, this in turn means that the parameter estimates are not optimal due to increased sampling variation and there are other estimators that will be mor efficient than OLS, the biased sampling variance estimate will affect the SE estimate, and will affect all statistical inferences using the sampling variance (e.g. confidence intervals, test statistic, p-values etc.) Confidence intervals can be “extremely inaccurate” when ignoring heteroscedasticity (Wilcox, 2010) But this assumption matters only if group sizes are unequal Assessing Homo/Heteroscedasticity Description The assumption applies to the population model, it is best to assess heteroscedasticity in the population through heteroscedasticity in the sample model Ways to assess heteroscedasticity Plots Statistical tests Plots Residual Plots Residuals vs fitted plot Residuals vs fitted plot Description The residuals are plotted against the fitted values A plot that shows the vertical distribution of residuals in the y-axis for each of the fitted values in the x-axis Basically it is the scatter plot but detrended It can be based on raw or standardised scores Visualisation ## Base R model &nbsp plot(model, which = 1) Statistical Tests Statistical Tests Levene’s test Brown-Forsythe test Hartley’s Fmax Levene’s tests Description Tests the null hypothesis that the variances in different groups are equal It is a one-way ANOVA with absolute deviation as the outcome variable and the groups/values of x as the predictor variable It tests whether the absolute deviation varies between groups/values of x Original Levene’s test Mathematics \\(\\textit{W} = \\frac{\\sum_{i=1}^{n}{n_{g}(|\\bar{d}|_{g} - |\\bar{d}|_{G})^2}}{\\sum_{i=1}^{n}{n_{g}(|{d}|_{i} - |\\bar{d}|_{g})^2}} \\times \\frac{n-k}{k-1}\\) Where \\(g\\) - Denotes group \\(G\\) - Denotes grand \\(d_i = y_i - \\bar{y}_{g}\\) - A deviation means the difference between a score and mean of the group in which it belongs Note Levene’s test statistic is expressed as such but it can be interpreted as the F-statistic Brown-Forsythe test Description Similar to Levene’s test but uses the median in the absolute deviation instead (\\(d_i = y_i - \\tilde{y}_{g}\\)) Levene’s test using trimmed mean Description Similar to Levene’s test but uses the trimmed mean instead of the mean Comparing the tests Trimmed mean performed best when the underlying data followed a Cauchy distribution and the median performed best when the data followed a chi-squared distribution with 4 degrees of freedom (sharply skewed distribution) (Brown &amp; Forsythe, 1974; extracted from Wiki) Hartley’s Fmax (Pearson &amp; Harley, 1954) Description It is the ratio of the variances between the group with the biggest variance and the group with the smallest variance It then conducts a null hypothesis significance test for the ratio The Fmax isn’t used very often, so it is harder to find a critical value table Mathematics \\(\\textit{F}_{max} = \\frac{\\max{s_{g}^{2}}}{\\min{s_{g}^{2}}}\\) Where \\(\\max{s_{g}^{2}}\\) - The largest group variance among all the groups \\(\\min{s_{g}^{2}}\\) - The smallest group variance among all the groups Cochrane’s C test Description It tests whether the variance of a particular group is exceptionally larger than the rest of the group (it is an upper tail test) The test statistic is the ratio of the variane of a group of interest and the overall variance (of all of the groups) It then conducts a null hypothesis significance test on the ratio It is used to test for variance outlier Mathematics \\(\\textit{C}_{g} = \\frac{s_{g}^{2}}{\\sum_{i=1}^{n}{s_{g}^{2}}}\\) Where \\(s_{g}^{2}\\) - The variance of a particular group \\(\\sum_{i=1}^{n}{s_{g}^{2}}\\) - The sum of all the variances (the total variance) Null Hypothesis Significance Test Hypothesis \\(\\textit{H}_{0}:\\) All variances are equal \\(\\textit{H}_{1}:\\) At least one group variance is significantly larger than the other variances Don’t use these tests Sample size In large samples, even trivial heteroscedasticity can produce significant Levene’s result In small samples, Levene’s test has inadequate power to detect heteroscedasticity Group size equality Homoscedasticity tests work best when group sample sizes are equal (when heteroscedasticity does not matter as mentioned before) and does not work well when sample sizes are unequal (when heteroscedasticity matters) Adjust for it instead… There are robust methods for heteroscedasticity, hence, no need to drill on testing heteroscedasticity Read Zimmerman (2004) about the problems with heteroscedasticity tests… Remedies Heteroscedasticity-Consistent Standard Errors Weighted Least Squares Heteroscedasticity-Consistent Standard Error Description Aka Heteroscedasticity-Robust Standard Errors Robust Standard Errors Eicker-Huber-White standard errors Huber-White Standard Errors White Standard Errors Sandwich Estimator (because the matrix expression looks like a sandwich) Versions HC1 HC2 HC3 HC4 HC0 Description The first version of the HC standard error Mathematical desciption In the mathematical expression of the estimate of the sampling variance of the beta parameters, the variance of errors given x is not treated as fixed/a constant, rather, it relaxes the assumption of homoscedasticity and treat the variance of errors given x are different at different values of x Mathematics Population Model \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=1}^{n}{(x_i - \\bar{x})^2})^2} \\times \\sum_{i=1}^{n}{(x_i - \\bar{x})^2 \\textit{Var}(\\epsilon_i|X)}\\) Sample Estimatte \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=1}^{n}{(x_i - \\bar{x})^2})^2} \\times \\sum_{i=1}^{n}{(x_i - \\bar{x})^2 e_{i}^{2}}\\) Where \\(e_{i}^{2}\\) - Squared residual for observation i (it is basically the variance of residual given x, but we usually only have 1 observation given x) Mathematics (Matrix) \\(\\boldsymbol{\\hat\\beta_1} = (\\mathbf{X&#39;X})^{-1}(\\sum_{i=1}^{n}{e_{i}^{2}x&#39;_{i}x_{i}})(\\mathbf{X&#39;X})^{-1}\\) Evaluation Simulations show that it is better than the normal estimation when sample size is large but worse when sample size is small Which version is the best? HC3 is better than HC0 and HC2 (Long &amp; Ervin, 2000) HC4 appears to be more robust than HC3 when there are influential cases and non-normal errors (See Hayes and Cai, 2007 for a review) Heteroscedastic-Autocorrelation-consistent variance estimation Description Variance estimation robust to heteroscedasticity and autocorrelation They are extensions of the HC standard error Types Newey and West (1987) Cluster Robust Standard Errors Description Standard Errors estimation that accounts for heteroscedasticity and correlated errors Based on the concept that data points can be clustered in a certain way Mathematics \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{i=1}^{n}{\\textit{Var}(\\epsilon_{i}(x_{i} - \\bar{x}))} + 2\\sum_{j=1}^{n}{\\sum_{i=1}^{n}{(x_{i} - \\bar{x})(x_{j} - \\bar{x})(\\epsilon{}_{i}\\epsilon{}_{j})}}\\) \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{j=1}^{n}{\\sum_{i=1}^{n}{\\textit{Cov}((x_{i} - \\bar{x})\\epsilon{}_{i}, (x_{j} - \\bar{x})\\epsilon{}_{j})}}\\) \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{j=1}^{n}{\\sum_{i=1}^{n}{(x_{i} - \\bar{x})(x_{j} - \\bar{x})(\\epsilon{}_{i}\\epsilon{}_{j})}}\\) \\(\\textit{Var}(\\hat\\beta_{1}) = \\frac{1}{(\\sum_{i=i}^{n}{(x_{i} - \\bar{x})^2})^2} \\times \\sum_{j=1}^{n}{\\sum_{i=1}^{n}{(x_{i} - \\bar{x})(x_{j} - \\bar{x})(\\epsilon{}_{i}\\epsilon{}_{j})}}1[A]\\) Where \\(1[A]\\) \\(A\\) is an event in which i and j are in the same cluster \\(1[A]\\) - A indicator function that indicates whether 2 errors belong to the same cluster The indicator function \\(1[A]\\) equals 1 if event A happens (i and j are in the same cluster) The indicator function \\(1[A]\\) equals 0 if event A does not happen (i ad j are not in the same cluster) This results in a block matrix where the diagonal blocks are the error variance-covariance submatrices and the off-diagonal blocks are all 0s (assuming that the clusters are uncorrelated with each other) - For this reason, it is robust for arbitrary within cluster error correlation but not between cluster error correlation Mathematics (Matrix) \\(\\textbf{V} = (\\textbf{X}&#39;\\textbf{X})^{-1} \\textbf{X}&#39;\\boldsymbol\\Phi \\textbf{X}(\\textbf{X}&#39;\\textbf{X})^{-1}\\) Where \\(\\boldsymbol\\Phi\\) - Error Variance-Covariance matrix The Bootstrap (Efron &amp; Tibshirani, 1993) Description Estimates the properties of the sampling distribution empirically from the sample data Autocorrelation Description Aka serial correlation Errors given x are correlated with errors in other values of x Autocorrelation is often described with respect to time because autocorrelation usually occurs in time series data (correlated across time) Reasons for autocorrelation Temporal autocorrelation Description Autocorrelation across time time Examples Repeated measures designs Time series Longitudinal designs Growth models Spatial autocorrelation Description Autocorrelation due to space Examples Clustered data Ignoring covariates If a covariate is not included in the model, the errors will capture the effect of that covariate The error is not random - There are systematic process in the errors that are mistreated as random or ignored Model misspecification E.g. ignoring linearity Measurement error in the independent variable Types of serial correlation First-order serial correlation First-order serial correlation Description The correlation between errors at any one point of x (\\(e_{x}\\)) and errors at the point of x that is 1 unit less (\\(e_{x-1}\\)) Note that t is often used instead of x, but I am keeping things general here Visualisation Positive first-order serial correlation Negative first-order serial correlation Mathematical concept Tests testing for first-order serial correlation usually assess whether errors given \\(x\\) can be predicted by errors a unit of x that is 1 unit smaller (\\(x - 1\\)) Mathematics \\(e_{t} = \\rho{}e_{x-1} + u_x\\) Where \\(\\rho{}\\) - First-order serial correlation coefficient The process for the error term is called the first-order autoregressive process (AR1) Tests for first-order serial correlation Durbin-Watson test Breusch-Godfrey test Durbin-Watson test The Durbin-Watson statistic Mathematics \\(d = \\frac{\\sum_{t=2}^{n}{(e_{t} - e_{t-1})^2}}{\\sum_{t=1}^{n}{e_{t}^{2}}}\\) Interpretation \\(0 ≤ d ≤ 4\\) \\(d = 2\\) - No serial correlation \\(d &lt; 2\\) - Positive serial correlation (\\(d &lt; 1\\) is interpreted as serious positive serial correlation and thus concerning) \\(d &gt; 2\\) - Negative serial correlation Significance testing Statistical significance of the Durbin-Watson statistic us often sort through assessing whether the observed Durbin-Watson statistic is inside or outside the critical boundary in the sampling distribution under the null hypothesis The critical boundary is a function of sample size and the number of variables Breusch-Godfrey test (Breusch &amp; Godfrey, 1978) Concept Aka auxiliary regression (A regression for supplementary purposes) Tests the extent to which the errors given x can be predicted by the predictor in the model and the model error at a value of x that is 1 less, model error at a value of x that is 2 less, model error at a value of x that is 3 less, and so on (these are the lagged variables, and the number of lagged variables is defined by the researcher, but some people may include lagged variables until the errors in that auxiliary regression is no longer autocorrelated) Mathematics \\(e_{t} = \\beta_{0} + \\beta_{1}x_i + \\beta_{2}e_{t-1} + \\beta_{3}e_{t-2} + \\beta_{4}e_{t-3} + \\cdots + \\beta_{p+1}e_{t-p} + u_i\\) Null Hypothesis Significance Test The Breusch-Godfrey test statistic Mathematics (wiki) \\(nR^2\\) Distribution of the Breusch-Godfrey test statistic \\(nR^2 \\sim{} \\chi_{p}^{2}\\) Warning A source (zed statistics) said it is \\((n-p)R^2 \\sim{} \\chi_{p}^{2}\\) But they are very similar, so I think it’s fine Hypotheses \\(H_0:\\) There is no autocorrelation \\(H_1:\\) There is autocorrelation in the lagged variables in the model Assessing Normality Ways to assess normality Plots Statistical Tests Plots Types of plots Frequency Distribution plot (e.g. histogram) Theoretical vs Observed distribution plots Theoretical vs Observed Distribution plots Description A plot with the distributional points of a particular distribution of interest and the distributional points of the data It visualises how well the distributional point of each of the data points fit their corresponding distributional point a particular distribution of interest (e.g. a normal distribution) The corresponding distributional point is the expected value that the observed score should have in a particular distribution of interest Types P-P plot Q-Q plot P-P plot Description A plot with the probability of each of the points of a particular distribution of interest against the probability of each of the data points in the data (relative to the whole data) P-P plot for the normal distribution Concept A plot with the probability of each of the points of a normal distribution against the probability of each of the data points in the data (relative to the whole data) Q-Q plot Description A plot with the quantile or z-score of each of the points of a particular distribution of interest against the quantile or z-score of each of the data points in the data (relative to the whole data) Detrending (Thode, 2002) Concept Detrending - Transforming the plots such that the line showing the theoretical distributional values is horizontal and not diagonal The diagonal arrangement of Q-Q and P-P plots can lead to visual bias, detrending the plots can reduce visual bias Visualisation Q-Q plot for the normal distribution Concept A plot with the quantile or z-score of each of the points of a normal distribution against the quantile or z-score of each of the data points in the data (relative to the whole data) Visualisation # Q-Q plot (general) ggplot2::ggplot(data = iris, aes(sample = Sepal.Length)) + &nbsp&nbsp qqplotr::stat_qq_point(size = 1, alpha = 0.6, detrend = FALSE) + &nbsp&nbsp qqplotr::stat_qq_line(colour = \"#008898\", detrend = FALSE) + &nbsp&nbsp qqplotr::stat_qq_band(fill = \"#008898\", alpha = 0.3, detrend = FALSE) + &nbsp&nbsp labs(x = \"Theoretical Quantile\", y = \"Observed Quantile\") + &nbsp&nbsp theme_minimal() &nbsp # Q-Q plot for Model residual plot(model_object, which = 2) Statistical Tests Statistical Tests Kolmogorov-Smirnov test Shapiro-Wilk test Kolmogorov-Smirnov test Description Tests the null hypothesis that the scores are normally distributed It compares the scores to a normally distributed set of scores with the same mean and standard deviation The problem with statistical tests Statistical significance does not necessarily mean practical importance Statistical significance tells how real an effect is but does not tell the practical importance/size of the effect It doesn’t work when you need it, but works when you don’t need it (that’s what she said) The smaller the sample size, the lower the power of statistical tests, the less likely normality problems are detected by these statistical tests, however, this is when the assumption of normality is more of a concern The larger the sample size, the higher the power of statistical tests, the more likely normality problems are detected by these statistical tests, however, this is when the assumption of normality is less of a concern due to the CLT Robust methods are the way to go Violation of the normality assumption can be adjusted, hence, we don’t have to drill on statistical tests (comparing non-robust result with robust result as a sensitivity analysis may be a better option) Robust Techniques Robust Techniques Trimming Winsorizing The Bootstrap Robust Estimators Robust Estimators M-estimators L-estimators S-estimators R-estimators Extremum estimator (\\(\\hat\\theta\\)) Description Extremum Estimator - A class of estimators in which the parameters are estimated through maximisation or minimisation of a certain obective function that depends on the data The theory of extremum estimators was developed by Amemiya (1985) M-estimators Description M-estimators - Extremum estimators for which the obejct function is a sample average History These are called M-estimators because Maximum likelihood is one of the first of these estimators, hence, Huber (1981, p. 43) called these estimators as M-estimators to denote that these are “maximum likelihood-type” estimators Examples Least squares - Because the estimator is defined as a minimum of the sum of the squared errors/residuals Maximum Likelihood- Because it maximises the likelihood function over the parameter space Trimming Description Deleting scores from the extremes Robust methods tend to use trimming (e.g. trimmed mean, the median, M-estimators, etc.) Trimming can be baesd on percentage or standard deviation of the scores relative to the data Standard deviation-based trimming is a bad idea (Field, 2023) Percentage-based trimming Description Deleting certain percentage of data points from each of the extremes before any statistic is calculated 10% trimming Deleting 10% of the data points from each of the extremes If the sample size is 100, then 10 of the data points at the higher end (because 0% of 100 is 10) and 10 data points at the lower end will be deleted The median is a trimmed mean The median is the mean when all but the middle data point are trimmed A trimmed statistic A statistic that is calculated on the trimmed data (e.g. trimmed mean) Winsorizing Description Arranging the data in ascending order and replacing values outside a threshold boundary (outside of which is considered extremes of the data) woth the value at the threshold boundary Defining the threshold boundary Percentage-based threshold boundary Description The threshold boundary is determined by a percentage of the data Example The data 9, 19, 22, 4, 8, 21, 9, 10, 0, 0, 1, 17, 4, 19, 13, 11, 0, 5, 3, 4 Arrange in ascending order 0, 0, 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 21, 22 Winsorize 5% Winsorization The bottom 5% and upper 5% of the data is replaced 5% of a data with n = 20 is 1 Hence, the bottom first data point is replaced with the value after it, and the upper first data point is replaced with the value before it 0, 0, 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 21, 21 10% Winsorization The bottom 10% and upper 10% of the data is replaced 10% of a data with n = 20 is 2 Hence, the bottom 2 data points are replaced with the value after the second data point, the upper 2 data points are replaced with the value before the boundary 0, 0 , 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 19, 19 20% Winsorization The data points in the bottom 20% and those in the upper 20% of the data is replaced 20% of the data of N = 20 is 4 Hence the bottom 4 data points are replace with the value at the lower boundary, the upper 4 data poitns are replaced with the value at the upper boundaryy 3, 3, 3, 3 , 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 17, 17, 17, 17 Standard-deviation based threshold boundary The threshold boundary is determined by the standard deviation of the data The scores at the are replaced by the unstandardised score that has a standardised score at the boundary (not repeating the actual score at the boundary as in trimming) Theil-Sen Estimator Description Robust regarding outliers among the predictor variables Mathematics \\(\\hat\\beta_{p} = \\tilde{\\beta_{p,ij}}\\) (need to double check) "],["generalised-least-squares.html", "Chapter 2 Generalised Least Squares", " Chapter 2 Generalised Least Squares Slope Estimation Description Mathematics Mathematics (Matrix) \\(\\hat{\\mathbf{\\beta}} = (\\mathbf{X&#39;V^{-1}X})^{-1}\\mathbf{X&#39;V^{-1}Y}\\) Where \\((\\mathbf{X&#39;V^{-1}X})^{-1}\\) - The sampling error variance-covariance ## Saving 2500 x 1500 px image ## Saving 2500 x 1500 px image ## Saving 2500 x 1500 px image ## Warning: Removed 1 rows containing missing values (geom_text). ## Saving 2500 x 1500 px image Expectation Expectation of a random variable Concept weighted by its probability or proportion of occurrence/proportion relative to the entire population or density of 1 The expectation is the area under the function in which the value of the random variable is weighted by its probability (or proportion of occurrence relative to the entire population of density of 1) (the area of the function \\(f(x) = xp(x)\\)) The function produces a distribution with weighted value against the value of the random variable. The distribution is made up of many individual columns with each for each of the possible values of the random variable. The heights of the columns represent the weighted values of their corresponding values of the random variable and all columns have a width of 1. This means that the area of each column simply represents the weighted value of a particular value of the random variable. The also means that the sum of the area of all the columns, which is the area of the distribution, is the weighed average or weighted sum of that random variable, which is the expectation of the random variable. Hence, the expectation of a random variable is found through the area of the function \\(f(x) = xp(x)\\) Expectation of a discrete random variable Concept In the discrete case, the function produces a discrete distribution with weighted value against the discrete random variable, which is like a histogram as opposed to a continuous curve like a pdf. The expectation is found through summation where the areas of all the columns are summed. Hence, it really is just the sum of the function \\(f(x) = x_ip(x_i)\\) for all possible values of the discrete random variable and can be viewed as the weighted average or weighted sum. (when viewed as the weighted average, it is the sum of all possible values of the discrete random variable weighted by its number of occurrence and have the entire sum divided by the sum of all the weighteds; when viewed as the weighted sum, it is the sum of each possible value of the discrete random varible weighted by its probability or proportion of occurrence relative to the entire population or density of 1) Mathematics \\(\\displaystyle \\text{E}(X) = \\sum_{i = 1}^{k}{x_i p(x_i)}\\) Where \\(x\\) - A discrete random variable (e.g. the number of the success event in a binomial distribution) \\(x_i\\) - A discrete random variable at a particular discrete value \\(i\\) \\(p(x_i)\\) - The proportion of the discrete random variable at a particular value \\(i\\) Examples Example 1 Scenario Let \\(X\\) be a discrete random variable, specifically, the number of cases of theft at a Sainsbury’s store per day Histogram Turn the frequency to proportions The expected value \\(\\begin{aligned}\\displaystyle \\text{E}(X) &amp;= \\sum_{i = 1}^{k}{x_i p(x_i)}\\\\ &amp;= 0(\\frac{3}{100}) + 1(\\frac{5}{100}) + 3(\\frac{10}{100})+ 3(\\frac{15}{100}) + 4(\\frac{20}{100}) + 5(\\frac{25}{100}) + \\\\ &amp;~~~~~5(\\frac{10}{100}) + 7(\\frac{5}{100}) + 8(\\frac{3}{100}) + 9(\\frac{3}{100}) + 10(\\frac{1}{100}) \\\\ &amp;= 0(.03) + 1(.05) + 3(.10)+ 3(.15) + 4(.20) + 5(.25) + \\\\ &amp;~~~~~5(.10) + 7(.05) + 8(.03) + 9(.03) + 10(.01) \\\\ &amp;= 4.31 \\end{aligned}\\) Example 2 Scenario There are 3 possible categories of marks for a final exam, 60, 70, and 80. Out of the cohort of 100 students, 60 achieved a mark of 60, 30 achieved 70, and 10 achieved 80. Turn the frequency to proportions The expectation \\(\\begin{aligned} \\displaystyle \\text{E}(M) &amp;= \\sum_{i = 1}^{K}{x_i p(x_i)} \\\\ &amp;= 60\\left(\\frac{6}{10}\\right) + 70\\left(\\frac{3}{10}\\right) + 80\\left(\\frac{1}{10}\\right) \\\\ &amp;= 60(.60) + 70(.30) + 80(.10) \\\\ &amp;= 65\\end{aligned}\\) Notes It is the mean version for the discrete case This is same as the mean in the discrete case where the expectation is expressed as a weighted average weighted on the proportion of each of the values instead of the sum of each of the observation over the total Mathematics \\(\\begin{aligned} \\displaystyle \\text{E}(M) &amp;= \\frac{1}{N}\\sum_{i = 1}^{n}{x_i} \\\\ &amp;= \\frac{1}{10}\\left[ 60 + 60 + 60 + 60 + 60 + 60 + 70 + 70 + 70 + 80 \\right] \\\\ &amp;= \\frac{1}{10} \\left[ 60(6) + 70 (3) + 80(1)\\right] \\\\ &amp;= 60\\left(\\frac{6}{10}\\right) + 70\\left(\\frac{3}{10}\\right) + 80\\left(\\frac{1}{10}\\right)\\end{aligned}\\) Which is just $ x_1p(x_1) + x_2p(x_2) + x_3p(x_3) = _{i = 1}^{K}{x_i p(x_i)} $ Expectation of a continuous random variable Concept It is the centre of mass of the density The weighted sum of a continuous variable - The sum of each possible value of the continuous random variable weighted by its probability or proportion of occurrence/proportion in the entire density or population of 1 (which can be based on its pdf) It is the area under the curve of the graph with probability of occurrence against each possible value of x Since this is a continuous case, integration is used to take into account all possible values. In some cases, the expectation can be converted into its summation form Mathematics \\(\\displaystyle \\text{E}(X) = \\int_{-\\infty}^{\\infty}{xf(x)} ~ dx\\) Where \\(X\\) is a continuous random variable with pdf \\(f(x)\\) Expectation of functions of random variables Expectation of functions of discrete random variables Mathematics \\(\\text{E}(g(X)) = \\sum_{x}^{}{g(x)p(x)}\\) Proof Expectation of functions of continuous random variables Mathematics \\(\\displaystyle \\text{E}(g(X)) = \\int_{-\\infty}^{\\infty}{g(x)p(x)} ~ dx\\) "],["mathematics.html", "Chapter 3 Mathematics", " Chapter 3 Mathematics Mapping Concept Mapping is the process of pairing elements from the input set with elements from the output set based on certain criteria Types of mapping One-to-One One element from the input set is mapped onto one element from the output set One input has one output One value of x has one value of y Many-to-One Multiple elements from the input set is mapped onto one element from the output set Multiple inputs have one output Multiple values of x have one value of y One-to-Many The element from the input set is mapped onto multiple elements of the output set One input has multiple outputs The value of x has multiple values of y Many-to-Many Multiple elements of the input set is mapped onto multiple elements of the output set Multiple inputs have multiple outputs Multiple values of x have multiple values of y Testing the type of mapping of a mathematical relational expression Testing for one-to-many The vertical line test Place a vertical line on the graph of the mathematical expression and see if they intersect with each other more than once If they intersect with each other more than once, then the mathematical expression has a one-to-many mapping Testing for many-to-one The horizontal line test Place a horizontal line on the graph of the mathematical expression and see if they intersect with each other more than once If they intersect with each other more once, then the mathematical expression has a many-to-one mapping Testing for many-to-many The horizontal and vertical line test Basically using both the horizontal and vertical line tests Functions Concept A mathematical relational expression that has a one-to-one or many-to-one mapping (that maps one or more values of a set of inputs to a single output) Mathematical relational expressions that have a one-to-many or many-to-many are not functions Components of a function Variables Concept Aka indeterminates Variable - Anything that can vary in quantity A function consists of variables of which the output is a function (in other words, a function consists of variables that varies with the outcome variable) Coefficient Concept Coefficients - Constants that act as multipliers for the variables The intercept Concept The intercept is the coefficient for the variable with an exponent of 0 What the intercept tells us?? In general sense The intercept defines the change on the y-axis from the x-axis in addition to the terms with variables in the function. In terms of graphical transformation, the intercept defines how the graph of the function is shifted on the y-axis from the x-axis In terms of the intercept The intercept defines the y value when x = 0 (or the value of \\(f(0)\\)). In terms of graphical transformation, the intercept defines the value of y in the coordinate of the graph that touches the y-axis Related concepts Domain of a function Range of a function Root(s) of a function Inverse of a function Extrema of a function Domain of a function The set of possible inputs for a function or the the set of possible values for the variables in the function Range of a function The set of possible outputs of a function Root of a function The values of x for which the output is 0 (\\(f(x) = 0\\)) Graphically, it is the x-coordinate of the point(s) of the graph of the function that intersect(s) with the x-axis Intercept of a function The values of y for which the input is 0 (\\(f(0)\\)) Graphically, it is the y-coordinate of the point of the graph of the function that intersects with the y-axis Inverse of a function - A function that is a reversal of another function (it reverses the subject) Extrema of a function Concept The most extreme value of the output of a function (plural is extrema) Types of Extremum Maximum vs Minimum Maximum The highest possible value of the output of a function Minimum The smallest possible value of the output of a function Global vs Local Local extrema The extrema of the function within a certain range of the domain (the variables) Global extrema The extrema of the function on the entire domain of the function Visualsation Some Useful Functions General Exponential Log Sigmoid Reciprocal Polynomial Functions Linear Quadratic Cubic Quartic Quintic Trigonometric Functions Ordinary Trigonometric Functions Sine Cosine Tangent Hyperbolic Functions Hyperbolic Sine Hyperbolic Cosine Hyperbolic Tangent Probability/Distributional Functions (Probability and Cumulative) Probability Density Function Normal Distribution Chi-squared Distribution Log-Normal Distribution Exponential Distribution F Distribution Probability Mass Function Binomial Distribution Poisson Distribution Bernoulli Distribution Rademacher Distribution Polynomial Functions Concept Polynomial functions - Functions that are made up of the sum of one or more algebraic terms each of which is the product of a coefficient and one or more variables each of which has a non-negative integer exponent The Leading Coefficient - The coefficient of the variable term with the highest degree or the largest exponent in a polynomial function - The leading coefficient in a polynomial function must not be 0 Degree of a polynomial The highest possible degree of the polynomial’s monomial (individual terms) with non-zero coefficients (the degree of a term is the exponent of the variable) Types of Polynomial Function Polynomials by degree A constant (degree of 0) Linear (degree of 1) Quadratic (degree of 2) Cubic (degree of 3) Quartic (degree of 4) Quintic (degree of 5) and so on… The Linear Function The Linear Function Concept The linear function describes the output as a linear function of one variable (variable x) Aka a polynomial with a degree of 1 Mathematics \\(f(x) = ax + b\\) The Linear Equation Concept The univariate univariable linear equation is an equation that describes a variable (variable y) as a linear function of another variable (variable x) Mathematics \\(y = ax + b\\) Where \\(y\\) Variable y (A variable represented on the y-axis) This variable is usually the output or the variable of interest, which is also called the outcome variable or the dependent variable (DV) in mathematical modelling \\(x\\) Variable x (A variable represented on the x-axis) This variable is usually the input, which is also called the predictor variable or the independent variable (IV) in mathematical modelling \\(a\\) The coefficient for \\(x\\) It is the weight of the x variable on the y variable It defines the magnitude of the linear relationship x has on y Graphically, it defines the slope of the line \\(b\\) The intercept The Quadratic Function The Quadratic Function Concept The univariable quadratic function describes the output as a quadratic function of one variable A function with a non-zero coefficient for any variables with an exponent of 2 In the factored form, it is the product of 2 linear factors Mathematics \\(f(x) = ax^{2} + bx + c\\) The Quadratic Equation Concept The univariate univariable quadratic describes a variable (variable y) as a quadratic function of one variable (variable x) Mathematics \\(y = ax^{2} + bx + c\\) Where \\(a\\) Coefficient for \\(x^{2}\\) It is the weight of variable x on variable y It defines the magnitude of the quadratic relationship x has on y Graphically, it defines the slope of the quadratic curve \\(b\\) Coefficient for \\(x\\) \\(c\\) Intercept The Quadratic Graph Real life examples Distance Travelled (\\(s = ut + \\frac{1}{2}at^{2}\\)) Yerkes and Dodson Law (performance is a negative quadratic function of arousal) Bridges Sydney Barbour Bridge (Negative quadratic) Clifton Suspension Bridge (Positive quadratic) The Cubic Function The Cubic Function Concept The univariable cubic function describes the output as a cubic function of one variable (variable x) The cubic function has a non-zero coefficient for any variables with an exponent of 3 In the factored form, it is the product of 3 linear factors or the product of a linear factor and a quadratic factor Mathematics \\(f(x)= ax^{3} + bx^{2} + cx + d\\) The Cubic Equation Concept The univariate univariable cubic equation describes a variable (variable y) as a cubic function of one variable (variable x) Mathematics \\(y = ax^{3} + bx^{2} + cx + d\\) Where \\(a\\) - Coefficient for \\(x^{3}\\) \\(b\\) - Coefficient for \\(x^{2}\\) \\(c\\) - Coefficient for \\(x\\) \\(d\\) - Intercept The Cubic Curve The Quartic Function The Quartic Function Concept The univariable univariate quartic function describes the output as a quartic function of one variable (variable x) The leading coefficient must be non-zero, in quartic functions, it means that the coefficient for any variables with an exponent of 4 must be non-zero In the factored form, it can be the product of 4 linear factors 2 quadratic factors 1 linear factor and 1 cubic factor Mathematics \\(f(x)= ax^{4} + bx^{3} + cx^{2} + dx + e\\) The Quartic Equation Concept The univariate univariable quartic equation describes a variable (variable y) as a quartic function of one variable (variable x) Where \\(a\\) - Coefficient of \\(x^4\\) \\(b\\) - Coefficient of \\(x^3\\) \\(c\\) - Coefficient of \\(x^2\\) \\(d\\) - Coefficient of \\(x\\) \\(e\\) - Intercept The Quartic graph The Quintic Function The Quintic Function Concept The univariable univariate quintic function describes the output as a quintic function of one variable (variable x) The leading coefficient must be non-zero, in quintic functions, this means that the coefficient for any variables with an exponent of 5 must be non-zero In the factored form, it can be the product of 5 linear factors 3 linear factors and 1 quadratic factor 2 linear factors and 1 cubic factor 1 linear factor and 2 quadratic factors 1 linear factor and 1 quartic factors 1 quadratic factor and 1 cubic factor Mathematics \\(f(x) = ax^{5} + bx^{4} + cx^{3} + dx^{2} + ex + f\\) The Quintic Equation Concept The univariate univariable quintic equation describes a variable (variable y) as a quintic function of one variable (variable x) Mathematics \\(y = ax^{5} + bx^{4} + cx^{3} + dx^{2} + ex + f\\) Where \\(a\\) - Coefficient of \\(x^5\\) \\(b\\) - Coefficient of \\(x^4\\) \\(c\\) - Coefficient of \\(x^3\\) \\(d\\) - Coefficient of \\(x^2\\) \\(e\\) - Coefficient of \\(x\\) \\(f\\) - Intercept The Quintic Graph Finding the Intercept of the function Finding the Root(s) of a function Finding the slope of the tangent line of a point of a function Methods Drawing a triangle Differentiation Finding the Extremum of a function Transforming functions ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will ## replace the existing scale. ## Warning: Removed 1 rows containing missing values (geom_segment). ## Removed 1 rows containing missing values (geom_segment). ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 Vectors Vectors A vector is a quantity that quantifies a movement (in other words, it is a mathematical representation of a movement) A vector has a magnitude and a direction A vector can be represented by a directed line segment Modulus of a vector Concept The magnitude of a vector The modulus of vector \\(\\mathbf{v}\\) is usually denoted as \\(|\\mathbf{v}|\\) The modulus of a vector is the length of that vector Finding the length/modulus of a vector Concept The length of a vector with more than 1 dimension can be found by using Pythagoras’ theorem Examples Finding the length of a 2d vector Given that \\(\\mathbf{v}= \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\) \\(|\\mathbf{v}| = \\sqrt{x^{2} + y^{2}}\\) Finding the length of a 3d vector Given that \\(\\mathbf{v}= \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\\) \\(|\\mathbf{v}| = \\sqrt{x^{2} + y^{2} + z^{2}}\\) Components of a vector Concept The vectors that are parallel to the coordinate axes when the vector is drawn on a Cartesian grid that make up the vector of interest Components of a vector are linearly independent to each other They are basically the \\(x\\), \\(y\\), and \\(z\\) in the vector \\(\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\\) Visualisation Zero vector Concept A vector that has zero magnitude and no direction Denoted as \\(\\mathbf{0}\\) Unit vector Concept A vector that has a magnitude of 1 A unit vector is denoted by a hat on the vector (e.g. \\(\\hat{\\mathbf{v}}\\)) Position vectors Concept Vectors that stem from the origin Vectors that represent the position of a point in relation to the origin \\(O\\) A vector that represent the position of a point \\(P\\) in relation to the origin \\(O\\) would be denoted as \\(\\overrightarrow{OP}\\) ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d ## No trace type specified: ## Based on info supplied, a &#39;scatter3d&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter3d Resultant vector - A vector that summaries a set of vectors - It is the vector that is the combined result of multiple vectors - - The triangle law for vector addition - Concept - When 3 vectors are joined with the end of each vector linking with the end of another vector such that a closed triangle is formed, any one of those vectors is the combination of the other 2 vectors - Hence, the resultant vector of 2 vectors that are joined end-to-end (or successively put together) is the vector that would form a triangle when it is linked up with those two vectors - Mathematics - \\(\\overrightarrow{AB} + \\overrightarrow{BC} = \\overrightarrow{AC}\\) - Where - \\(\\overrightarrow{AC}\\) - The resultant vector - Graph - The polygon law of vector addition Concept The resultant vector of 3 or more vectors is a vector that would form a closed polygon when it is linked up with the vectors that have been successively put together You are basically applying the triangle law multiple times in succession Mathematics \\(\\overrightarrow{AB} + \\overrightarrow{BC} + \\overrightarrow{CD}= \\overrightarrow{AD}\\) Visualisation Explanation \\(\\mathbf{r}\\) is the resultant vector of vector \\(\\mathbf{k}\\) and \\(\\mathbf{c}\\) \\(\\mathbf{r} = \\mathbf{k} + \\mathbf{c}\\) \\(\\mathbf{k}\\) in turn is the resultant vector of vector \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) \\(\\mathbf{k} = \\mathbf{a} + \\mathbf{b}\\) Hence \\(\\mathbf{r} = \\mathbf{k} + \\mathbf{c}\\) \\(\\mathbf{r} = (\\mathbf{a} + \\mathbf{b}) + \\mathbf{c}\\) \\(\\mathbf{r} = \\mathbf{a} + \\mathbf{b} + \\mathbf{c}\\) Scalars Scalar Concept Any numbers that are used to multiply another thing (e.g. variables or vectors) It is called scalar because it is used to multiply or “scale’ other things (e.g. variables or vectors) It can also refer to any quantities that only has a magnitude or can be fully described using a magnitude (e.g. speed, mass) Unit vectors parallel to the coordinate axes Concept Unit vectors that are parallel to the coordinate axes \\(\\mathbf{i} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\) is a unit vector that is parallel to the x-axis \\(\\mathbf{j} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) is a unit vector and is parallel to the y-axis \\(\\mathbf{k} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\) is a unit vector and is parallel to the z-axis Hence, a 2-D vector \\(\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\) can be written as \\(x\\mathbf{i} + y\\mathbf{j}\\) and a 3-D vector \\(\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\\) can be written as \\(x\\mathbf{i} + y\\mathbf{j} + z\\mathbf{k}\\) Finding a unit vector parallel to a vector of interest Concept A unit vector parallel to a vector of interest is the vector per the length of the vector Basically, you are scaling that vector such that it has a length of 1 (this is done by standardising the original vector per unit) Mathematics A unit vector parallel to vector \\(\\mathbf{v}\\) is \\(\\frac{\\mathbf{v}}{\\mathbf{|v|}}\\) Example Consider a 2D vector \\(\\mathbf{v} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}\\) The unit vector parallel to vector \\(\\mathbf{v}\\) is \\(\\frac{\\mathbf{v}}{\\mathbf{|v|}} = \\frac{\\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}}{5} = \\begin{pmatrix} \\frac{4}{5} \\\\ \\frac{3}{5} \\end{pmatrix}\\) Distance between 2 points Distance between 2 3-D points Mathematics \\(d_{1,2} = \\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}+(z_{1}-z_{2})^{2}}\\) Explanation Imagine that we need to find the distance between the 2 black dots (or the black vector) To find the distance of the black vector, you need to first find the length of the blue and red vectors Finding the length of the blue vector Since the blue vector is parallel to the y-axis, the length of the blue vector can be found by the difference between the y-coordinates of the black dots \\(length~of~blue~vector = y_{1} - y_{2}\\) Finding the length of the red vector Since the red vector is not parallel to any of the axes, to find the length of the red vector, we need to first find the length of the yellow and green vectors so that the length of the red vector can be found by using the Pythagoras’ theorem Finding the length of the yellow vector Since the yellow vector is parallel to the x-axis, it can be found by the difference between the x-coordinates of the black dots \\(length~of~yellow~vector = x_{1} - x_{2}\\) Finding the length of the green vector Since the green vector is parallel to the z-axis, it can be found by the difference between the z-coordinates of the black dots \\(length~of~green~vector = z_{1} - z_{2}\\) Now that we know the length of the yellow and green vectors, we can apply the Pythagoras’ theorem to find the length of the red vector \\(length~of~red~vector = \\sqrt{(length~of~yellow~line)^2+(length~of~green~vector)^2}\\) \\(length~of~red~vector = \\sqrt{(x_{1} - x_{2})^2+(z_{1} - z_{2})^2}\\) Now that we know the length of the red vector and the blue vector we found earlier, we can apply the Pythagoras’ theorem to find the black vector \\(length~of~black~vector = \\sqrt{(length~of~red~line)^2+(length~of~blue~vector)^2}\\) \\(length~of~black~vector = \\sqrt{(\\sqrt{(x_{1} - x_{2})^2+(z_{1} - z_{2})^2})^2+(y_{1} - y_{2})^2}\\) \\(length~of~black~vector = \\sqrt{(x_{1} - x_{2})^2+(z_{1} - z_{2})^2+(y_{1} - y_{2})^2}\\) \\(length~of~black~vector = \\sqrt{(x_{1} - x_{2})^2+(y_{1} - y_{2})^2+(z_{1} - z_{2})^2}\\) Other concepts Two vectors are equal if have the same magnitude and direction One vector is the negative of another vector if they have the same magnitude but opposite direction One vector is a scalar multiple of another vector if they have the same direction but different magnitude (they are scaled versions of each other) Collinear points are points that line on the same straight line ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## &#39;expression&#39; ## quartz_off_screen ## 2 Dot product of 2 vectors Concept The dot product basically quantifies the extent to which 2 vectors interact with each other (or how much each of the two vectors reinforce each other in their magnitude) When the vectors are orthogonal to each other, in other words, they don’t interact with each other, then they have a dot product of \\(0\\) (because when 2 vectors are orthogonal, \\(\\theta = 90\\), and \\(\\cos{90} = 0\\), therefore, the dot product is \\(0\\) ) When 2 vectors are parallel to each other, in other words, they full interact with each other, then they have a dot product of \\(|\\mathbf{a}||\\mathbf{b}|\\) Mathematics The usual form \\(\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}||\\mathbf{b}|\\cos{\\theta}\\) The component form \\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{d}{a_{d}b_{d}}\\) \\(\\mathbf{a} \\cdot \\mathbf{b} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + \\cdots + a_{d}b_{d}\\) Where \\(a_{d}\\) - The magnitude of vector \\(\\mathbf{a}\\) in the direction of the d axis \\(b_{d}\\) - The magnitude of vector \\(\\mathbf{b}\\) in the direction of the d axis \\(a_{d}b_{d}\\) - The combined magnitude of vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of the d-axis \\(d\\) - The total number of dimensions Explanation Given 2 2-D vectors, vector \\(\\mathbf{a}\\) and vector \\(\\mathbf{b}\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{b}\\) The magnitude of \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{b}\\) (\\(\\mathbf{a}_{\\mathbf{b}}\\)) can be found using the ratio of \\(\\cos{}\\) \\(\\mathbf{a}_{\\mathbf{b}} = |\\mathbf{a}|\\cos{\\theta}\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{b}\\) is the multiplicative effect between the magnitude of \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{b}\\) (or \\(\\mathbf{a}_{\\mathbf{b}}\\)) and the magnitude of \\(\\mathbf{b}\\) \\(\\mathbf{a}\\cdot\\mathbf{b} = |\\mathbf{a}|\\cos{\\theta} |\\mathbf{b}|\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\) The magnitude of \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\) (\\(\\mathbf{b}_{\\mathbf{a}}\\)) can be found using the ratio of \\(\\cos{}\\) \\(\\mathbf{b}_{\\mathbf{a}} = |\\mathbf{b}|\\cos{\\theta}\\) The combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\) is multiplicative effect between the magnitude of \\(\\mathbf{a}\\) in the direction of \\(\\mathbf{a}\\) (or \\(\\mathbf{b}_{\\mathbf{a}}\\)) and the magnitude of \\(\\mathbf{a}\\) \\(\\mathbf{a}\\cdot\\mathbf{b} = |\\mathbf{b}|\\cos{\\theta}|\\mathbf{a}|\\) Duality We can see that the combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{b}\\) is exactly the same as the combined magnitude of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in the direction of \\(\\mathbf{a}\\). Hence, the order does not matter (\\(\\mathbf{a}\\cdot\\mathbf{b} = \\mathbf{b}\\cdot\\mathbf{a}\\)) Alternative thinking \\(|\\mathbf{a}||\\mathbf{b}|\\) is the maximum possible amount of interaction between the 2 vectors \\(\\cos{\\theta}\\) is like a quantity that quantifies how much the 2 vectors project to the same direction as each other. This quantity ranges from -1 to +1. +1 means that the 2 vectors are fully in the same direction. When the 2 vectors are entirely in the same direction, then \\(\\theta = 0\\) and \\(\\cos{90} = 1\\) When the 2 vectors are entirely in the opposite direction, then \\(\\theta = 180\\) and \\(\\cos{180} = -1\\) When the 2 vectors are orthogonal to each other, then \\(\\theta = 90\\) and \\(\\cos{90} = 0\\) \\(|\\mathbf{a}||\\mathbf{b}| \\cos{\\theta}\\) is like scaling \\(|\\mathbf{a}||\\mathbf{b}|\\) by the factor \\(\\cos{\\theta}\\) such that the result reflects the amount of interaction between the 2 vectors When \\(\\cos{90} = 1\\), then you get the maximum possible amount of interaction When \\(\\cos{180} = -1\\), then you get maximum possible amount of interaction When \\(\\cos{0} = 0\\), then you 0 percent of that interaction Other tips \\(\\mathbf{a} \\cdot (\\mathbf{b} + \\mathbf{c}) = \\mathbf{a} \\cdot\\mathbf{b} + \\mathbf{a} \\cdot\\mathbf{c}\\) "],["anova.html", "Chapter 4 ANOVA", " Chapter 4 ANOVA ANOVA Analysis of Variance (ANOVA) Concept Under the null hypothesis, it assumes that each of the groups is an independent random sample from the population ANOVA involves assessing the agreement between two estimators of the population variance The Statistical Model of ANOVA Concept The value of observation \\(i\\) on outcome variable \\(y\\) is the grand mean plus the effect of the grouping variable it is in plus random error Mathematics \\(y_{i,k} = \\mu + \\tau_{k} + \\epsilon_{i, k}\\) Where \\(y_{i,k}\\) - The value of the outcome variable \\(y\\) of observation \\(i\\) in population \\(k\\) \\(\\mu\\) - The overall/grand mean of the outcome variable \\(y\\) (the mean of the superpopulation) \\(\\tau_{k}\\) The deviation of population \\(k\\) from the overall mean (\\(\\mu\\)) The nonrandom effect of the group Group or sample \\(k\\) is assumed to be sampled from this population \\(k\\) Since they are normalised, \\(\\sum_{k=1}^{k}{\\tau_k} = 0\\) \\(\\mu + \\tau_{k}\\) The mean of population \\(k\\) The expected value of observation \\(i\\) in population \\(k\\) (\\(y_{i,k}\\)) \\(\\epsilon_{i,k}\\) The random error for observation \\(i\\) in population \\(k\\) The difference between an observed value (\\(y_{i,k}\\)) and its expected value (\\(\\mu + \\tau_{k}\\)) Assuming that \\(y_{ik}\\) is a random independent normal variable with \\(E(y_{ik}) = 0\\) and \\(Var(y_{ik}) = 0\\), since the error term is the difference between \\(y_{i,k}\\), a normally distributed random variable with variance of \\(\\sigma^2\\), and its mean, this follows that the errors are also a random independent normal variable with \\(E(\\epsilon_{ik}) = 0\\) (because subtracting each of the errors by its mean will normalize it to 0) and \\(Var(\\epsilon_{i,k}) = \\sigma^2\\) Hypothesis Testing for ANOVA Conceptual Hypotheses in ANOVA Null hypothesis \\(H_0:\\) All populations are the same (are from the same superpopulation) \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\) Alternative hypothesis \\(H_1:\\) The populations are different \\(H_0: \\mu_1 ≠ \\mu_2 ≠ \\cdots ≠ \\mu_k 0\\) Operationalising the null hypothesis to be tested General Concept The operationalisation of the ANOVA null hypothesis is based on the agreement/disaggreement between 2 estimators of the variance of the superpopulation (\\(\\sigma^@\\)) (or it’s called common variance) Estimating the population variance \\(\\sigma^2\\) There are 2 estimators of \\(\\sigma^2\\) \\(MS_W\\) \\(MS_B\\) The \\(MS_W\\) estimator Concept \\(SS_W\\), and thus also \\(MS_W\\), are statistics derived from multiple independent random samples, hence, \\(SS_W\\), and also \\(MS_W\\), are independent random variables And the expectation of \\(MS_W\\) is \\(\\sigma^2\\) Mathematics \\(E(MS_W) = \\sigma^{2}\\) Implications for \\(MS_W\\) as an estimator of \\(\\sigma^2\\) Hence, the observed \\(MS_W\\) is an unbiased estimator of \\(\\sigma^2\\) The \\(MS_B\\) estimator Concept \\(SS_B\\) and \\(MS_B\\) are statistics derived from multiple independent random samples, hence, \\(SS_B\\) and \\(MS_B\\) are independent random variables \\(MS_B\\) is an independent random variable that has a Chi-squared distribution with an expectation of \\(\\sigma^2 + (\\frac{1}{k-1})\\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2}\\) Mathematics \\(\\displaystyle E\\left( MS_B \\right) = \\sigma^2 + \\left(\\frac{1}{k-1}\\right)\\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2}\\) Notes See proof below Implication for \\(MS_B\\) as an estimator of \\(\\sigma^2\\) Under the null hypothesis, that is when all \\(\\tau_ks = 0\\), this linear combination \\(\\left(\\frac{1}{k-1}\\right)\\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2}\\) is \\(0\\), therefore, \\(\\displaystyle E\\left( MS_B \\right) = \\sigma^2 + 0\\), which means that the observed \\(MS_B\\) is an unbiased estimator of \\(\\sigma^2\\) If the null hypothesis is false, that is when at least one of the \\(\\tau_ks ≠ 0\\), the linear combination of \\(\\tau_ks\\) (\\(\\left(\\frac{1}{k-1}\\right)\\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2}\\)) will be larger than \\(0\\), hence, \\(MS_B\\) is a positively biased estimator of \\(\\sigma^2\\) (it will tend to overestimate \\(\\sigma^2\\)) The test statistic: The F ratio Concept The conceptual null hypothesis of ANOVA (\\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\cdots = \\mu_k\\)) is formally operationalised as \\(H_0: \\sum_{k=1}^{k}{\\tau_k^2} = 0\\) and this is tested by comparing the \\(MS_B\\) estimator to the \\(MS_W\\) estimator as a ratio of \\(\\frac{MS_B}{\\sigma^2}\\) to \\(\\frac{MS_W}{\\sigma^2}\\) This ratio of \\(MS_B\\) to \\(MS_W\\) is the test statistic for the null hypothesis of ANOVA and this test statistic is called the \\(F\\) ratio (or \\(F\\) statistic) Mathematics \\(F = \\frac{MS_B}{MS_W}\\) Interpretation If the null hypothesis is true, that is \\(\\sum_{k=1}^{k}{\\tau_k^2} = 0\\), the expected value of \\(E(MS_B)\\) is just the random variation in the superpopulation (\\(E(MS_B) = \\sigma^2\\)), and \\(F = \\frac{E(MS_B)}{E(MS_W)} = \\frac{\\sigma^2}{\\sigma^2} = 1\\). Using the sample \\(MS_B\\) and \\(MS_W\\) as estimators of \\(E(MS_B)\\) and \\(E(MS_B)\\) respectively would yield an \\(F\\) statistic that is close to 1 If the null hypothesis is false, that is when at least one of the \\(\\tau_ks &gt; 0\\) (\\(\\sum_{k=1}^{k}{\\tau_k^2} &gt; 0\\)), in other words, when at least one of the populations is different, the expected value \\(E(MS_B)\\) consists of both the random variation in the superpopulation and the overall variation induced by the groups (\\(E(MS_B) = \\sigma^2 + \\left(\\frac{1}{k-1} \\right)\\sum_{k=1}^{k}{n_k\\tau_k^2}\\)), then \\(F = \\frac{E(MS_B)}{E(MS_W)} = \\frac{\\sigma^2 + \\left(\\frac{1}{k-1} \\right)\\sum_{k=1}^{k}{n_k\\tau_k^2}}{\\sigma^2}\\), where \\(\\left(\\frac{1}{k-1} \\right)\\sum_{k=1}^{k}{n_k\\tau_k^2} &gt; 0\\), the \\(F\\) statistic will be larger than 1 NHST The null hypothesis of ANOVA is thus tested by an Upper-Tail \\(F\\) test The distribution of the F-statistic Concept Since \\(MS_B\\) is \\(SS_B\\) (a chi-squared-distributed independent random variable) divided by its degrees of freedom, and \\(MS_W\\) is \\(SS_W\\) (another chi-squared-distributed random variable) divided by its degrees of freedom, the ratio of \\(MS_B\\) to \\(MS_W\\) has a Central \\(F\\) distribution, that is an \\(F\\) distribution with a degrees of freedom for the numerator of \\(v_1 = k-1\\) and a degrees of freedom for the denominator of \\(v_2 = N - k\\) for the denominator Mathematics \\(F_{central} \\sim F\\left(k - 1, N - k \\right)\\) The F test Concept The process of assessing the probability of an F statistic as extreme as or more extreme than the observed F statistic in the Central F distribution NHST for ANOVA Hypotheses Null hypothesis Concept There are no difference between the means The samples are from the same population Mathematics \\(H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\cdots = \\mu_k\\) or \\((\\tau_k - \\bar{\\tau})^2 = 0\\) Alternative hypothesis Concept At least one of the samples is likely from another population Mathematics \\(H_1: \\mu_1 ≠ \\mu_2 ≠ \\mu_3 ≠ \\cdots ≠ \\mu_k\\) or \\((\\tau_k - \\bar{\\tau})^2 &gt; 0\\) NHST The null hypothesis is tested by an upper-tailed \\(F\\) test The null hypothesis is tested by assessing the probability of getting an F-statistic equal to or larger than the observed \\(F\\) statistic in the Central \\(F\\) distribution The F test for 2 samples is equivalent to the t test \\(H_0: \\sum_{k=1}^{k}{\\tau_k^2} = 0\\) (operationally) - Hypothesis Testing for ANOVA Total Sum of Squares Concept The Sum of Squares that represents the total amount of variation in the outcome variable It is the sum of all the squared deviations between each observed value and the grand mean Visualisation Mathematics \\(\\displaystyle \\begin{alignat*}{10} {SS_{T}} = {\\sum_{k=1}^{k}{\\sum_{i=1}^{n_k}{(y_{i,k} - \\bar{y})^2}}} \\end{alignat*}\\) Where \\(k\\) represents group \\(k\\) \\(i\\) represents observation \\(i\\) \\(y_{i,k}\\) - Observation \\(i\\) in group \\(k\\) \\(\\bar{y}\\) - The grand mean Notes Alternatively, it can be expressed simply as: \\(\\displaystyle \\sum_{n=1}^{n}{(y_i - \\bar{y})^2}\\) Total degrees of freedom \\(\\begin{aligned}df_T &amp;= df_M + df_E \\\\ df_T &amp;= N-1 \\end{aligned}\\) Mean Total Sum of Squares (Total Variance) Concept Total Variance in the outcome variable The mean amount of variation in the outcome variable The average amount by squared deviation between each observation and the grand mean It is the standardised version of the Total Sum of Squares Because the Total Sum of Squares is the sum of all the squared deviations, it depends on the degrees of freedom. Finding the mean, specifically, dividing the Total Sum of Squares by the degrees of freedom will standardise the quantity Mathematics \\(MS_T = \\frac{SS_T}{df_T}\\) ANOVA and variance partitioning Concept ANOVA partitions the Total Sum of Squares of the outcome variable into 2 components: The Model Sum of Squares The Error Sum of Squares Mathematics \\(\\displaystyle \\begin{alignat*}{10} {SS_{T}} &amp;= {SS_{M}} + {SS_{E}} \\\\ {\\sum_{k=1}^{k}{\\sum_{i=1}^{n_k}{(y_{i,k} - \\bar{y})^2}}} &amp;= {\\sum_{k = 1}^{k}{n_k(\\bar{y}_k - \\bar{y})^2} + {\\sum_{k=1}^{k}{\\sum_{i=1}^{n_k}{(y_{i,k} - \\bar{y}_k})^2} }} \\end{alignat*}\\) Between/Model Sum of Squares Concept The amount of variation between groups/samples/populations In an experimental design, it represents the amount of variation in the outcome variable that is induced or explained by the model in the model (systematic variation) It is the sum of all the squared deviations between the predicted value of each observation (which is the mean of the group to which the observation belongs) and the grand mean Visualisation Mathematics \\({SS_{B}} = {\\sum_{k = 1}^{k}{n_k(\\bar{y}_k - \\bar{y})^2}}\\) Where \\(n_k\\) - Group size \\(\\bar{y}_k\\) - The mean of group $k$0 Distribution of \\(SS_B\\) \\(\\frac{SS_B}{\\sigma^2} \\sim \\chi()\\) Model degrees of freedom \\(df_B = K - 1\\) Distribution of \\(SS_B\\) \\(\\displaystyle \\frac{SS_B}{\\sigma^2} \\sim \\chi \\left(K - 1 \\right)\\) Mean Model Sum of Squares Concept Between-group/sample variance - The variance between groups In a 2-group/sample design, it is the variance between 2 groups Variance in the outcome variable explained by the model The mean amount of variation in the outcome variable explained by the model The average amount of squared deviation between the predicted value of each observation and the grand mean Mathematics \\(MS_M = \\frac{SS_M}{df_M}\\) Error Sum of Squares Concept It represents the amount of variation in the outcome variable that is not explained by the model or attributable to the predictors in the model and is assumed to be due to random error It is the sum of all the squared deviations between each observed value and the mean of the group to which it belongs Visualisation Mathematics \\({SS_{E}} = {\\sum_{k=1}^{k}{\\sum_{i=1}^{n_k}{(y_{i,k} - \\bar{y}_k})^2}}\\) Notes Alternatively, it is the sum of the variance of each group times the degrees of freedom of that group (so you get the error sum of squares of that group) \\(\\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\\) Degrees of freedom \\(df_e = N-k\\) Distribution of \\(\\frac{SS_W}{\\sigma^2}\\) \\(\\displaystyle \\frac{SS_W}{\\sigma^2} \\sim \\chi \\left(N - K \\right)\\) Mean Error Sum of Squares (Variance Unexplained/Error Variance) Concept Variance in the outcome variable not explained by the model and is assumed to be due to random error The mean amount of variation in the outcome variable not explained by the model The average amount of squared deviation between the predicted value of each observation and its predicted value Mathematics \\(MS_E = \\frac{SS_E}{df_e}\\) Alternatively, it is just the error variance \\(\\displaystyle MS_E = s^2 = \\frac{1}{n-k}\\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\\) The Model as a whole Mathematics \\(\\displaystyle \\begin{alignat*}{10} {SS_{T}} &amp;= {SS_{M}} + {SS_{E}} \\\\ {\\sum_{k=1}^{k}{\\sum_{i=1}^{n_k}{(y_{i,k} - \\bar{y})^2}}} &amp;= {\\sum_{k = 1}^{k}{n_k(\\bar{y}_k - \\bar{y})^2} + {\\sum_{k=1}^{k}{\\sum_{i=1}^{n_k}{(y_{i,k} - \\bar{y}_k})^2} }} \\end{alignat*}\\) Proof of \\(E(MS_B) = \\sigma^2\\) \\(\\displaystyle MS_B = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k \\left(\\bar{y}_{k} - \\bar{y} \\right)^2}\\) Since: \\(\\begin{aligned} \\displaystyle \\bar{y}_{k} &amp;= \\frac{1}{n_k}\\sum_{i = 1}^{n_k}{y_{ik}} \\\\ \\displaystyle &amp;= \\frac{1}{n_k}\\sum_{i = 1}^{n_k}{\\left( \\mu + \\tau_k + \\epsilon_{ik} \\right)} \\\\ \\displaystyle &amp;= \\mu + \\tau_k + \\frac{1}{n_k}\\sum_{i = 1}^{n_k}{\\epsilon_{ik}} \\\\ \\displaystyle &amp;= \\mu + \\tau_k + \\bar{\\epsilon}_k \\end{aligned}\\) \\(\\begin{aligned} \\displaystyle \\bar{y} &amp;= \\frac{1}{N}\\sum_{k = 1}^{k}{\\sum_{i = 1}^{n_k}{y_{ik}}} \\\\ \\displaystyle &amp;= \\frac{1}{N}\\sum_{k = 1}^{k}{\\sum_{i = 1}^{n_k}{\\left( \\mu + \\tau_k + \\epsilon_{ik} \\right)}} \\\\ \\displaystyle &amp;= \\mu + \\frac{1}{N}\\sum_{k = 1}^{k}{n_k \\tau_k} + \\frac{1}{N}\\sum_{k = 1}^{k}{\\sum_{i = 1}^{n_k}{\\epsilon_{ik}}} \\\\ \\displaystyle &amp;= \\mu + \\bar{\\tau} + \\bar{\\epsilon} \\end{aligned}\\) \\(\\displaystyle MS_B = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k \\left(\\bar{y}_{k} - \\bar{y} \\right)^2} \\\\ \\displaystyle MS_B = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k \\left[(\\mu + \\tau_k + \\bar{\\epsilon}_k) - (\\mu + \\bar{\\tau} + \\bar{\\epsilon}) \\right]^2} \\\\ \\displaystyle MS_B = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k \\left[(\\tau_k - \\bar{\\tau}) + (\\bar{\\epsilon}_k - \\bar{\\epsilon}) \\right]^2} \\\\ \\displaystyle MS_B = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k \\left[(\\tau_k - \\bar{\\tau})^2 + 2(\\tau_k - \\bar{\\tau})(\\bar{\\epsilon}_k - \\bar{\\epsilon}) + (\\bar{\\epsilon}_k - \\bar{\\epsilon})^2 \\right]} \\\\ \\displaystyle MS_B = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{ \\left[n_k(\\tau_k - \\bar{\\tau})^2 + 2n_k(\\tau_k - \\bar{\\tau})(\\bar{\\epsilon}_k - \\bar{\\epsilon}) + n_k(\\bar{\\epsilon}_k - \\bar{\\epsilon})^2 \\right]} \\\\ \\displaystyle MS_B = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right)\\sum_{k=1}^{k}{2n_k(\\tau_k - \\bar{\\tau})(\\bar{\\epsilon}_k} - \\bar{\\epsilon}) + \\left( \\frac{1}{k - 1}\\right)\\sum_{k=1}^{k}{n_k(\\bar{\\epsilon}_k - \\bar{\\epsilon})^2} \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) E\\left[\\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2}\\right] + \\left( \\frac{1}{k - 1}\\right) E\\left[\\sum_{k=1}^{k}{2n_k(\\tau_k - \\bar{\\tau})(\\bar{\\epsilon}_k} - \\bar{\\epsilon})\\right] + \\left( \\frac{1}{k - 1}\\right) E\\left[ \\sum_{k=1}^{k}{n_k(\\bar{\\epsilon}_k - \\bar{\\epsilon})^2}\\right]\\) Since: \\(\\tau_k\\) and thus \\(\\bar{\\tau}\\) are constants and \\(E(\\epsilon_{ik}) = E(\\bar\\epsilon_{i}) = E(\\bar\\epsilon) = 0\\) \\(\\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) E\\left[ \\sum_{k=1}^{k}{n_k(\\bar{\\epsilon}_k - \\bar{\\epsilon})^2}\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) E\\left[ \\sum_{k=1}^{k}{n_k(\\bar{\\epsilon}_{k}^{2} - 2\\bar{\\epsilon}_{k}\\bar{\\epsilon} + \\bar{\\epsilon}^{2})}\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) E\\left[ \\sum_{k=1}^{k}{n_k\\bar{\\epsilon}_{k}^{2} - 2n_k\\bar{\\epsilon}_{k}\\bar{\\epsilon} + n_k\\bar{\\epsilon}^{2}}\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) E\\left[ \\sum_{k=1}^{k}{(n_k\\bar{\\epsilon}_{k}^{2})} - 2N\\bar{\\epsilon}^{2} + N\\bar{\\epsilon}^{2}\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) E\\left[ \\sum_{k=1}^{k}{(n_k\\bar{\\epsilon}_{k}^{2})} - N\\bar{\\epsilon}^{2}\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) \\left[\\sum_{k=1}^{k}{n_k E(\\bar{\\epsilon}_{k}^{2})} - NE(\\bar{\\epsilon}^{2})\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) \\left[\\sum_{k=1}^{k}{n_k \\frac{\\sigma^2}{n_k}} - N\\frac{\\sigma^2}{N}\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) \\left[\\sum_{k=1}^{k}{\\sigma^2} - \\sigma^2\\right] \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) (k{\\sigma^2} - \\sigma^2) \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\left( \\frac{1}{k - 1}\\right) (k-1)\\sigma^2 \\\\ \\displaystyle E(MS_B) = \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2} + \\sigma^2 \\\\ \\displaystyle E(MS_B) = \\sigma^2 + \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k(\\tau_k - \\bar{\\tau})^2}\\) Since: \\(\\bar{\\tau} = 0\\) \\(\\displaystyle E(MS_B) = \\sigma^2 + \\left( \\frac{1}{k - 1}\\right) \\sum_{k=1}^{k}{n_k\\tau_k^2}\\) Other stuff The T and F The squared independent random variable with a t-distribution with \\(v\\) degree of freedom has an F distribution with degrees of freedom 1 of 1 and degree of freedom 2 of \\(v\\) My interpretation (Read with caution) t and F are ratios of the variation due to the model to the natural random variation in the distribution t is the ratio of the variation due to the model in terms of difference in their expected values to the natural random variation in the distribution (the distribution being the sampling distribution; so they look at the expected values) F is the ratio of the variation due to the model in terms of individual scores to the natural random variation in the distribution (the distribution being the population distribution; so they look at individual scores rather than expected values) \\(t^2 = F\\) Variance of a linear combination of random variables Mathematics \\(\\begin{aligned} \\displaystyle y &amp;= \\sum_{i = 1}^{n}{a_ix_i} = a_1x_1 + a_2x_2 + a_3x_3 + \\cdots + a_nx_n \\\\ \\displaystyle \\text{Var}(y) &amp;= \\sum_{i=1}^{n}{a_i^2\\text{Var}(x_i)} + 2\\sum_{}^{}{\\sum_{1≤i&lt;j≤n}^{}{\\text{Cov}(a_i, a_j)}} \\\\ \\displaystyle \\text{Var}(y) &amp;= \\text{Var}(x_i)\\sum_{i=1}^{n}{a_i^2} + 2\\sum_{}^{}{\\sum_{1≤i&lt;j≤n}^{}{\\text{Cov}(a_i, a_j)}} \\end{aligned}\\) Where The double sum is over all pairs \\((i,j)\\) with \\(i &lt; j\\) Ref Wackerly, Mendenhall and Scheaffer Applications of this fact Sampling variance/standard error Concept The sampling variance or standard error of a linear function/combination of random variables can be found using this fact Most statistics are a linear combination of random variables (e.g. sample mean, sample beta, etc.) Using the fact to derive the sampling variance of the mean Let \\(\\bar{y_i}\\) be a sample mean The sample mean is a linear function \\(\\bar{y_i} = \\frac{x_1 + x_2 + x_3 + \\cdots + x_n}{n} \\\\ \\bar{y_i} = \\frac{1}{n}x_1 + \\frac{1}{n}x_2 + \\frac{1}{n}x_3 + \\cdots + \\frac{1}{n}x_n \\\\ \\bar{y}_i = \\sum_{i = 1}^{n}{\\frac{1}{n}x_i} \\\\ \\displaystyle \\text{Var}(\\bar{y_i}) = \\text{Var}(x_i)\\sum_{i = 1}^{n}{\\left(\\frac{1}{n}\\right)^2} + 2\\sum_{}^{}{\\sum_{1≤i&lt;j≤n}^{}{\\text{Cov}(x_i, x_j)}}\\) Assuming that \\(x_i\\) are independent, which implies that \\(\\sum_{}^{}{\\sum_{1≤i&lt;j≤n}^{}{\\text{Cov}(x_i, x_j)}} = 0\\) \\(\\displaystyle \\text{Var}(\\bar{y_i}) = \\text{Var}(x_i)\\sum_{i = 1}^{n}{\\left(\\frac{1}{n}\\right)^2} \\\\ \\displaystyle \\text{Var}(\\bar{y_i}) = \\text{Var}(x_i)n{\\left(\\frac{1}{n}\\right)^2} \\\\ \\displaystyle \\text{Var}(\\bar{y_i}) = \\text{Var}(x_i)\\frac{1}{n} \\\\ \\displaystyle \\text{Var}(\\bar{y_i}) = \\frac{\\sigma^2}{n} \\\\ \\displaystyle \\text{sd}(\\bar{y_i}) = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\) The F statistic The F statistic Concept The \\(F\\) statistic is the ratio of a independent random variable with a Chi-squared distribution divided by its degrees of freedom to another independent random variable with a Chi-squared distribution divided by its degrees of freedom Mathematics \\(F = \\frac{\\frac{S_1}{df_1}}{\\frac{S_2}{df_2}}\\) Where \\(S_1\\) is an independent random variable with a Chi-squared distribution with degrees of freedom of \\(df_1\\) \\(S_2\\) is another independent random variable with a Chi-squared distribution with degrees of freedom of \\(df_2\\) The F-distribution: The distribution of the F statistic Concept The \\(F\\) statistic has an \\(F\\) distribution Mathematics I am not including the PDF maths here Sum of Squared Deviations Concept Aka Sum of Squares A measure of variation The Sum of Squared Deviation is the sum of all the squared deviations where a deviation is the difference between an observed value and its expected value It is the standardized version of the variance Mathematics \\(\\displaystyle SS = \\sum_{i=1}^{n}{\\left( y_i - E(y)\\right)^2}\\) Where \\(y\\) is a variable ANOVA as a Linear Model Introduction ANOVA can be implemented in a Linear Model 2-sample ANOVA as a Linear Model Mathematics \\(y_{ik} = \\beta_{0} + \\beta_{k}x_k + \\epsilon_{ik}\\) Where \\(y_{ik}\\) Observation \\(i\\) in group/sample \\(k\\) \\(\\beta_{0}\\) The reference group/sample - The group/sample against which each of the other samples are compared If there is a control group, it would make the most sense if the control group is used as the reference group \\(\\beta_{k}\\) The difference between the mean of sample/group \\(k\\) and \\(\\beta_0\\) \\(\\beta_k = \\mu_k - \\mu_0\\) Represents the effect of level \\(k\\) of the categorical predictor variable relative to the reference group \\(x_k\\) A dummy or indicator variable for \\(\\beta_k\\) \\(x_k = \\begin{cases} 1 ~~~ \\text{if the observation is from the group} ~ k \\\\ 0 ~~~ \\text{if the observation is not from the group}~k \\end{cases}\\) \\(\\beta_{0} + \\beta_{k}x_k\\) The expected value of group \\(k\\) This is equivalent to \\(\\mu_k\\) \\(\\epsilon_{ik}\\) The random error of observation \\(i\\) in group \\(k\\) \\(\\epsilon_{ik}\\) is a random independent normal variable with \\(E(\\epsilon_{ik}) = 0\\) and \\(Var(\\epsilon_{ik}) = \\sigma^2\\) NHST Concept To test the null hypothesis that the \\(\\beta_1 = \\mu_1 - \\mu_0 = 0\\) For this 2-sample test, you can use a t-statistic or an F statistic Hypotheses Null hypothesis Beta 1 is 0 \\(H_0: \\beta_1 = 0\\) t test The t statistic \\(t = \\frac{\\hat\\mu_1 - \\hat\\mu_0}{\\text{sd}(\\hat\\mu_1 - \\hat\\mu_0)} = \\frac{\\hat\\beta_1}{\\text{sd}(\\hat\\beta)}\\) Where \\(\\text{sd}(\\hat\\beta)\\) is the standard error of the \\(\\hat\\beta_1\\) F test The F-statistic \\(F = \\frac{MS_B}{MS_E}\\) 3-sample ANOVA as a Linear Model with Dummy Coding Mathematics \\(y_{ik} = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 + \\epsilon_{ik}\\) Where \\(\\beta_{0}\\) The mean of the sample at the reference/baseline group/level It represents the predicted value in the outcome variable when all other levels of the Factor are 0 (controlling for the effect of all other levels of the Factor) \\(\\beta_{0} = \\mu_{reference ~ level}\\) For example, if level 1 is chosen as the reference level, then \\(\\beta_{0} = \\mu_{1}\\) \\(\\beta_{1}\\) Dummy variable 1 Represents the effect of one level of the Factor (by default, R chooses the second level of the Factor) The difference between the mean of the sample at one level of the Factor and the intercept For example, if this level is level 2 of the Factor, then \\(\\beta_{1} = \\mu_{2} - \\mu_{1}\\) \\(x_{1}\\) Weight or indicator function for dummy variable 1 Dummy coding \\(x_{1} = \\begin{cases} 1 ~~~ \\text{if the observation is from sample 1} \\\\ 0 ~~~ \\text{if the observation is not from sample 1} \\end{cases}\\) \\(\\beta_{2}\\) \\(\\beta_{2} = \\mu_{2} - \\mu_{0}\\) Dummy variable 2 Represents the effect of the other level of the Factor compared to the intercept (by default, R chooses the third level of the Factor) The difference between the mean of the sample at the other level of the Factor and the intercept \\(x_{2}\\) Weight or indicator function for dummy variable 2 Dummy contrast coding \\(x_{2} = \\begin{cases} 1 ~~~ \\text{if the observation is from sample 1} \\\\ 0 ~~~ \\text{if the observation is not from sample 1} \\end{cases}\\) 3-sample ANOVA as a Linear Model with Helmert Contrast Coding Mathematics \\(y_{ik} = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 + \\epsilon_{ik}\\) Where \\(\\beta_{0}\\) \\(\\beta_{0} = \\mu_{grand}\\) The grand mean The mean of group means With a 3-sample ANOVA with Helmert coding, it is \\(\\beta_{0} = \\mu_{grand} = \\frac{\\mu_{1} + \\mu_{2} + \\mu_{3}}{3}\\) \\(\\beta_{1}\\) \\(\\displaystyle \\beta_{1} = \\mu_{1} - \\left(\\frac{\\mu_{2} + \\mu_{3}}{2} \\right)\\) Helmert Contrast variable 1 The difference between the mean of the sample at level 1 of the factor and the mean of the sample at level 2 and 3 lumped together Represents the effect of level 2 and 3 compared to level 1 \\(x_{1}\\) Weight or indicator function for Helmert Contrast variable 1 \\(x_{1} = \\begin{cases} \\frac{2}{3} ~~~ \\text{if the observation is from the sample at level 1} \\\\ -\\frac{1}{3} ~~~ \\text{if the observation is from the sample at level 2} \\\\ -\\frac{1}{3} ~~~ \\text{if the observation is from the sample at level 3}\\end{cases}\\) \\(\\beta_{2}\\) \\(\\beta_{2} = \\mu_{2} - \\mu_{3}\\) Helmert Contrast variable 2 Dummy variable for category 2 The difference between the mean of the sample at level 2 of the factor and the mean of the sample at level 3 of the factor Represents the effect of level 2 of the factor compared to level 3 of the factor \\(x_{2}\\) Weight or indicator function for Helmert Contrast variable 2 \\(x_{2} = \\begin{cases} 0 ~~~ \\text{if the observation is from the sample at level 1} \\\\ \\frac{1}{2} ~~~ \\text{if the observation is from the sample at level 2} \\\\ -\\frac{1}{2} ~~~ \\text{if the observation is from the sample at level 3}\\end{cases}\\) Helmert Contrast Bootstrap For robust confidence intervals and p-values Non-parametric F Kruskal-Wallis test Robust F Welch’s F Mann-Whitney test Concept Aka Wilcoxon rank sum test Non-parametric test for difference between 2 independent samples Observations from both samples are grouped together and ranked in ascending order The test statistic the rank sum (sum of the ranks) of one of the samples (this statistic is called R) Statistical significance is assessed by finding the probability of the observed statistic in the probability distribution of the R test statistic under the null hypothesis Procedure Group all observations from both samples together and rank the observations in ascending order Calculate the sample R statistic (the R statistic is the sum of the ranks of those observations in either one of the 2 groups) Find the probability distribution of R under the null hypothesis Find or estimate the probability of the observed R in the probability distribution of R under the null hypothesis Finding the probability distribution of R under the null hypothesis Find all possible combination of ranks (the number of combination of ranks should be \\(n\\choose r\\) where n is the total number of ranks and r is the number of ranks in the group of interest) Under the null hypothesis, each of all possible combination of ranks have an equal chance, hence, each of all possible combination occurs only once Find the R of each of all possible combinations of ranks The distribution of the set of Rs found is the distribution of R under the null hypothesis Example The raw data: group y Control 6 Control 4 Experimental 1 Experimental 3 Group all observations from both samples together and rank the observations in ascending order Rank Combinations W 1, 2 3 1, 3 4 1, 4 5 2, 3 5 2, 4 6 3, 4 7 Calculate the sample W statistic The W statistic is the sum of the ranks of those observations in either one of the 2 groups Here, the Control group is chosen. The W statistic is 7 (\\(W = 7\\)) Find all possible rank combinations for either one of groups and calculate the W statistic for each of the rank combinations Find the probability distribution of W under the null hypothesis Find all possible rank combinations for either one of the groups Find the probability distribution of the rank combinations under the null hypothesis Under the null hypothesis, each of the rank combination has an equal chance. Hence, each of the rank combination occur only once. Calculate the W statistic for each of the rank combinations. Rank Combinations W 1, 2 3 1, 3 4 1, 4 5 2, 3 5 2, 4 6 3, 4 7 The probability distribution of the resultant set of W statistics is the probability distribution of W under the null hypothesis W 3 4 5 6 7 Freq 1 1 2 1 1 prob 0.167 0.167 0.333 0.167 0.167 Find the probability of the observed W statistic in the probability distribution under the null hypothesis As mentioned, \\(W = 7\\) \\(P(W = 7) = 0.167\\) Critical regions The critical regions can be found in published tables If the probability of the observed W statistic falls outside the critical boundary, then it is statistically significant Ties If there are ties in the data, then the ranks for the ties will be the same and it will be the mean of the supposed ranks of the ties Example If the data is 10.5, 11.2, 15.7, 15.7, 15.7, 15.7, 16.2, 18.5 The ranks would be 1, 2, 3, 4.5, 4.5, 4.5, 4.5, 8, 9 Kruskal Wallis test Concept A nonparametric test of difference between several samples (One-way ANOVA) A generalisation of the Wilcoxon test Based on ranked scores Scores are grouped together and then transformed into ranked scores, the ranked scores are analysed The null hypothesis is tested by the \\(K\\) statistic (a chi-squared transformed ranked version of the \\(SS_B\\); see below) It involves assessing the probability of having a \\(K\\) statistic equal to or more extreme than the observed \\(K\\) statistic in the distribution of \\(K\\) under the null hypothesis (that the samples are from the same population - That there are no differences between samples) (because the statistic is standardised, the K statistic basically assess how much bigger the observed SSB is compared to the SSB in under the null, because the SSB statistic is converted to a standardised Chi-squared statistic, it compares how much bigger the standardised version of the SSB is compare to 1, because the sd (or se) of the chi-squared distribution is 1, this is the natural variation measured as sd in the null chi-squared distribution, the K statistic tells you how many times the observed SSB (or estimate of the variance of the population) is larger than the variance of the null distribution, again the variance is measured as sd) It involves enumerating the null distribution of \\(SS_B\\) for all possible combinations of \\(k\\) and \\(n_k\\) The Kruskal Wallis \\(SS_B\\) Let \\(R_{ik}\\) be the rank of \\(y_{ik}\\) \\(SS_B = \\sum_{k=1}^{k}{n_k\\left(\\bar{R}_{k} - \\bar{R}\\right)^2}\\) Where \\(\\bar{R}_{k}\\) The mean of the ranked scores of sample \\(k\\) \\(\\displaystyle \\bar{R}_{k} = \\frac{1}{n_k}\\sum_{i = 1}^{n_k}{R_{ik}}\\) \\(\\bar{R}\\) The overall mean of the ranked scores \\(\\displaystyle \\bar{R} = \\frac{1}{N}\\sum_{k=1}^{k}{\\sum_{i = 1}^{n_k}{R_{ik}}} = \\frac{N+1}{2}\\) The Kruskal Wallis statistic (\\(K\\)) Concept The \\(SS_B\\) statistic converted to a standardised ch-square score Mathematics \\(\\displaystyle K = \\frac{12}{N(N+1)}SS_B\\) Null Distribution of the Kruskal Wallis statistic Mathematics \\(K \\sim \\chi^2\\left( k-1 \\right)\\) Kruskal Wallis test in R # Kruskal Wallis test kruskal.test(outcome_variable ~ predictor_variable, data = data) Factorial ANOVA Two-way ANOVA Concept A continuous categorical variable predicted by 2 categorical variables The statistical model of 2-way ANOVA Mathematics \\(y_{ijk} = \\mu + \\alpha_{j} + \\beta{k} + \\delta_{jk} + \\epsilon_{ijk}\\) Where \\(y_{ijk}\\) Observation \\(i\\) in level \\(j\\) of one factor and level \\(k\\) of another factor \\(\\mu\\) The overall/grand mean \\(\\alpha_{j}\\) The effect of level \\(j\\) factor 1 The difference between the mean of sample \\(j\\) of factor 1 and the overall mean \\(\\alpha_{j} = \\mu{}_{j\\cdot{}} - \\mu_0\\) \\(\\beta{k}\\) The effect of level \\(k\\) of factor 2 The difference between the mean of sample \\(k\\) of factor 2 and the overall mean \\(\\beta{k} = \\mu{}_{\\cdot{}k} - \\mu_0\\) \\(\\delta_{jk}\\) The interaction effect Here the interaction effect is the effect of level \\(k\\) of factor 1 on level \\(j\\) of factor 2 \\(\\delta_{jk} = (\\mu_{jk} - \\mu{}_{j\\boldsymbol{\\cdot}}) - (\\mu_{\\boldsymbol{\\cdot}k} - \\mu_0)\\) Where \\(\\mu_{jk}\\) - The mean of sample in level \\(j\\) of factor 1 and level \\(k\\) of factor 2 \\(\\mu{}_{j\\boldsymbol{\\cdot}}\\) - The mean of the sample in level \\(j\\) of factor 1 regardless of the level of factor 2 \\(\\mu_{\\boldsymbol{\\cdot}k}\\) - The mean of level \\(k\\) of factor 2 regardless of the level of factor 1 \\(\\mu_0\\) - Overall mean \\((\\mu_{jk} - \\mu{}_{j\\boldsymbol{\\cdot}})\\) The difference between the mean of a specific sample (here it is the sample at level \\(j\\) of factor 1 and level \\(k\\) of factor 2) and the overall mean for sample at level \\(j\\) of factor 1 (overall in the sense that it is regardless of the level of factor 2) The effect of level \\(k\\) of factor 2 on level \\(j\\) of factor 1 Two-Way ANOVA partitioning variances Concept Total Sum of Squares Between-Group Sum of Squares Between-Group Sum of Squares for Factor 1 Between-Group Sum of Squares for Factor 2 Between-Group Sum of Squares for the interaction between Factor 1 and Factor 2 The Error Sum of Squares Total Sum of Squares Concept The sum of squared deviation between each observation and the overall/grand mean This represents the total variation in the outcome variable Mathematics \\(\\begin{aligned} \\displaystyle SS_T &amp;= SS_B + SS_W \\\\ \\displaystyle SS_T &amp;= \\left( SS_A + SS_B + SS_{A\\times B} \\right) + SS_W \\end{aligned}\\) Operationalsing variability The Model Sum of Squares Components of the Model Sum of Squares Sum of Squares for Factor A Sum of Squares for Factor B Sum of Squares for the Interaction between Factor A and B Sum of Squares for Factor A (\\(SS_A\\)) Concept The sum of squared deviation between the predicted value of observation \\(i\\) in level \\(j\\) of Factor A regardless of the level of Factor B and the overall/grand mean It represents the main effect of Factor A (the effect of Factor A regardless of levels of the other factor) in the form of variation in the outcome variable that is attributable to Factor A Mathematics \\(\\displaystyle SS_A = \\sum_{j = 1}^{J}{n_{j \\boldsymbol{\\cdot}} \\left( \\bar{y}_{j\\boldsymbol{\\cdot}} - \\bar{y} \\right)^2}\\) Where \\(n_{j \\boldsymbol{\\cdot}}\\) - Number of observations in level \\(j\\) of Factor A regardless of the level of Factor B \\(\\bar{y}_{j\\boldsymbol{\\cdot}}\\) - The mean of sample in the level \\(j\\) of Factor regardless of the level of Factor B - The predicted value for observation \\(i\\) in level \\(j\\) of Factor A regardless of the level of Factor B \\(\\bar{y}\\) - Overall/grand mean Degrees of freedom \\(df_A = J - 1\\) Where \\(J\\) - The total number of levels in Factor A Mean Factor A Sum of Squares Concept Variance in the outcome variable that is attributable to Factor A The mean/average amount of squared deviation in the outcome variable that is attributable to Factor A (the average in terms of between levels) Represents the main effect of Factor A in the form of variance in the outcome variable induced by Factor A Mathematics \\(\\displaystyle MS_{A} = \\frac{SS_A}{df_A}\\) Sum of Squares for Factor B (\\(SS_B\\)) Concept The sum of squared deviation between the predicted value for observation \\(i\\) in level \\(k\\) of Factor B regardless of the level in Factor A and the overall mean It represents the main effect of Factor B (the effect of Factor B regardless of levels of the other Factor) in the form of variation in the outcome variable that is attributable to Factor B Mathematics \\(\\displaystyle SS_B = \\sum_{k = 1}^{K}{n_{\\boldsymbol{\\cdot}k} \\left( \\bar{y}_{\\boldsymbol{\\cdot}k} - \\bar{y} \\right)^2}\\) Where \\(n_{\\boldsymbol{\\cdot}k}\\) - The number of observations in level \\(k\\) of Factor B regardless of the level of Factor A \\(\\bar{y}_{\\boldsymbol{\\cdot}k}\\) - The mean of level \\(k\\) of Factor B regardless of the level of Factor A - The predicted value for observation \\(i\\) in level \\(k\\) of Factor B regardless of the level of Factor A Degree of Freedom \\(df_B = K - 1\\) Where \\(K\\) - The number of levels in Factor B Mean Factor B Sum of Squares Concept Variance in the outcome variable that is attributable to Factor A The mean/average amount of squared deviation in the outcome variable that is attributable to Factor B (the average in terms of between levels) It represents the main effect of Factor B in the form of variance in the outcome variable induced by Factor A Mathematics \\(\\displaystyle MS = \\frac{SS_B}{df_B}\\) Sum of Squares for the Interaction between Factor A and Factor B (\\(SS_{A\\times B}\\)) Concept It represents the interaction/combined effect of Factor A and B (intereffect?) in the form of variation in the outcome variable that is attributable to the interaction of Factor A and B Factor A and B) Mathematics \\(\\displaystyle SS_{A \\times B} = \\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{n_{ij}\\left[ (\\bar{y}_{\\boldsymbol{\\cdot}k} - \\bar{y}) - (\\bar{y}_{jk} - \\bar{y}_{j\\boldsymbol{\\cdot}}) \\right]^2 }}\\) Where \\(n_{jk}\\) - The number of observations in level \\(j\\) of Factor A and level \\(k\\) of Factor B \\((\\bar{y}_{jk} - \\bar{y}_{j\\boldsymbol{\\cdot}})\\) The difference between the mean of the sample in level \\(j\\) of Factor A and level \\(k\\) of Factor B and the grand mean of level \\(j\\) The effect of level \\(k\\) of Factor B on level \\(j\\) of Factor A \\((\\bar{y}_{\\boldsymbol{\\cdot}k} - \\bar{y})\\) The difference between the grand mean of level \\(k\\) of Factor A and the overall mean This represents the main effect of level \\(k\\) of Factor A $({y}{k} - {y}) - ({y}{jk} - {y}_{j}) $ The interaction effect The effect of Factor A on the effect of Factor B The effect of level \\(j\\) of Factor A on the effect of Factor B The effect of level \\(k\\) of Factor B on the effect of Factor A Degrees of Freedom \\(df_{A\\times B} = (J-1)(K-1)\\) Mean Interaction Sum of Squares Concept Variance in the outcome variable that is attributable to the interaction between Factor A and B The mean amount of squared deviation in the outcome variable that is attributable to the interaction between Factor A and B The average amount of squared deviation in the outcome variable that is attributable to the interaction between Factor A and B per each of the \\(J\\times K\\) samples Mathematics \\(MS_{AB} = \\frac{SS_{A \\times B}}{df_{A\\times B}}\\) Proof $SS_T = {j = 1}^{J}{{k = 1}^{K}{{i = 1}^{n{jk}}{(y_{ijk} - {y})^2}}} \\ E(SS_T) = E\\ E(SS_T) = (N-1)^2 + {j = 1}^{J}{{k = 1}{K}{{i = 1}{n_{jk}}{({j}^{2} + {k}{2} + {jk}{2})^2}}} $ Since \\(\\sum_{j = 1}^{J}{\\alpha_{j}} = \\sum_{k = 1}^{K}{\\beta_{k}} = \\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{\\delta_{jk}}} = 0\\), all the cross-level products are 0, leaving us with the following: \\(\\\\ \\displaystyle E(SS_T) = (N-1)\\sigma^2 + \\sum_{j = 1}^{J}{n_{j}\\alpha_{j}^{2}} + \\sum_{k = 1}^{K}{n_{k}\\beta_{k}^{2}} + \\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{n_{jk}\\delta_{jk}^{2}}} \\\\ \\displaystyle E(SS_T) = \\sum_{j = 1}^{J}{n_{j}\\alpha_{j}^{2}} + \\sum_{k = 1}^{K}{n_{k}\\beta_{k}^{2}} + \\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{n_{jk}\\delta_{jk}^{2}}} + (N-1)\\sigma^2 \\\\ \\displaystyle E(SS_T) = E(SS_A) + E(SS_B) + E(SS_{AB}) + E(SS_W)\\) Estimating the common variance Estimators of the population variance \\(MS_W\\) \\(MS_A\\) \\(MS_B\\) \\(MS_{AB}\\) The \\(MS_W\\) estimator See One-Way ANOVA The \\(MS_A\\) estimator Mathematics \\(\\displaystyle E(MS_A) = \\sigma^2 + \\frac{1}{J-1}K\\sum_{j = 1}^{J}{n_j\\alpha_{j}^{2}}\\) Implication of \\(MS_A\\) as an estimator of the population variance Under the null hypothesis, that is when all \\(\\alpha{}_{j}s = 0\\), the linear combination \\(\\frac{1}{J-1}K\\sum_{j = 1}^{J}{n_j\\alpha_{j}^{2}} = 0\\), therefore, \\(\\displaystyle E(MS_A) = \\sigma^2\\), the observed \\(MS_A\\) is an unbiased estimator of \\(\\sigma^2\\) However, if the null hypothesis is false, that is when at least one of the \\(\\alpha{}_{j}s≠ 0\\), the linear combination \\(\\frac{1}{J-1}K\\sum_{j = 1}^{J}{n_j\\alpha_{j}^{2}} &gt; 0\\), hence, \\(MS_A\\) is a positively biased estimator of \\(\\sigma^2\\) The \\(MS_B\\) estimator Mathematics \\(\\displaystyle E(MS_B) = \\sigma^2 + \\frac{1}{K-1}J\\sum_{k = 1}^{K}{n_k\\beta_{k}^{2}}\\) Implication of \\(MS_B\\) as an estimator of the population variance Under the null hypothesis, that is when all \\(\\beta{}_{k}s = 0\\), the linear combination \\(\\frac{1}{K-1}J\\sum_{k = 1}^{K}{n_k\\beta_{k}^{2}} = 0\\), therefore, $ E(MS_A) = ^2$, the observed \\(MS_B\\) is an unbiased estimator of \\(\\sigma^2\\) However, if the null hypothesis is false, that is when at least one of the \\(\\beta{}_{k}s≠ 0\\), the linear combination \\(\\frac{1}{K-1}J\\sum_{k = 1}^{K}{n_k\\beta_{k}^{2}} &gt; 0\\), hence, \\(MS_B\\) is a positively biased estimator of \\(\\sigma^2\\) The \\(MS_{AB}\\) estimator Concept Mathematics \\(\\displaystyle E(MS_{AB}) = \\sigma^2 + \\frac{1}{(J-1)(K-1)}\\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{n_{jk}\\delta{}_{jk}^{2}}}\\) Implication of \\(MS_B\\) as an estimator of the population variance Under the null hypothesis, that is when all \\(\\delta{}_{jk}s = 0\\), the linear combination \\(\\frac{1}{(J-1)(K-1)}\\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{n_{jk}\\delta{}_{jk}^{2}}} = 0\\), therefore, $ E(MS_{AB}) = ^2$, the observed \\(MS_{AB}\\) is an unbiased estimator of \\(\\sigma^2\\) However, if the null hypothesis is false, that is when at least one of the \\(\\delta{}_{jk}s ≠ 0\\), the linear combination \\(\\frac{1}{(J-1)(K-1)}\\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{n_{jk}\\delta{}_{jk}^{2}}} &gt; 0\\), hence, \\(MS_B\\) is a positively biased estimator of \\(\\sigma^2\\) Effects in a Two-Way ANOVA Main Effect of Factor A Main Effect of Factor B Interaction effect of Factor A and B Main Effect of Factor A General concept The effect of Factor A on the outcome variable regardless of the levels of other categorical predictors in the model Conceptual Hypotheses Null Hypothesis Populations at different levels of Factor A have the same mean (the samples are from the same population) Factor A, regardless of the levels of other categorical predictors), has no effect on the outcome variable There are no differences in the outcome variable across levels of Factor A regardless of the level of all other categorical predictors in the model \\(H_{0,A}: \\mu_{1\\boldsymbol{\\cdot}} = \\mu_{2\\boldsymbol{\\cdot}} = \\cdots = \\mu_{J\\boldsymbol{\\cdot}}\\) Alternative Hypothesis Populations at different levels of Factor A have different means (the samples are from different populations) Factor A, regardless of the levels of the other categorical predictors in the model), has an effect on the outcome variable regardless of the levels of other categorical predictors in the model There are differences in the outcome variable across levels of Factor A (regardless of the level of all other categorical predictors in the model) \\(H_{1,A}: \\mu_{1\\boldsymbol{\\cdot}} ≠ \\mu_{2\\boldsymbol{\\cdot}} ≠ \\cdots ≠ \\mu_{J\\boldsymbol{\\cdot}}\\) Operationalising the null hypothesis Concept It is based on the agreement/disagreement between the \\(MS_{A}\\) estimator and the \\(MS_{W}\\) estimator of the population variance (\\(\\sigma^2\\)) The Test statistic: The F-ratio Concept The first conceptual null hypothesis is formally operationalised as \\(\\sum_{j = 1}^{J}{n_j\\alpha_{j}^{2}} = 0\\) The null hypothesis is tested by a test statistic that compares the \\(MS_A\\) estimator with the unbiased \\(MS_W\\) estimator as the \\(F\\) ratio of \\(MS_A\\) to \\(MS_W\\) A standardised F-score How much larger is the \\(MS_A\\) estimate compared to \\(MS_W\\) How big is the positive bias? Mathematics \\(\\displaystyle F_A = \\frac{MS_A}{MS_W}\\) Null Hypothesis Significance Testing Concept To assess the probability of an F-statistic as extreme as or more extreme than the observed F-statistic in the distribution of the central F-statistic (the distribution of the F-statistic under the null hypothesis) The Central F-distribution \\(F \\sim F\\left(J-1 , N-JK \\right)\\) Main Effect of Factor B Hypotheses Null Hypothesis Populations for different levels of Factor B have the same mean (the samples are from the same population) Factor B has no effect on the outcome variable regardless of the levels of all other categorical predictors in the model There are no systematic or real differences in the outcome variable across levels of Factor B regardless of the level of all other categorical predictors in the model \\(H_{1,B}: \\mu_{\\boldsymbol{\\cdot}1} = \\mu_{\\boldsymbol{\\cdot}2} = \\cdots = \\mu_{\\boldsymbol{\\cdot}K}\\) Alternative Hypothesis Populations for different levels of Factor B have different means (the samples are from different populations) Factor B has an effect on the outcome variable regardless of the levels of all other categorical predictors in the model There are systematic or real differences in the outcome variable across levels of Factor B regardless of the levels of all other categorical predictors in the model \\(H_{1,B}: \\mu_{\\boldsymbol{\\cdot}1} ≠ \\mu_{\\boldsymbol{\\cdot}2} ≠ \\cdots≠ \\mu_{\\boldsymbol{\\cdot}K}\\) Operationalising the null hypothesis Concept It is based on the agreement/disagreement between the \\(MS_{B}\\) estimator and the \\(MS_{W}\\) estimator of the population variance (\\(\\sigma^2\\)) The Test statistic: The F-ratio Concept The first conceptual null hypothesis is formally operationalised as \\(\\sum_{k = 1}^{K}{n_k\\beta_{k}^{2}} = 0\\) The null hypothesis is tested by a test statistic that compares the \\(MS_B\\) estimator with the unbiased \\(MS_W\\) estimator as the \\(F\\) ratio of \\(MS_B\\) to \\(MS_W\\) A standardised F-score How much larger is the \\(MS_B\\) estimate compared to \\(MS_W\\) How big is the positive bias? Mathematics \\(\\displaystyle F_B = \\frac{MS_B}{MS_W}\\) Null Hypothesis Significance Testing Concept To assess the probability of an F-statistic as extreme as or more extreme than the observed F-statistic in the distribution of the central F-statistic (the distribution of the F-statistic under the null hypothesis) The Central Distribution of \\(F_B\\) \\(F_B \\sim F\\left(K-1 , N-JK \\right)\\) Interaction effect of Factor A and B General Concept The effect of one Factor on the other Factor The effect of Factor A on Factor B, or equivalently, the effect of Factor B on Factor A The combined effect of Factor A and B It is something like the dot product of 2 vectors Hypotheses Null Hypothesis There is no interaction effect between Factor A and B - The effect of Factor A is the same across levels of Factor B, or equivalently, the effect of Factor B is the same across levels of Factor A The additive main effect of particular levels of Factor A is the same across all levels of Factor B \\(\\begin{bmatrix} \\alpha_1 = \\alpha_{1,1} = \\alpha_{1, 2} = \\cdots = \\alpha_{1, k} \\\\ \\alpha_2 = \\alpha_{2,1} = \\alpha_{2, 2} = \\cdots = \\alpha_{2, k} \\\\ \\vdots \\\\ \\alpha_j = \\alpha_{j,1} = \\alpha_{j, 2} = \\cdots = \\alpha_{j, k} \\end{bmatrix} \\\\ \\begin{bmatrix} \\beta_1 = \\beta_{1,1} = \\beta_{2, 1} = \\cdots = \\beta_{3, 1} \\\\ \\beta_2 = \\beta_{1,2} = \\beta_{2, 2} = \\cdots = \\beta_{3, 2} \\\\ \\vdots \\\\ \\beta_k = \\beta_{1,k} = \\beta_{2, k} = \\cdots = \\beta_{j, k} \\end{bmatrix}\\) Alternative Hypothesis There is interaction effect between Factor A and B - The effect of Factor A is different in different levels of Factor B, or equivalently, the effect of Factor A is different in different levels of Factor A The additive main effect of particular levels of Factor A is different in different levels of Factor B \\(\\begin{bmatrix} \\alpha_1 ≠ \\alpha_{1,1} ≠ \\alpha_{1, 2} ≠ \\cdots ≠ \\alpha_{1, k} \\\\ \\alpha_2 ≠ \\alpha_{2,1} ≠ \\alpha_{2, 2} ≠ \\cdots ≠ \\alpha_{2, k} \\\\ \\vdots \\\\ \\alpha_j ≠ \\alpha_{j,1} ≠ \\alpha_{j, 2} ≠ \\cdots ≠ \\alpha_{j, k} \\end{bmatrix} \\\\ \\begin{bmatrix} \\beta_1 ≠ \\beta_{1,1} ≠ \\beta_{2, 1} ≠ \\cdots ≠ \\beta_{3, 1} \\\\ \\beta_2 ≠ \\beta_{1,2} ≠ \\beta_{2, 2} ≠ \\cdots ≠ \\beta_{3, 2} \\\\ \\vdots \\\\ \\beta_k ≠ \\beta_{1,k} ≠ \\beta_{2, k} ≠ \\cdots ≠ \\beta_{j, k} \\end{bmatrix}\\) Operationalising the null hypothesis Concept It is based on the agreement/disagreement between the \\(MS_{AB}\\) estimator and the \\(MS_{W}\\) estimator of the population variance (\\(\\sigma^2\\)) The Test statistic: The F-ratio Concept The first conceptual null hypothesis is formally operationalised as \\(\\sum_{j = 1}^{J}{\\sum_{k = 1}^{K}{n_{jk}\\delta{}_{jk}^{2}}} = 0\\) The null hypothesis is tested by a test statistic that compares the \\(MS_{AB}\\) estimator with the unbiased \\(MS_W\\) estimator as the \\(F\\) ratio of \\(MS_{AB}\\) to \\(MS_W\\) A standardised F-score How much larger is the \\(MS_{AB}\\) estimate compared to \\(MS_W\\) How big is the positive bias? Mathematics \\(\\displaystyle F_{AB} = \\frac{MS_{AB}}{MS_W}\\) Null Hypothesis Significance Testing Concept To assess the probability of an F-statistic as extreme as or more extreme than the observed F-statistic in the distribution of the central F-statistic (the distribution of the F-statistic under the null hypothesis) The Central Distribution of \\(F_B\\) \\(F_{AB} \\sim F\\left((J-1)(K-1) , N-JK \\right)\\) Analysing Interactions Methods for analysiing Interactions Simple Effect Analysis Johnson Neyman Interval Simple Effect Analysis Concept The process of assessing the effect of one Factor at each level of another Factor Evaluation for ANOVA as a Linear Model It requires \\(k-1\\) dummy variables to be defined Two-Way ANOVA as a Linear Model 2x2 ANOVA with interaction with Dummy Coding Mathematics \\(y_{ijk} = \\beta_{0} + \\beta_{A}x_{A} + \\beta_{B}x_{B} + \\beta_{AB}x_{AB} + \\epsilon_{ijk}\\) Where \\(y_{ijk}\\) Observation \\(i\\) in level \\(j\\) of Factor A and level \\(k\\) of Factor B \\(\\beta_{0}\\) The intercept The mean of the reference group \\(\\beta_{0} = \\mu_{1,1}\\) \\(\\beta_{A}\\) The effect of Factor A controlling for the effect of Factor B and their interaction The difference between the mean of the sample at level 2 of Factor A and level 1 of Factor B (which is same as the intercept) and the intercept \\(\\beta_{A} = \\mu_{2, 1} - \\mu_{1, 1}\\) \\(x_{A}\\) Weight for dummy variable A Dummy Coding \\(x_{A} = \\begin{cases} 1 ~~~ \\text{if the observation is in level 2 of Factor A} \\\\ 0 ~~~ \\text{if the observation is not in level 2 of Factor A} \\end{cases}\\) \\(\\beta_{B}\\) The effect of Factor B controlling for the effect of Factor A and their interaction The difference between the mean of the sample at level 1 of Factor A (which is same as the intercept) and level 2 of Factor B and the intercept \\(x_{B}\\) Weight for dummy variable B Dummy Coding \\(x_{B} = \\begin{cases} 1 ~~~ \\text{if the observation is in level 2 of Factor B} \\\\ 0 ~~~ \\text{if the observation is not in level 2 of Factor B} \\end{cases}\\) \\(\\beta_{B} = \\mu_{1, 2} - \\mu_{1, 1}\\) \\(\\beta_{AB}\\) The interaction effect of Factor A and B controlling for the effect of Factor A and B The combined effect of Factor A and B controlling for the effect of Factor A and B The effect of level 2 of Factor A on \\(\\beta_{B}\\) \\(x_{AB}\\) Weight for dummy variable AB Dummy Coding \\(x_{AB} = \\begin{cases} 1 ~~~ \\text{if the observation is in level 2 of Factor A and level 2 of Factor B} \\\\ 0 ~~~ \\text{if the observation is not in level 2 of Factor A and level 2 of Factor B} \\end{cases}\\) Other Types of Contrast Coding Introduction In general, if you do other types of coding, the intercept will be the grand mean (the mean of group means) instead of the mean of the reference group (e.g. control group) Types of coding Dummy coding Simple coding Deviation Coding Orthogonal Polynomial Coding Helmert Coding Reverse Helmert Coding Forward Difference Coding Backward Difference Coding Dummy Contrast Coding Concept Each of the dummy variables represents the difference between the mean of the sample at each of the levels and the intercept which is the mean of the sample at the reference level (the reference group) Simple Contrast Coding Concept Each dummy variable represents the difference between the mean of the sample at each of the levels and the mean of the sample at the reference level (the reference level), the intercept is the grand mean This is similar to dummy coding, the only thing that is different is the intercept Deviation Contrast coding Concept Each of the dummy variables represents the difference between the mean of each of the levels and the intercept which is the grand mean Basically it is like dummy coding, however, the reference level is now the grand mean instead of the mean of the reference group, this is useful if you don’t have an apparent reference group Polynomial contrast Concept Used for polynomial trend analysis - To test whether there are certain polynomial trends along the levels Resources UCLA Statistical Methods and Data Analytics ## Rows: 200 Columns: 11 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (11): id, female, race, ses, schtyp, prog, read, write, math, science, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 200 × 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ## 7 50 0 3 2 1 1 50 59 42 53 61 ## 8 11 0 1 2 1 2 34 46 45 39 36 ## 9 84 0 4 2 1 1 63 57 54 58 51 ## 10 48 0 3 2 1 2 57 55 52 50 51 ## # … with 190 more rows ## .L .Q .C ## [1,] -0.6708204 0.5 -0.2236068 ## [2,] -0.2236068 -0.5 0.6708204 ## [3,] 0.2236068 -0.5 -0.6708204 ## [4,] 0.6708204 0.5 0.2236068 Deviation coding with interaction (both with deviation coding) The intercept is the grand mean Beta 1 is the effect of categorical variable 1 compared to the grand mean when the other categorical variable is at its mean level (the intercept) Beta 2 is the effect of categorical variable 2 compared to the grand mean when the other categorical variable is at its mean level (the intercept) Dummy coding for the first variable and deviation coding for the second variable with interaction The intercept is the value in the outcome variable when categorical variable 1 is at 0 and when categorical variable 2 is at the grand mean Beta 1 is the effect of categorical variable 1 at the mean level of categorical variable 2 Beta 2 is the effect of categorical variable 2 (the difference between the level of interest and the grand mean) when categorical variable 1 is at 0 Deviation coding for the first variable and dummy coding for the second variable with interaction Beta 1 is the effect of categorical variable 1 (the difference between the level of interest and the mean) when the categorical variable 2 is at the intercept (in this case it is at the meat level of categorical variable 2) Beta 2 is the effect of categorical variable 2 (the difference between the level of interest and its intercept) when categorical variable 1 is at the intercept (in this case it is the mean level) General Beta 1 is the effect of categorical variable 1 (the difference between the mean of the level of interest and its intercept) when beta 2 is held constant at its intercept The same goes for beta 2 Beta 2 is the effect of categorical variable 2 (the difference between the mean of the level of interest and its intercept) when beta 1 is held constant at its intercept The specific interpretation depends on how the intercept is defined Let’s look at some examples: Dummy coding for both categorical variables and without interaction The intercept Description The intercept is the fitted value of the outcome variable when both categorical variables 1 and 2 are at 0 (their reference levels) How it is calculated The value of the intercept is the predicted value on the regression slope for the effect of categorical variable 1 at 0 (the reference level) of categorical variable 2 (which has a slope of beta 1 and crosses the mean value of categorical variable 2’s reference level) when categorical variable 2 is at 0 (the reference level) Beta 1 Description Beta 1 is the difference between the fitted value for categorical variable 1’s level of interest when categorical variable 2 is at 0 (its reference level) and the fitted value for its reference level when categorical variable 2 is at 0 (its reference level) How it is calculated This difference is the difference between the mean of categorical variable 1’s level of interest at the mean level of categorical variable 2 (or what I like to describe as the mean at categorical variable 1’s level of interest regardless of the levels of the other categorical variable) and the mean of the reference level of categorical variable 1 at the mean level of categorical variable 2 (or what I like to describe as the mean of categorical variable 1’s reference level regardless of the level of the other categorical variable) Hence, this difference is not the actual difference between the means but it is the difference between the fitted values. Beta 2 Description Beta 2 is similar to beta 1 Beta 2 is the difference between the fitted value for categorical variable 2’s level of interest when categorical variable 1 is at 0 (its reference level) and the fitted value for its reference level when categorical variable 1 is at 0 (its reference level) How it is calculated This difference is the difference between the mean of categorical variable 2’s level of interest at the mean level of categorical variable 1 (or what I like to describe as the mean at categorical variable 2’s level of interest regardless of the levels of the other categorical variable) and the mean of the reference level of categorical variable 2 at the mean level of categorical variable 1 (or what I like to describe as the mean of categorical variable 2’s reference level regardless of the level of the other categorical variable) Similar to beta 1, this difference is not the actual difference between the means but it is the difference between the fitted values. Homogeneity of regression slopes A model without an interaction term assumes Homogeneity of Regression Slopes. Homogeneity of Regression Slopes - The effect of one predictor is the same across values of other predictors (there is no interaction effect) Dummy coding for both categorical variables and with interaction The intercept Description The intercept is the fitted value of the outcome variable when both categorical variables 1 and 2 are at 0 (their reference levels) taking into account interaction effects (if any) How it is calculated The value of the intercept is the predicted value on the regression slope for the effect of categorical variable 1 at 0 (the reference level) of categorical variable 2 (which has a slope of beta 1 and crosses the mean value of categorical variable 2’s reference level) when categorical variable 2 is at 0 (the reference level) adding the interaction effect (the effect of categorical variable 2’s reference level on the effect of categorical variable 1’s reference level) Beta 1 Description Beta 1 is the difference between the fitted value for categorical variable 1’s level of interest when categorical variable 2 is at 0 (its reference level) at the mean level of the interaction effect and the fitted value for its reference level when categorical variable 2 is at 0 (its reference level) at the mean level of the interaction effect How it is calculated This difference is the difference between the mean of categorical variable 1’s level of interest at the mean level of categorical variable 2 (or what I like to describe as the mean at categorical variable 1’s level of interest regardless of the levels of the other categorical variable) at the mean level of interaction effect (the effect of categorical variable 2’s level of interest on the effect of categorical variable 1’s reference level) and the mean of the reference level of categorical variable 1 at the mean level of categorical variable 2 (or what I like to describe as the mean of categorical variable 1’s reference level regardless of the level of the other categorical variable) at the mean level of interaction effect (the effect of categorical variable 2’s reference level on the effect of categorical variable 1’s reference level) Beta 2 Description Beta 2 would be similar to beta 1 but flipped Deviation coding for both categorical variables and without interaction Deviation coding for both categorical variables and with interaction The intercept is the grand mean Beta 1 is the difference between the categorical variable 1’s level of interest and its mean level when categorical variable 2 is at its mean level Beta 2 is the difference between the categorical variable 2’s level of interest and its mean level when categorical variable 2 is at its mean level ANCOVA Types of Sum of Squares This matters only if you are doing ANOVA as a linear model (using OLS) Types of Sum of Squares Type 1 Type 2 Type 3 Type 4 Two-Way ANOVA as a Linear Model "],["derivation.html", "Chapter 5 Derivation", " Chapter 5 Derivation "],["multivariate-analysis.html", "Chapter 6 Multivariate Analysis", " Chapter 6 Multivariate Analysis Multivariate Analysis Multivariate analysis Concept Any analyses involving more than one outcome variable The process of estimating the relationship between the linear combination of several outcome variables and one or more predictor variables Strengths of Multivariate Analysis A multivariate model allows the relationships between the set of predictor variables and multiple outcome variables to be tested in one test, which can maintain the Type I error rate (as opposed to testing the relationship between the same set of predictor variables and different outcome variables with separate models, which would inflate Type I error rate due to multiple testing) Takes into account the relationship between the outcome variables More power to detect whether groups differ along a combination of dimensions MANOVA has greater power than ANOVA to detect effects because it takes account of the correlations between the outcome variables (Huberty &amp; Morris, 1989) Centroids Concept A centre point in a multidimensional geometric space Mean centroid Concept A centre point that represents the mean on multiple dimensions A mean centroid is represented by a column vector Mathematics (Matrix) \\({\\boldsymbol{\\mu}} = \\begin{pmatrix} \\mu_{1} \\\\ \\mu_{2} \\\\ \\mu_{3} \\\\ \\vdots \\\\ \\mu_{d} \\end{pmatrix}\\) Where \\(d\\) - The total number of outcome variables (variates) Hotelling’s \\(t^2\\) (Hotelling, 1931) Concept Multivariate version of the t-statistic A test statistic for the difference between 2 mean centroids (or multivariate means; means on multiple outcome variables) Tests whether 2 means differ on the linear combination of multiple outcome variables Mathematics (Matrix) \\(t^2 = \\frac{n_{1}n_{2}}{n_{1}n_{2}}(\\mathbf{Y_{1}-\\mathbf{Y_2}})\\mathbf{&#39;}(\\mathbf{S})^{-1}(\\mathbf{Y_{1}-\\mathbf{Y_2}})\\) Where \\(\\mathbf{S}\\) - Variance covariance matrix \\(\\mathbf{Y_{1}}\\) - Mean centroid for group 1 \\(\\mathbf{Y_{2}}\\) - Mean centroid for group 2 The relationship between Mahalanobis \\(D^2\\) and Hotelling’s \\(t^2\\) Concept The relationship between Mahalanobis \\(D^2\\) and Hotelling’s \\(t^2\\) is like the relationship between Cohen’s \\(d\\) and Student’s \\(t\\) Mathematics \\(t^2 = D^2 (\\frac{n_{1}n_{2}}{n_{1}+n_{2}})\\) Distribution of Hotelling’s \\(t^2\\) Distribution of Hotelling’s \\(t^2\\) as itself Concept Hotelling’s \\(t^2\\) has a Hotelling’s T-squared distribution with degrees of freedom 1 of \\(p\\) and degree of freedom 2 of \\(n_{1} + n_{2} - 2\\) Mathematics \\(t^2 \\sim T^2(p, n_{1} + n_{2} - 2)\\) Distribution of Hotelling’s \\(t^2\\) as \\(F\\) Concept Hotelling’s \\(T^2\\) can be converted into an \\(F\\) statistic that has an \\(F\\) distribution with degrees of freedom 1 of \\(p\\) and degrees of freedom 2 of \\(n_{1} + n_{2} - p -1\\) Hence, you can convert Hotelling’s \\(T^2\\) into an \\(F\\) statistic and significance test with the \\(F\\) distribution Mathematics \\(F = t^2\\times\\frac{v-p+1}{p(v)}\\) Distribution of \\(F\\) \\(F = t^2\\times\\frac{v-p+1}{p(v)} \\sim F(p, n_{1} + n_{2} - p -1)\\) NHST Hypotheses \\(H_0: \\mathbf{Y_1} = \\mathbf{Y_2}\\) - The 2 mean centroids are the same - The 2 mean centroids are from the same population \\(H_1: \\mathbf{Y_1} ≠ \\mathbf{Y_2}\\) - The 2 mean centroids are not the same - The 2 mean centroids are from different populations Assumptions Assumptions of MANOVA Uncorrelated errors Random sampling Multivariate normality Homogeneity of Covariance Homogeneity of Covariance Concept The variance of each outcome variable and the covariance between any outcome variables is equal between all groups The variance-covariance matrices are identical between groups Mathematics \\(\\Sigma_1 = \\Sigma_2 = \\Sigma_3 = \\cdots = \\Sigma_k\\) Where \\(\\Sigma_k\\) - The variance Covariance matrix for group \\(k\\) \\(k\\) - The total number of groups in the MANOVA Effects of violation of this assumption Only under this condition that the sampling distribution of \\(t^2\\times\\frac{v-p+1}{p(v)}\\) has an \\(F\\) distribution with degrees of freedom 1 of \\(p\\) and degrees of freedom 2 of \\(n_{1} + n_{2} - p -1\\) Testing for Homogeneity of Covariance Statistical Tests for Homogeneity of Covariance Box M test Box M test Concept A test that test the null hypothesis that multiple covariance matrices are equal The Box M statistic Mathematics (Huberty &amp; Olejnik, 2006, p.41) \\(M = v_{e}\\ln{|\\mathbf{S_e}| - \\sum_{k=1}^{k}{v_{k}\\ln{|\\mathbf{S_k}|}}}\\) Where \\(k\\) - Number of groups \\(v_k\\) - Degrees of freedom for group k (which is \\(n_k - 1\\)) \\(v_e\\) - Error degrees of freedom (which is \\(\\sum_{k=1}^{k}{v_k}\\)) \\(\\mathbf{S_k}\\) - Covariance matrix for group \\(k\\) \\(\\mathbf{S_e}\\) - Error covariance matrix (Which is \\(\\frac{\\mathbf{E}}{v_e}\\)) Distribution of Box M Chi-squared transformed Box M Concept Box M can be transformed to a statistic that has a Chi-Squared distribution with degrees of freedom of \\(\\frac{(k-1)(p+1)p}{2}\\) Transforming Box M Mathematics For equal sample sizes \\(M_{\\chi} = M \\times \\left[ 1-\\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \\right]\\) For unequal sample sizes \\(M_{\\chi} = M \\times \\left[ 1-\\frac{2p^{2} + 3p -1}{6(p+1)(k-1)} \\times \\left( \\sum_{k=1}^{k}{v_{k}^{-1} - v_{e}^{-1}} \\right) \\right]\\) Distribution of \\(M_{\\chi}\\) \\(M_{\\chi} \\sim \\chi^2\\left(\\frac{\\left(k-1\\right)(p+1)p}{2}\\right)\\) F-transformed Box M Concept Box M is transformed to a statistic that has an \\(F\\) distribution Details can be found in Rencher (2002, p.255-259) Assumptions Uncorrelated errors Multivariate normality Robust multivariate test of difference Robust multivariate test of differences Yao Test Yao Test Concept A robust Hotelling’s \\(t^2\\) The Yao statistic Mathematics \\(T^{2}_{Yao} = (\\mathbf{Y_{1}-\\mathbf{Y_2}})\\mathbf{&#39;}\\left(\\frac{\\mathbf{S_1}}{n_1}+\\frac{\\mathbf{S_2}}{n_2}\\right)^{-1}(\\mathbf{Y_{1}-\\mathbf{Y_2}})\\) Distribution of Yao statistic F-transformed Yao Concept \\(T^{2}_{Yao}\\) can be transformed to a statistic that has an F distribution with degrees of freedom 1 of \\(p\\) and degrees of freedom of \\(f - p + 1\\) Mathematics $ T^{2}{Yao, F} = T^{2}{Yao} $ Where \\(f = \\sum_{k=1}^{k}{\\frac{1}{n_k - 1}\\left(\\frac{V_k}{T^{2}_{Yao}}\\right)^2}\\) Where \\(V_k = (\\mathbf{Y_{1}-\\mathbf{Y_2}})\\mathbf{&#39;} \\mathbf{W}^{-1} \\mathbf{W}_k \\mathbf{W}^{-1} (\\mathbf{Y_{1}-\\mathbf{Y_2}})\\) Where \\(\\mathbf{W}= \\sum_{k=1}^{k}{\\mathbf{W_{k}}}\\) \\(\\mathbf{W}_k = \\frac{\\mathbf{S}_k}{n_k}\\) Distribution of \\(T^{2}_{Yao, F}\\) \\(T^{2}_{Yao, F} \\sim F \\left( p, f-p+1\\right)\\) Multivariate ANOVA MANOVA Concept MANOVA - Multivariate Analysis of Variance The comparison of multiple group mean centroids The comparison of multiple group means on multiple outcome variables Hypotheses Null Hypothesis Multiple group mean centroids are identical (or are from the same population) \\(\\begin{alignat*}{1} &amp;H_0: ~~~~~\\boldsymbol{\\mu}_{1}&amp; &amp;= ~~~~~\\boldsymbol{\\mu}_{2}&amp; &amp;= ~~~~~\\boldsymbol{\\mu}_{3}&amp; &amp;= \\cdots&amp; &amp;= ~~~~~\\boldsymbol{\\mu}_{k}&amp; \\\\ &amp;H_0: \\begin{pmatrix} \\mu_{1, 1} \\\\ \\mu_{2, 1} \\\\ \\mu_{3, 1} \\\\ \\vdots \\\\ \\mu_{v, 1}\\end{pmatrix}&amp; &amp;= \\begin{pmatrix} \\mu_{1, 2} \\\\ \\mu_{2, 2} \\\\ \\mu_{3, 2} \\\\ \\vdots \\\\ \\mu_{v, 2}\\end{pmatrix}&amp; &amp;= \\begin{pmatrix} \\mu_{1, 3} \\\\ \\mu_{2, 3} \\\\ \\mu_{3, 3} \\\\ \\vdots \\\\ \\mu_{v, 3}\\end{pmatrix}&amp; &amp;= \\cdots&amp; &amp;= \\begin{pmatrix} \\mu_{1, k} \\\\ \\mu_{2, k} \\\\ \\mu_{3, k} \\\\ \\vdots \\\\ \\mu_{v, k}\\end{pmatrix}&amp; \\end{alignat*}\\) Where \\(\\mu_{v, k}\\) - Mean of group \\(k\\) on outcome \\(v\\) \\(k\\) - Group \\(v\\) - Variate Multivariate F-ratio: The \\(\\mathbf{HE}^{-1}\\) matrix Concept A matrix that represents the multivariate version of the F-ratio A matrix that represents the ratio of systematic variance in a set of outcome variables (the variance in the linear combination of multiple outcome variables explained by the model) to the unsystematic variance in the set of outcome variables (the variance in the linear combination of multiple outcome variables not explained by the model) Eigenvectors of the \\(\\mathbf{HE}^{-1}\\) matrix Each of the Eigenvectors of the \\(\\mathbf{HE}^{-1}\\) matrix represents each of the underlying variates and the Eigenvalue of each Eigenvector represents the effect of the associated variate in the form of F-ratio (ratio of variance explained to variance unexplained) Mathematics (Matrix) \\(\\mathbf{HE}^{-1}\\) Where \\(\\mathbf{H}\\) - Hypothesis SSCP matrix \\(\\mathbf{E}\\) - Error SSCP matrix Note \\(\\textbf{H}\\) and \\(\\textbf{E}^{-1}\\) are both symmetric, hence, the order in which they are multiplied does not matter, which means that \\(\\mathbf{HE}^{-1} = \\mathbf{E}^{-1}\\mathbf{H}\\) Eigenvalues of the \\(\\mathbf{HE}^{-1}\\) matrix Each of the eigenvalues of the \\(\\mathbf{HE}^{-1}\\) matrix represents the F-ratio of each of the underlying variates Test statistic for MANOVA Wilk Lambda Pillai-Bartlett trace Hotelling-Lawley trace Roy’s largest root Wilks lambda (Wilks, 1932) Wilks lambda statistic Introduction The oldest and perhaps the most commonly used criterion Mathematical description It is the ratio of the determinant of the Error SSCP matrix to the determinant of the Total SSCP matrix It represents the ratio of error variance to total variance (or the proportion of unexplained variance) Represents the ratio of the variance in all of the outcome variables not explained by the model to the total variance in all of the outcome variables Method 1 Description Mathematics \\(\\displaystyle \\Lambda = \\cfrac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|} = \\cfrac{|\\mathbf{E}|}{|\\mathbf{T}|}\\) Method 2 Description The product of the ratio of variance unexplained to total variance for all variates Mathematics \\(\\displaystyle \\Lambda = \\prod\\limits_{v=1}^{v}{\\frac{1}{1+\\lambda_{v}}}\\) Where \\(v\\) - Variate \\(\\lambda_{v}\\) - Eigenvalue for variate \\(v\\) Note \\(\\frac{1}{1+\\lambda_{v}}\\) changes the ratio of variance explained to the variance unexplained for variate \\(v\\) (\\(\\lambda_{v}\\)) to a ratio of variance unexplained to the total variance for variate \\(v\\) \\(\\begin{aligned} \\frac{1}{1+\\lambda_{v}} &amp;= \\frac{1}{1+\\frac{SS_{M,v}}{SS_{R,v}}} \\\\ &amp;= \\frac{1}{\\frac{SS_{R,v}}{SS_{R,v}}+\\frac{SS_{M,v}}{SS_{R,v}}} \\\\ &amp;= \\frac{SS_{R,v}}{SS_{R,v} + SS_{M,v}} \\\\ &amp;= \\frac{SS_{R,v}}{SS_{T,v}} \\end{aligned}\\) Distribution of Wilks lambda F-transformed Lambda Description Wilks lambda is transformed to a test statistic that has an F distribution F-transformed Lambda for two groups \\(\\Lambda_{F} = \\cfrac{1-\\Lambda}{\\Lambda}\\times \\cfrac{v_{e} - p + 1}{p} \\sim F\\left( p, v_e - p + 1\\right)\\) F-transformed Lambda for three groups \\(\\Lambda_{F} = \\cfrac{1-\\sqrt{\\Lambda}}{\\sqrt{\\Lambda}} \\times \\cfrac{v_{e} - p + 1}{p} \\sim F\\left( 2p, 2v_e - 2p + 2\\right)\\) Pillai-Bartlett trace Concept Aka Pillai’s trace Represents the ratio of variance in all the variates explained by the predictor variable to the total variance in all the variates \\(\\left( \\frac{SS_M}{SS_T} \\right)\\) It is the sum of the ratio of variance explained to the total variance of each of the variates Mathematics \\(\\displaystyle V = \\sum_{v=1}^{v}{\\frac{\\lambda_{v}}{1 + \\lambda_v}}\\) Note \\(\\frac{\\lambda_{v}}{1 + \\lambda_v}\\) The squared canonical correlation between the grouping variable and the \\(v^{th}\\) linear discriminant function This transforms \\(\\lambda_v\\) to the ratio of variance explained to the total variance \\(\\begin{aligned} \\frac{\\lambda_{v}}{1 + \\lambda_v} &amp;= \\frac{\\frac{SS_M}{SS_R}}{1 + \\frac{SS_M}{SS_R}} \\\\ &amp;= \\frac{\\frac{SS_M}{SS_R}}{\\frac{SS_R}{SS_R} + \\frac{SS_M}{SS_R}} \\\\ &amp;= \\frac{\\frac{SS_M}{SS_R}}{\\frac{SS_R + SS_M}{SS_R}} \\\\ &amp;= \\frac{\\frac{SS_M}{SS_R}}{\\frac{SS_T}{SS_R}} \\\\ &amp;= \\frac{SS_M}{SS_R} \\times \\frac{SS_R}{SS_T} \\\\ &amp;= \\frac{SS_M}{SS_T} \\end{aligned}\\) Distribution of Pillai’s trace F-transformed Pillai’s trace Concept Pillai’s trace is transformed to a test statistic that has an \\(F\\) distribution with degrees of freedom 1 of \\(br\\) and degrees of freedom of \\(r\\left( df_e - p + r \\right)\\) Mathematics \\(\\displaystyle V_F = \\frac{V}{r - V} \\times \\frac{v_e - p + r}{b}\\) Where \\(b = \\max{\\left( p, df_h \\right)}\\) \\(r = \\min{\\left( p, df_h \\right)}\\) Distribution of F-transformed Pillai’s trace \\(\\displaystyle V_F \\sim F \\left( br, r\\left( df_e - p + r \\right)\\right)\\) Hotelling-Lawley trace Concept It represents the ratio of variance explained to the variance unexplained It is the sum of the eigenvalues for all the variates Mathematics \\(\\displaystyle T = \\sum_{v=1}^{v}{\\lambda_{v}}\\) F-transformed Hotelling-Lawley trace Description \\(T\\) is transformed to a statistic so that it has a central \\(F\\) distribution Mathematics $T_F = T $ Distribution of F-transformed Hotelling-Lawley trace \\(\\displaystyle T_F \\sim F \\left( br, r(df_e - p -1) + 2 \\right)\\) Where \\(b = \\max \\left(p, df_h \\right)\\) \\(r = \\min \\left(p, df_h \\right)\\) Roy’s largest Root Concept Roy’s largest root suggests only to consider the largest eigenvalue (which is this \\(\\lambda_1\\), because the first eigenvalue is the largest one) and ignore the rest (if there are multiple eigenvalues) You can interpret it in anyway you like, for example, you can convert this this eigenvalue to a ratio of variance explained to the total variance using this \\(\\frac{\\lambda_1}{1+\\lambda_1}\\), the point is that you only need to consider the largest eigenvalue and do whatever you want with it This method tends to be advocated Because it has the largest ratio of variance explained to variance unexplained, it has the the most power Mathematics \\(\\Theta = \\lambda_1\\) F-transformed Roy’s largest Root Concept Roy’s largest root can be transformed to a statistic that has an \\(F\\) distribution Roy’s largest root is transformed to this statistic for NHST Mathematics \\(\\Theta_F = \\Theta \\times \\frac{N-b-1}{b}\\) Distribution of \\(\\Theta_F\\) \\(\\Theta_F \\sim F \\left( b, N - b - 1\\right)\\) Where \\(b = \\max{\\left( p, df_h\\right)}\\) Notes Rencher (2002) suggest that \\(df_e\\) might be used instead of \\(N\\) Central vs noncentral distribution Central distribution - Distributions that assume the null hypothesis is true Noncentral distribution - Distributions that assume the alterative hypothesis is true MANOVA effect sizes MANOVA effect sizes Multivariate Eta-squared Multivariate Omega-squared Tau-squared Xi-squared Zeta-squared Multivariate Eta-squared Concept The original Eta-squared generalized to multivariate contexts Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (proportion of variance explained) (Regardless of the number of constructs underlying the outcome variable) Mathematics Huberty and Olejnik (2006) \\(\\eta^{2}_{Multivariate}= 1 - \\Lambda\\) Cambridge \\(\\eta^{2}_{Multivariate}= 1 - \\Lambda^{\\frac{1}{r}}\\) Where \\(r = \\min\\left(k - 1, y\\right)\\) Where \\(k\\) - The number of levels of the factor \\(y\\) - The number of outcome variables Multivariate Eta-squared Concept The multivariate extension of the univariate Omega-squared Suggested by Tatsuoka (1970) Represents the ratio of variance in the set of outcome variables explained by the model to the total variance in the set of outcome variables (regardless of the number of constructs underlying the outcome variables) Mathematics \\(\\omega^{2}_{multivariate} = 1 - \\frac{N\\Lambda}{\\left( N - df_H - 1 \\right) + \\Lambda}\\) Tau-squared (Cramer &amp; Nicewander, 1979) Concept Mathematics Method 1 \\(\\displaystyle \\tau^2 = 1 - \\left( \\prod \\limits_{v=1}^{v}{1 - \\frac{\\lambda_v}{1+\\lambda_v}} \\right)^{\\frac{1}{r}}\\) Where \\(r = \\min{\\left(y, df_M \\right)}\\) Where \\(y\\) - The number of outcome variables \\(df_M\\) - Model degrees of freedom Note \\(\\left( \\prod \\limits_{v=1}^{v}{1 - \\frac{\\lambda_v}{1+\\lambda_v}} \\right)^{\\frac{1}{r}}\\) - This is the geometric mean proportion of variation in all of the variates that is not explained by the model Xi squared Concept This is the mean squared canonical correlation (mean proportion of variance in all the variates that is explained by the model; the variance explained per variate; it takes into account the number of variates estimated) Mathematics \\(\\xi^2 = \\frac{V}{s}\\) Where \\(s\\) - The number of variates estimated Adjustment for positive bias of variance-based effect size Serlin’s Adjustment (Serlin, 1982) Mathematics $ ^2_{adj} = 1- ( 1 - ^2 )$ Evaluation Kim and Olejnik (2005) provided simulation support for Serlin’s adjustment It can also be used with \\(\\tau^2\\) and \\(\\zeta^2\\) but they work best when there are only 2 levels and when \\(df_M ≥ 2\\) if the sample is large Hotelling-Lawley Effect size (Zeta-squared) Description This effect size measure is associated with the Hotelling-Lawley test statistic It takes into account the number of variates estimated, it represents the variance explained per variate (the average variance explained by each variate) Mathematics \\(\\displaystyle \\zeta^2 = \\frac{V}{s + V}\\) Where \\(s = \\min \\left( y, df_M\\right)\\) Where \\(y\\) - The number of variates estimated Canonical Correlation Description The correlation between the linear combination of predictor variables and the linear combination of outcome variables In other words, it is the correlation between the set of predictor variables and \\(v^{th}\\) variate Mathematics \\(r_c = \\sqrt{V}\\) ans\\(adj.r.squared &lt;- 1 - (1 - ans\\)r.squared) * ((n - df.int)/rdf) Positive bias of proportion of variance explained Concept The sample proportion of variance explained is a positively-based estimator of the population proportion of variance explained There are methods for adjusting for this bias Moderators of this bias The degree of positive bias increases as The number of levels of the predictor increases The number of outcome variables increases The group sample size decreases Adjustments for \\(R^2\\) The Smith formula (Ezekiel, 1928) Mathematics \\(\\displaystyle R_{adj}^{2} = 1 - \\left( 1 - R^2\\right) \\times \\frac{n}{n-p}\\) Where \\(p\\) - The number of predictors The Wherry formula-1 Mathematics \\(\\displaystyle R_{adj}^{2} = 1 - \\left( 1 - R^2\\right) \\times \\frac{n-1}{n-p-1}\\) The Wherry formula-2 Introduction This is implemented by many statistical packages (e.g. SAS, SPSS, R) lm() uses this Mathematics \\(\\displaystyle R_{adj}^{2} = 1 - \\left( 1 - R^2\\right) \\times \\frac{n-1}{n-p}\\) The Olkin and Pratt formula (Olkin &amp; Pratt, 1958) Mathematics \\(\\displaystyle R_{adj}^{2} = 1 - \\frac{\\left( N-3 \\right) ( 1-R^2 )}{N - p - 1} \\times \\left[ 1 + \\frac{2 (1 - R^2)}{N - p + 1}\\right]\\) The Pratt formula Mathematics \\(\\displaystyle R^2 = 1 - \\frac{(N - 3)(1 - R^2)}{N - p - 1} \\times \\left[ 1 + \\frac{2 (1 - R^2)}{N - p - 2.3}\\right]\\) The Claudy formula-3 Mathematics \\(\\displaystyle R_{adj}^{2} = 1 - \\frac{(N - 4)(1 - R^2)}{N - p - 1} \\times \\left[ 1 + \\frac{2 (1 - R^2)}{N - p + 1}\\right]\\) Which is the best?? Yin and Fan (2001) conducted a simulation study in which they compared the 6 methods, they found that when the \\(\\frac{N}{p}\\) is large, almost all of those 6 methods were unbiased, and the Pratt formula better than all the others especially when the \\(\\frac{N}{p}\\) ratio is small. So Pratt’s method is probably the way to go, but most statistical packages don’t use it… (Also see Stackexchange) Contrasts Concept Contrasts are operationalised as a linear combination in which each term is the product of a group mean and its weight A specific contrast is defined by the specific weights set for each mean such that it compares a specific pair of means Mathematics \\(\\displaystyle \\begin{aligned} \\psi =&amp; \\sum_{k=1}^{k}{w_{k}\\bar{x}_{k}} \\\\ \\psi =&amp; w_{1}\\bar{x}_{1} + w_{2}\\bar{x}_{2} + w_{3}\\bar{x}_{3} + \\cdots + w_{k}\\bar{x}_{k} \\end{aligned}\\) Where \\(k\\) - The number of levels of the categorical predictor \\(w_k\\) - Weight for the mean of group \\(k\\) \\(\\bar{x}_k\\) - The mean of group \\(k\\) Note \\(\\displaystyle \\sum_{k = 1}^{k}{w_k} = 0\\) Example A 3-group example \\(\\psi = w_{1}\\bar{x}_{1} + w_{2}\\bar{x}_{2} + w_{3}\\bar{x}_{3}\\) A comparison of group 1 and 3 \\(\\psi = 1\\bar{x}_{1} + 0\\bar{x}_{2} - 1\\bar{x}_{3}\\) A comparison of group 2 and 1 \\(\\psi = -1\\bar{x}_{1} + 1\\bar{x}_{2} + 0\\bar{x}_{3}\\) Sampling variance of a contrast Mathematics \\(\\displaystyle s_{\\psi}^{2} = s_{e}^{2}\\sum_{k=1}^{k}{\\frac{w_k}{n_k}}\\) Where \\(s_{e}^{2}\\) - Error variance - The error variance is based on all \\(k\\) groups in the study - This is the error variance across all the groups Standard error of a contrast Mathematics \\(\\displaystyle s_{\\psi} = \\sqrt{s_{e}^{2}\\sum_{k=1}^{k}{\\frac{w_k}{n_k}}}\\) Student’s \\(t\\) of a contrast Mathematics \\(\\displaystyle t = \\frac{\\psi}{s_{\\psi}}\\) Multivariate contrasts Concept A comparison between any two group centroids A specific multivariate contrast is defined by the specific set of weights for the mean centroids Mathematics \\(\\begin{aligned}\\displaystyle \\boldsymbol{\\psi} &amp;= \\sum_{k=1}^{k}{w_{k}\\mathbf{y}_{k}} \\\\ &amp;= w_{1}\\mathbf{y}_{1} + w_{2}\\mathbf{y}_{2} + w_{3}\\mathbf{y}_{3} + \\cdots + w_{k}\\mathbf{y}_{k} \\end{aligned}\\) Where \\(\\boldsymbol{\\psi}\\) - A column vector representing a contrast \\(w_k\\) - A scalar or weight for group \\(k\\) \\(\\mathbf{y}_k\\) - Mean centroid of group \\(k\\) Test statistics for multivariate contrasts Hotelling’s \\(T^2\\) Mathematics \\(\\displaystyle T^{2}_{\\psi} = \\boldsymbol{\\psi}&#39; \\left( \\sum_{k = 1}^{k}{\\textbf{S}_e \\frac{w_{k}^{2}}{n_k} } \\right)^{-1} \\boldsymbol{\\psi}\\) Where \\(\\textbf{S}_e\\) - Error variance-covariance matrix \\(w_k\\) - Weight of group \\(k\\) \\(n_k\\) - Sample size of group \\(k\\) Multivariate \\(F\\) The multivariate \\(F\\) Mathematics \\(\\mathbf{H_{\\psi}}\\mathbf{E}^{-1}\\) The multivariate F test statistic Wilks lambda Pillai’s trace Roy’s largest root Hotelling-Lawley SSCP matrix for contrasts Hypothesis SSCP for a contrast Mathematics \\(\\displaystyle \\begin{aligned}\\mathbf{H_{\\psi}} &amp;= \\frac{1}{\\sum_{k=1}^{k}{\\frac{w_{k}^{2}}{n_k}}} \\left(\\boldsymbol{\\psi \\psi&#39;} \\right) \\\\ &amp;= \\begin{bmatrix} SS_{\\psi, y_{1}} &amp; CP_{\\psi, y_{1}, y_{2}} &amp; CP_{\\psi, y_{1}, y_{3}} &amp; \\cdots &amp; CP_{\\psi, y_{1}, y_{y}} \\\\ CP_{\\psi, y_{2}, y_{1}} &amp; SS_{\\psi, y_{2}, y_{2}} &amp; CP_{\\psi, y_{2}, y_{3}} &amp; \\cdots &amp; CP_{\\psi, y_{2}, y_{y}} \\\\ CP_{\\psi, y_{3}, y_{1}} &amp; SS_{\\psi, y_{3}, y_{2}} &amp; SS_{\\psi, y_{3}} &amp; \\cdots &amp; CP_{\\psi, y_{3}, y_{y}} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ CP_{\\psi, y_{y}y_{1}} &amp; CP_{\\psi, y_{y}, y_{2}} &amp; CP_{\\psi, y_{y}, y_{3}} &amp; \\cdots &amp; SS_{\\psi, y_{y}}\\end{bmatrix} \\end{aligned}\\) Notes When sample sizes are equal, it is simplified to \\(\\displaystyle \\mathbf{H_{\\psi}} = \\frac{n}{\\sum_{k=1}^{k}{w_{k}^{2}}} \\left(\\boldsymbol{\\psi \\psi&#39;} \\right)\\) Contrasts Complex contrasts Concept Comparing 2 sets of group means (not just 2 group means) Mathematics \\(\\psi_{ab} = w_{a}\\left(\\sum_{k=1}^{k_a}{\\bar{x}_k} \\right) - w_{b}\\left(\\sum_{k=1}^{k_b}{\\bar{x}_k} \\right)\\) Where \\(\\psi_{ab}\\) - Contrast for set \\(a\\) set \\(b\\) \\(w_a\\) - The weight for the \\(a\\) set of group means \\(w_b\\) - The weight for the \\(b\\) set of group means \\((\\sum_{k=1}^{k_a}{\\bar{x}_k} \\right)\\) - The sum of means in the \\(a\\) set \\((\\sum_{k=1}^{k_b}{\\bar{x}_k} \\right)\\) - The sum of means in the \\(b\\) set Notes Weights are summed to 0 Concept The sum of the weights must be 0 Mathematics \\(w_{a} + w_{b} = 0\\) Ensuring weights sum to 0 \\(\\displaystyle w_a = \\frac{k_b}{k}\\) \\(\\displaystyle w_b = \\frac{k_a}{k}\\) Where \\(k_b\\) - The number of groups/levels in set \\(b\\) \\(k_a\\) - The number of groups/levels in set \\(a\\) \\(k\\) - The total number of groups/levels of the categorical predictor variable Example A 5-group example A complex contrast comparing group 1 and 2 with group 3, 4, and 5 \\(\\displaystyle \\begin{aligned}\\psi_{ab} &amp;= w_{a}\\left(\\bar{x}_{1} + \\bar{x}_{2}\\right) - w_{b}\\left(\\bar{x}_{3} + \\bar{x}_{4} + \\bar{x}_{5} \\right) \\\\ &amp;= \\frac{3}{5}\\left(\\bar{x}_{1} + \\bar{x}_{2}\\right) - \\frac{2}{5}\\left(\\bar{x}_{3} + \\bar{x}_{4} + \\bar{x}_{5} \\right) \\\\ &amp;= \\frac{3}{5}\\bar{x}_{1} + \\frac{3}{5}\\bar{x}_{2}- \\frac{2}{5}\\bar{x}_{3} - \\frac{2}{5}\\bar{x}_{4} - \\frac{2}{5}\\bar{x}_{5} \\end{aligned}\\) Discriminant Function Analysis Concept The process of finding a variate that can best separate groups (maximising the \\(F\\) ratio for the groups) so that it can best discriminate between groups§ The groups are transformed in a way that their \\(F\\) ratio are maximised The first variate is calculated by maximising the \\(F\\) ratio, each of the subsequent variates is calculated after all preceding variates are partialled out. This means that the variates are orthogonal to each other Variate Concept A linear combination of outcome variables A variate is also called a linear discriminant function to reflect its goal of discriminating groups In a discriminant function analysis, different variates are identified and they are defined by the specific set of weights for the outcome variables (outcome variables are combined differently) Variates are latent variables (aka underlying dimensions/constructs of the outcome variables) Mathematics \\(\\displaystyle \\begin{aligned} V &amp;= \\sum_{p=1}^{p}{\\beta_{p}y_{p}} \\\\ &amp;= \\beta_{1}y_{1} + \\beta_{2}y_{2} + \\beta_{3}y_{3} + \\cdots + \\beta_{p}y_{p}\\end{aligned}\\) Where The eigenvectors and eigenvalues of the \\(\\mathbf{HE}^{-1}\\) matrix Concept Each of the eigenvectors of the \\(\\mathbf{HE}^{-1}\\) matrix represents a variate and the components in an eigenvector are the set of weights for the outcome variables in the variate and the eigenvalue associated with the eigenvector represents the ratio of variance in the variate explained by the predictor variable(s) to the variance in that variate unexplained by the predictor variable(s) Mathematics \\(\\displaystyle \\begin{aligned}\\lambda_v &amp;= \\frac{SS_{M_{v}}}{SS_{R_{v}}} \\\\ &amp;= \\frac{\\sum_{k = 1}^{k}{n_k (\\bar{x}_{k} - \\bar{x}_{grand})}}{\\sum_{n=1, k = 1}^{n, k}{(x_{i, k} - \\bar{x}_{k})}} \\end{aligned}\\) Example \\(\\lambda_1\\) is the ratio of the variance on LDF 1 explained by the predictor variable(s) to the variance on LDF 1 not explained by the predictor variable(s) Factor loading Concept Aka structure r’s The relationship between each component of a dimension/construct and the dimension/construct itself Factor loading in discriminant analysis Concept The relationship between each outcome variable in the variate and the variate itself (sometimes called the LDF-Variable correlations) That is how much each outcome variable in the variate contribute to the variate Assessing factor loading Methods Total group structure correlations Between-group structure correlation Within-group structure correlation Which is the best one?? Huberty and Olejnik (2006) advocates within-group structure correlation as it takes into account the group differences of mean vectors (which is meaningful) Interpreting variates Label of a variate The label of a variate should be based on the outcome variables with higher factor loading because these outcome variables are strongly contributing to the variate Dropping variates You may choose to drop the variates that are less meaningful (e.g. those with low eigenvalues) The LDF plot Concept A plot in which each of the axes represents each of the LDFs/variates and the scores represent scores of the original data that are transformed into variate scores (by putting the original scores into the LDFs) Types of LDF plot The mean LDF plot The mean LDF plot Description An LDF plot which plots the group LDF mean centroids (the group mean centroid but transformed into variate score) It can be used to determine the number of LDFs to retain for interpretation Constructing the mean LDF plot For group g on variate v, put the original group means on the discriminant variables into the corresponding places in the vth linear discriminant function, the result of that is the mean for group g on variate v Dropping/retaining variates Methods for dropping/retaining variates Proportion of variance based "],["catgorical-outcomes.html", "Chapter 7 Catgorical outcomes", " Chapter 7 Catgorical outcomes Analysing Categorical Outcomes Concept The process of analysing categorical outcomes Statistics Binomial test Pearson’s Chi-squared Cochran-Mantel-Haenszel Chi-squared test McNemar’s test Fisher’s Exact Test Barnard’s exact test Boschloo’s test G-test Cramér’s V Pearson’s Chi-squared test (Pearson, 1900) Concept A statistical test that tests the null hypothesis that 2 categorical variables are independent Test of independence between 2 categorical variable is like a test of homogeneity of conditional distributions For example, if factor A and factor B are independent, then the conditional probabilities are homogeneous (\\(P(\\text{Married once} | \\text{College}) = P( \\text{Married once}| \\text{No College})\\)) Conceptual Hypotheses Null Hypothesis The two categorical variables are independent Alternative Hypothesis The two categorical variables are not independent (they are associated) The probability of each cell under the null hypothesis Concept The probability of each joint-category under the null hypothesis Mathematics \\(\\pi_{ij} = \\pi_{i\\cdot} \\pi_{\\cdot j}\\) Estimation Concept The maximum likelihood estimation of the population probability of each of the joint-categories Mathematics \\(\\displaystyle \\hat{\\pi}_{ij} = \\hat\\pi_{i\\cdot} \\hat\\pi_{\\cdot j} = \\frac{n_{i \\cdot}}{n} \\times \\frac{n_{\\cdot j}}{n} = \\frac{n_{i \\cdot} n_{\\cdot j}}{n^2}\\) Where \\(n_{i\\cdot}\\) - The number of observations in the \\(i\\)th row \\(n_{\\cdot j}\\) - The number of observations in the \\(j\\)th column \\(n\\) - The total sample size The expected number of observations Concept The number of observations in each of the K joint-categories under the null hypothesis that the 2 categorical variables are independent Mathematics \\(\\displaystyle E(n_{ij}) = n\\hat{\\pi}_{ij} = n\\frac{n_{i \\cdot} n_{\\cdot j}}{n^2} = \\frac{n_{i \\cdot} n_{\\cdot j}}{n}\\) Pearson’s Chi-squared statistic Concept The sum of the standardised squared deviations of the observed number of observations and the expected number of observations under the null hypothesis of each of the K joint-categories Mathematics \\(\\begin{aligned} \\displaystyle \\chi^2 &amp;= \\sum_{i = 1}^{I}{\\sum_{j = 1}^{J}{\\frac{\\left[ n_{ij} - E(n_{ij})\\right]^2}{E(n_{ij})}}} \\\\ \\displaystyle &amp;= \\sum_{i = 1}^{I}{\\sum_{j = 1}^{J}{\\frac{\\left( n_{ij} - \\frac{n_{i\\cdot} n_{\\cdot j}}{n}\\right)^2}{\\frac{n_{i\\cdot} n_{\\cdot j}}{n}}}} \\end{aligned}\\) Where \\(n_{ij}\\) - The observed number of observations in joint-category \\(ij\\) \\(E(n_{jk}\\) - The expected number of observations in joint-category \\(ij\\) under the null hypothesis \\(\\left( n_{ij} - \\frac{n_{i\\cdot} n_{\\cdot j}}{n}\\right)^2\\) - The squared deviation for for joint-category \\(ij\\) Degrees of freedom (NOT ENTIRELY SURE) Concept The degrees of freedom under the null hypothesis is the number of joint-categories minus the number of independent linear restrictions placed on the cell probabilities Although the calculation of the number of degrees of freedom depends on the application of the Chi-squared test, the general principle is that the appropriate number of degrees of freedom will equal the number of joint-categories minus 1 df for each independent linear restriction placed on the cell probabilities. In some applications, other restrictions may also be introduced because of the necessity for estimating unkown parameters required in the calculation of the expected cell frequencies or because of the method used to collect the sample There is always one linear restriction in each cell because the sum of the cell probabilities must equal 1 (\\(p_1 + p_2 + p_3 + \\cdots + p_k = 1\\)) Mathematics \\(df = IJ - 1 - (I-1) - (J-1) = (I-1)(J-1)\\) Note The equation shows that df is the total number of joint-categories (\\(IJ\\)) minus the number of restrictions in the \\(I\\) rows, minus the number of restrictions in the \\(J\\) columns, and minus the number of restrictions in all the cells as a whole (minus the 1) (NOT ENTIRELY SURE) Introduction The calculation of the number of degrees of freedom depends on the application of the Chi-squared test The general principle is that the appropriate number of degrees of freedom will equal the number of cells, k, less 1 df for each independent linear restriction placed on the cell probabilities There is always one linear restriction in each cell because the sum of the cell probabilities must equal 1 (\\(p_1 + p_2 + p_3 + \\cdots + p_k = 1\\)) Distribution of the Chi-squared statistic Concept \\(X^2\\) tends towards a chi-squared distribution as n increases (it can be shown) The square root of the chi-squared statistic is a normal random variable (normally distributed) The square of a normal random variable has a chi-squared distribution Hence, the chi-squared statistic has a chi-square distribution Mathematics \\(X^2 \\sim \\chi\\) NHST Concept Tests the probability of having a Chi-squared statistic equal to or larger than the observed Chi-squared statistic in the Chi-square distribution under the null hypothesis Hypotheses \\(H_0: \\pi_{ij} = \\pi_{i \\cdot}\\pi_{\\cdot j}, ~~~~~ \\text{for} ~ i = 1:I, j = 1:J\\) Some caveats Number of cell counts There is a rule of thumb that it is all expected cell counts are required to be at least 5. Although Cochran (1952) noted that his can be as low as one for some situations Odds Ratio Odds Concept The ratio of the probability of an event occurring to the probability of the event not occurring Mathematics \\(\\displaystyle Odds(\\text{event}) = \\frac{P(\\text{event})}{1 - P(\\text{event})}\\) Conditional Odds Concept The ratio of the probability of an event occurring given a value/level of another categorical variable to the probability of it not occurring Mathematics \\(\\displaystyle Odds(A | x) = \\frac{P(A | x)}{1 - (A | x)}\\) The Odds Ratio Concept The ratio of the odds of an event given a value of a categorical variable and the odds of the same event given another value of the same categorical variable This ratio tells the influence of the level of the second categorical variable in the numerator have on the probability of the event of interest Mathematics \\(\\displaystyle OR(A) = \\frac{Odds(A|x_1)}{Odds{(A|x_2)}}\\) Bernoulli Trails Concept A Bernoulli trial is a random independent experiment with 2 possible outcomes (usually referred to as “success” or “failure”) and the probability of the “success” event (\\(\\pi\\)) is the same every time the experiment is conducted (identical trials) and the outcome of each of the trials does not influence the outcome of other trials (independent trials; sampling with replacement) \\(y_i\\) - The outcome of Bernoulli trial \\(i\\) in the sequence of \\(n\\) trials - 1 indicates that the “success” event had occurred and 0 indicates that the “failure” event had occurred in trial \\(i\\) \\(y\\) - The total number of “success” events in a sequence of \\(n\\) trials - \\(y = \\sum_{i = 1}^{n}{y_i}\\) \\(y\\) is treated as an independent random variable that can vary between the 2 possible outcomes (the “success” event or the “failure” event) Bernoulli process - When there is a sequence of Bernoulli trials \\(p= 1-q\\) \\(p\\) - The probability of event occurring \\(q\\) - The probability of the event not occurring Binomial Distribution Binomial Distribution Concept A special case of the multinomial distribution when the number of trials \\(K = 2\\) A discrete probability distribution for the discrete independent random variable (\\(y\\)) of the number of the “success” events in a sequence of a fixed/finite \\(n\\) number of Bernoulli trials/experiments (independent, identical) (the x-axis represents the number of successes; the y-axis represents the probability) (if trials are not independent or identical, \\(y\\) will not follow a binomial distribution, they will follow other distributions; e.g. hypergeometric distribution is for the case when trials are not independent; specifically, it samples without replacement) A Bernoulli distribution is a special case of the binomial distribution where \\(n = 1\\) The binomial distribution has 2 parameters, \\(n\\), the number of Bernoulli experiments in the sequence, and \\(\\pi\\), the probability of each of the numbers of “success” events An independent random variable (\\(y\\)) that has an approximate binomial distribution is denoted as \\(y \\sim B(n, \\pi)\\) Binomial probability mass function Concept The probability mass function for the binomial distribution Mathematics \\(P(y) = {n\\choose y} \\pi^y (1 - \\pi)^{n - y} ~~~, y = 0:n\\) Notes \\({n\\choose y}\\) - The binomial coefficient Parameters of the binomial distribution Mean Mathematics \\(\\mu = \\text{E}(y) = n\\pi\\) Variance Mathematics \\(\\sigma^2 = n\\pi(1-\\pi)\\) Characteristics of the binomial distribution \\(\\pi\\) and Skewness (biasedness) Concept The binomial is symmetric when \\(\\pi = 0.50\\) regardless of \\(n\\) The binomial distribution becomes more positively skewed as \\(\\pi\\) decreases from \\(0.50\\) towards 0 The binomial distribution becomes more negatively skewed as \\(\\pi\\) increases from \\(0.50\\) towards 1 Mathematics The skewness is described by the following equality: \\(\\displaystyle \\frac{\\text{E}(y-\\mu)^3}{\\sigma^3} = \\frac{1-2\\pi}{\\sqrt{n\\pi(1 - \\pi)}}\\) \\(n\\) and normality Concept The binomial distribution converges to normality as \\(n\\) increases For fixed \\(\\pi\\), the binomial distribution can be reasonably assumed to be normal when \\(n[\\min(\\pi, 1–\\pi)]\\) is as small as about 5 (Agresti, 2013, p.5) Statistical Inference with the Binomial distribution (Not developed) Wald test Concept A test statistic in which the standard error nonnull estimated (not the standard error of population distribution under the null hypothesis) The Wald statistic for probability Mathematics $W = $ The Wald z statistic \\(\\displaystyle z_W = \\frac{p - \\pi}{SE} = \\frac{p - \\pi}{\\frac{\\sqrt{\\hat\\pi(1 - \\hat\\pi)}}{n}}\\) Multinomial distribution Concept A discrete probability distribution for the discrete independent random variable (\\(y\\)) of the combination of numbers with each number being the number of occurrence of each of the multiple possible outcomes in a sequence of a fixed/finite \\(n\\) number of random independent and identical trial/experiment The categorical distribution is a special case of the multinomial distribution where \\(n = 1\\) The probability of obtaining a combination of \\(n_1, n_2, \\cdots, n_k\\) is denoted as \\(\\text{P}(n_1, n_2, \\cdots, n_k)\\) where \\(n_k\\) is the number of occurrence for outcome \\(k\\) in a sequence of \\(n\\) random, independent, and identical trials Alternative text In 1 trial, there are multiple (but fixed) number of possible outcomes, the outcome of a trial is represented by a combination (e.g. if there are 4 possible outcomes, the combination could be 0, 1, 0, 0, meaning that the outcome for this trial is category 2). Now imagine we have \\(16\\) of this trial, the combination could be something like 4, 4, 4, 4. The multinomial distribution models the probability of getting different combinations. In the case of 4, 4, 4, 4, this is a fair trial (the null hypothesis), where each of the categories have equal chance of occurrence, so this combination after \\(16\\) trials would be the most probable (it has a probability of 0.50), whereas a combination of something like 6, 5, 3, 2 would be less probable in the null multinomial distribution The outcome of a single multinomial trial (\\(y_i\\)) Concept The outcome of a single random, independent and identical multinomial trial The outcome can be one of multiple but fixed number of possible outcomes Mathematics \\(y_i = (y_{i,1}, y_{i,2}, \\cdots , y_{i,k})\\) Where \\(k\\) - The number of possible outcomes of a single trial The outcome of a sequence of \\(n\\) multinomial trials Concept The number of occurrence of each of the \\(k\\) outcomes after a sequence of \\(n\\) random, independent, and identical trials This is represented as a combination of \\(k\\) numbers with each number representing the number of occurrence of each of the \\(k\\) outcomes after a sequence of \\(n\\) random, independent, and identical trials Mathematics \\(y = (n_1, n_2, \\cdots , n_k)\\) Multinomial probability mass function Concept A function that models the probability of each of all possible outcome combinations The multinomial distribution has \\(K - 1\\) dimensions because \\(y_{i,k}\\) is linearly dependent on the others and therefore is redundant Mathematics \\(\\displaystyle \\text{P}(n_1, n_2, \\cdots , n_{k-1}) = \\left( \\frac{n!}{\\Pi_{k = 1}^{K}{n_k!}} \\right) \\Pi_{k = 1}^{K}{\\pi_{k}^{n_k}}\\) Parameters of a multinomial distribution Expectation Mathematics \\(E(n_j) = n\\pi_j\\) Variance Mathematics \\(\\sigma^2_{n_j} = n\\pi_j(1-\\pi_j)\\) Covariance between any two outcomes Mathematics \\(\\text{Cov}(n_j, n_k) = -n\\pi_j\\pi_k\\) Poisson distribution History Introduced by Poisson (1781 - 1840) and published with his probability theory in his work Recherches sur la probabilité des judgements en matié criminelle et en matiére civile (1837) where he theorised about the number of wrongful convictions in a given country by fousing on certain random variables N that count the number of discrete occurrenses that take place during a time-interval of given length Newcomb (1860) applied the Poisson distribution to estimate the distribution of the number of stars found in a unit of space Bortkiewicz (1898) applied the Poisson distribution to estimate the number of soldiers in the Prussian army killed accidentally by horse kicks (this experiment introduced the Poisson distribution to the field of reliability engineering) Concept A discrete probability distribution of the discrete independent random variable (\\(y\\)) of the number of the “success” events in a sequence of an infinite number of Bernoulli trials/experiments (independent, identical) within a fixed time interval and space The Poisson distribution is used for the number of events that occur randomly over a fixed window of time or space when outcomes in disjoint periods or regions are independent The Poisson distribution can be used for the binomial case when \\(n\\) is larger and \\(\\pi\\) is small (then \\(\\mu = n\\pi\\)) The Poisson distribution is conceptually similar to the binomial distribution, the difference is that the binomial distribution has a fixed number of trials (\\(n\\)), meanwhile, the Poisson distribution has an infinite number of trials (an infinite population of trials) Poisson probability mass function Mathematics \\(\\displaystyle \\text{P}(y) = \\frac{e^{-\\mu}\\mu^{y}}{y!}~~~ y = 0:.\\) Where \\(y\\) - The number of times an event occurred (sometimes it is denoted as \\(k\\)) \\(\\mu\\) - The expected number of times an event occurred (sometimes it is denoted as \\(\\lambda\\)) *Notes Sometimes it is expressed as such Mean of the Poisson PMF Concept The number of occurrence of an event in the null Poisson distribution Mathematics \\(\\mu = \\text{E}(y_i) = \\lambda\\) Mode of the Poisson distribution Concept The mode equals to the integer part of \\(\\mu\\) Variance of the Poisson PMF Concept The variance of the Poisson PMF is the same as its mean Mathematics \\(\\text{Var}(y_i) = \\text{E}(y_i)\\) Skewness The Poisson distribution approaches normal as \\(\\mu\\) increases (when \\(\\mu\\) is at least 10 it can be assumed to be normal) Mathematics The skewness is described by the following equality \\(\\displaystyle \\frac{\\text{E}(y - \\mu)^3}{\\sigma^3} = \\frac{1}{\\sqrt{\\mu}}\\) Binomial and Poission distribution and the problem of Overdispersion Concept Modelling count observations with binomial or Poission distributions often result in overdispersion. \\(\\mu\\) can vary because of unmeasured factor Unconditionally, The Expectation is \\(E(y) = E[E(y | \\mu)]\\) The variance is \\(\\text{Var}(y) = E[\\text{Var}(y|\\mu)] + \\text{Var}[E(y|\\mu)]\\) Overdispersion Concept The phenomenon in which the observed variability in the real world is larger than the variability predicted by the statistical model This may cause inaccurate inferences Overdispersion is a common feature because in practice populations are frequently heterogeneous Count observations using the binomial or Poission distribution often encounter this problem Likelihood Likelihood Concept The probability of the observed sample data/datum as a function of the parameters of the chosen statistical model/distribution In the case of data (when there are multiple observations), it is the joint probability between the observations under the statistical model/distribution given the values of the parameters of the statistical model/distribution Related terms Kernel - The part of a likelihood function that involves the model parameters (the relevant part of the likelihood function) Mathematics \\(\\displaystyle \\mathcal{L}(\\theta | X) = \\text{PF}(x_i)\\) Where \\(\\theta\\) - The parameter of the chosen statistical model \\(X\\) - The observed sample data Notes It can also be written as \\(\\text{P}(X | \\theta)\\), but this is less commonly used The likelihood function of a random sample Concept A function that describes the likelihood of a random sample data/datum as a function of the values of the parameters of the chosen statistical model In other words, it describes the probability of getting the sample data/datum under different values of the parameters of the statistical model The graph of the likelihood function is the likelihood against the values of the parameter(s) of the chosen statistical model It treats the sample data/datum is given and the parameter is a variable It is the joint probability between the observations at different values of the parameter of the model The graph is concave for most models Mathematics $(; ) = _{i = 1}^{n}{f(x_i; )} $ Where \\(f(x_i; \\theta)\\) - The probability density function (PDF) for the variable of interest (in this example here the statistical model has only one parameter, \\(\\theta\\), when it has two parameters, let’s say \\(\\sigma^2\\), then it is expressed as \\(f(x_i; \\theta, \\sigma^2)\\)) \\(\\mathbf{x} = (x_1, x_2, x_3, \\cdots , x_n)&#39;\\) - The observed random variable/data Notes \\(\\mathcal{L}(\\theta; \\mathbf{x})\\) can be denoted as \\(\\mathcal{L}(\\theta)\\) Example The beginning scenario You have 5 observations in your data and you are hypothesising that they came from a common population with pdf \\(f(x; \\theta)\\) The likelihood function is: $(; ) = _{i = 1}^{n}{f(x_i; )} $ Find the likelihood under all possible values of the parameter Examples: The likelihood of getting this sample data given that the true value of the parameter is 1 \\(\\displaystyle \\mathcal{L}(\\theta = 1; \\mathbf{x}) = f(x_1; \\theta= 1) \\times f(x_2; \\theta= 1) \\times f(x_3; \\theta= 1) \\times f(x_2; \\theta= 1) \\times f(x_3; \\theta= 1)\\) The likelihood of getting this sample data given that the true value of the parameter is 2 \\(\\displaystyle \\mathcal{L}(\\theta = 2; \\mathbf{x}) = f(x_1; \\theta= 2) \\times f(x_2; \\theta= 2) \\times f(x_3; \\theta= 2) \\times f(x_2; \\theta= 2) \\times f(x_3; \\theta= 2)\\) The likelihood of getting this sample data given that the true value of the parameter is 3 \\(\\displaystyle \\mathcal{L}(\\theta = 3; \\mathbf{x}) = f(x_1; \\theta= 3) \\times f(x_2; \\theta= 3) \\times f(x_3; \\theta= 3) \\times f(x_2; \\theta= 3) \\times f(x_3; \\theta= 3)\\) The likelihood of getting this sample data given that the true value of the parameter is 4 \\(\\displaystyle \\mathcal{L}(\\theta = 4; \\mathbf{x}) = f(x_1; \\theta= 4) \\times f(x_2; \\theta= 4) \\times f(x_3; \\theta= 4) \\times f(x_2; \\theta= 4) \\times f(x_3; \\theta= 4)\\) The likelihood of getting this sample data given that the true value of the parameter is 5 \\(\\displaystyle \\mathcal{L}(\\theta = 5; \\mathbf{x}) = f(x_1; \\theta= 5) \\times f(x_2; \\theta= 5) \\times f(x_3; \\theta= 5) \\times f(x_2; \\theta= 5) \\times f(x_3; \\theta= 5)\\) The likelihood of getting this sample data given that the true value of the parameter is 6 \\(\\displaystyle \\mathcal{L}(\\theta = 6; \\mathbf{x}) = f(x_1; \\theta= 6) \\times f(x_2; \\theta= 6) \\times f(x_3; \\theta= 6) \\times f(x_2; \\theta= 6) \\times f(x_3; \\theta= 6)\\) Plot these likelihoods into a plot of likelihood against the value of the parameter The likelihood graph will usually be concave This is the likelihood graph for this sample Log likelihood function Concept The log of the likelihood/likelihood function The log likelihood function is often used instead of the likelihood function because it is more convenient to use and it simplifies many of the later calculations (e.g. it is a sum rather than product of terms) No information is lost from logging the likelihood because the log is a one-to-one function Since the log is a strictly increasing function, the values of the model parameters that maximise \\(\\mathcal{l}(\\theta; \\text{x})\\) are the same as the values that maximise \\(\\mathcal{L}(\\theta; \\mathbf{x})\\) Mathematics \\(\\displaystyle \\mathcal{l}(\\theta; \\text{x})=\\log{\\mathcal{L}(\\theta; \\text{x})} = \\sum_{i = 1}^{n}{\\log f(x_i; \\theta)}\\) Notes \\(\\mathcal{l}(\\theta; \\text{x})\\) can be denoted as \\(\\mathcal{l}(\\theta)\\) Maximum Likelihood Estimation Maximum Likelihood Estimation Concept An estimation method in which the model parameters are estimated by finding the parameters of the statistical model that maximizes the likelihood of the data through setting the partial derivative of the likelihood function of the data to 0 Because the likelihood curve is concave, setting this to 0 will get the maxima of the curve; the point that represents the maximum likelihood of the data) Mathematics The value of \\(\\theta\\) with the maximum likelihood of the data \\(\\hat\\theta = Argmax\\mathcal{L}(\\theta; \\mathbf{x})\\) Where \\(Argmax\\mathcal{L}(\\theta; \\mathbf{x})\\) - This notation means that \\(\\mathcal{L}(\\theta; \\mathbf{x})\\) achieves its maximum value at \\(\\hat\\theta\\) - The value of \\(\\theta\\) with the maximum likelihood The value of \\(\\theta\\) with the maximum likelihood of the data is found through maximisation of the likelihood function by setting the partial derivative of the likelihood function to 0 \\(\\displaystyle \\frac{\\partial\\mathcal{L}(\\theta)}{\\partial \\theta} = 0\\) Notes The log likelihood function can be used but I just used the likelihood function here This is an estimating equation (EE), an equation for estimating the population parameters If there are multiple parameters (e.g. a normal distribution that has parameter \\(\\mu\\) and \\(\\sigma\\)), the parameters are solved through setting each of the partial derivative for each of the parameters to 0 Properties of the MLEs (THIS SECTION IS NOT DEVELOPED) Theorem 1 Concept If the parameter of interest $$ is the parameter \\(\\theta\\) in some sort of function \\(g(\\theta)\\), (\\(\\eta =g(\\theta)\\)), then the mle of \\(\\theta\\) in the same function is the mle of \\(eta\\) (\\(\\hat\\eta\\)) \\(g(\\hat\\theta)\\) is the MLE of \\(\\eta = g(\\theta)\\) \\(\\hat\\eta = g(\\hat\\theta)\\) “In some situations, besides the parameter \\(\\theta\\), we are also interested in the parameter \\(\\eta = g(\\theta)\\), it can be shown that the mle of \\(\\eta\\) is \\(\\hat\\eta = g(\\hat\\theta)\\) where \\(\\hat\\theta\\) is the mle of \\(\\theta\\)” Theorem 2 Concept The larger the sample size, the closer the mle is to the true parameter value ??? DON’T KNOW ABOUT THIS Hessian matrix Concept A square matrix in which each of the elements of the matrix is a second-order partial derivative of the function of interest with respect to the square of each of the variables A square matrix that describes the change in the rate of change at a particular point of a function (local curvature) of multiple variables The Hessian matrix is a general mathematical concept and it is applied in statistics Mathematics \\(\\displaystyle \\mathbf{H}_f = \\begin{bmatrix} \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{1}^{2}}} &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{1}} \\partial{x_{2}}} &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{1}} \\partial{x_{3}}} &amp; \\displaystyle \\cdots &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{1}} \\partial{x_{n}}} \\\\ \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{2}} \\partial{x_{1}}} &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{2}^2}} &amp;\\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{2}} \\partial{x_{3}}} &amp; \\displaystyle \\cdots &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{2}} \\partial{x_{n}}} \\\\ \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{3}} \\partial{x_{1}}} &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{3}}\\partial{x_{2}} } &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{3}^2}} &amp; \\displaystyle \\cdots &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{3}} \\partial{x_{n}}} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{n}} \\partial{x_{1}}} &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{n}}\\partial{x_{2}} } &amp; \\displaystyle \\frac{\\partial{n}^f}{\\partial{x_{n}} \\partial{x_{3}}} &amp; \\cdots &amp; \\displaystyle \\frac{\\partial{}^2f}{\\partial{x_{n}^2}} \\end{bmatrix}\\) Where \\(\\mathbf{H}_f\\) - The Hessian matrix of function \\(\\mathcal{f}\\) \\(\\frac{\\partial{}^2f}{\\partial{x_{i}} \\partial{x_{j}}}\\) - The second-order derivative of the function with respect to the cross-product of variable \\(x_i\\) and \\(x_j\\) The Score Concept The score is the slope of the log-likelihood function evaluated at a particular value of the parameter The Score at the true value of the parameter Concept The slope of the (log) likelihood function evaluated at the true value of the parameter (\\(\\theta\\)) The score is a variable because it is subject to sampling variation - That is, the score derived from a samples can be different from the scores derived from other samples In reality, we will never know this because the true value of the population parameter is unknown A single observation case The score Mathematics The likelihood function \\(\\begin{aligned}\\displaystyle \\mathcal{L}\\left( \\theta; x \\right) &amp;= \\prod_{i = 1}^{n = 1}{f(x_i; \\theta)} \\\\ &amp;= f(x; \\theta) \\end{aligned}\\) The log likelihood \\(\\begin{aligned} \\displaystyle \\mathcal{l}(\\theta; x) &amp;= \\log\\mathcal{L}\\left( \\theta; x \\right) \\\\ &amp;= \\log f(x; \\theta) \\end{aligned}\\) The score (first derivative of the log likelihood evaluated at the true value of the parameter) \\(\\begin{aligned}S_\\theta &amp;= \\displaystyle \\frac{\\partial \\mathcal{l}(\\theta; x)}{\\partial \\theta} \\\\ &amp;= \\frac{\\partial \\log f(x_i; \\theta)}{\\partial \\theta} \\end{aligned}\\) The expectation of the Score Mathematics \\(\\displaystyle E(S_\\theta) = E\\left( \\frac{\\partial \\log f(x_i; \\theta)}{\\partial \\theta}\\right)\\) The expectation of the score is 0 Concept The mean of the scores all with a single observation and evaluated at the true value of the parameter is 0 Mathematics \\(\\displaystyle E(S_\\theta) = E\\left( \\frac{\\partial \\log f(x_i; \\theta)}{\\partial \\theta}\\right) = 0\\) Proof Begin with the identity \\(\\displaystyle 1 = \\int_{-\\infty}^{\\infty}{\\mathcal{L}(\\theta; x)} ~ ~ dx = \\int_{-\\infty}^{\\infty}{\\mathcal{f}(x; \\theta)} ~ dx\\) The first derivative under the integral sign \\(\\displaystyle 0 = \\int_{-\\infty}^{\\infty}{\\frac{\\partial\\mathcal{f}(x; \\theta)}{\\partial \\theta}} ~ dx\\) Reversing the chain rule for log \\(\\begin{aligned}\\displaystyle 0 &amp;= \\int_{-\\infty}^{\\infty}{\\frac{\\partial\\mathcal{f}(x; \\theta)}{\\partial \\theta \\mathcal{f}(x; \\theta)}\\mathcal{f}(x; \\theta)} ~ dx \\\\ &amp;= \\int_{-\\infty}^{\\infty}{\\frac{\\partial \\log \\mathcal{f}(x; \\theta)}{\\partial \\theta}\\mathcal{f}(x; \\theta)} ~ dx \\end{aligned}\\) Reminder The chain rule for log \\(\\begin{aligned}y &amp;= \\log{(g(x))} \\\\ \\displaystyle \\frac{dy}{dx} &amp;= \\frac{1}{g(x)} \\times g&#39;(x) \\end{aligned}\\) Hence \\(\\begin{aligned} \\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{\\partial \\log{f(x ;\\theta)}}{\\partial \\theta} \\times f(x; \\theta) ~ dx} &amp;= \\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{1}{f(x; \\theta)} \\times f&#39;(x; \\theta) \\times f(x; \\theta) ~ dx} \\\\ &amp;= \\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{1}{f(x; \\theta)} \\times \\frac{\\partial f(x; \\theta)}{\\partial \\theta} \\times f(x; \\theta) ~ dx} \\\\ &amp;= \\displaystyle \\int_{-\\infty}^{\\infty}{\\frac{\\partial f(x; \\theta)}{\\partial \\theta f(x; \\theta)} \\times f(x; \\theta) ~ dx} \\end{aligned}\\) In the derivation above, it utilises this rule but in the reversed order Expressed as expectation \\(\\displaystyle 0 = E\\left[ \\frac{\\partial \\log \\mathcal{f}(x; \\theta)}{\\partial \\theta}\\right]\\) The variance of the score Concept The sampling variation of the scores given the true value of \\(\\theta\\) (alternatively speaking, it is the variation of the scores evaluated at the true value of \\(\\theta\\) due to sampling error) The imagination comes back again, similar as before, imagine you know the true value of the parameter, you then take many samples, each time you take a sample, you calculate the score with respect to the true value of the parameter, after doing this for each of the samples, you will have many scores, each coming from each sample, you then calculate the variance of the scores Variance defined as the second moment about the mean Concept The variance can be defined as the second moment about the mean Mathematics \\(\\displaystyle \\text{Var}(S) = \\text{E}\\left[ \\left(\\frac{\\partial \\log \\mathcal{f}(x; \\theta)}{\\partial \\theta}\\right)^2\\right]\\) Variance defined as the negative of the expectation of the second derivative of the likelihood function Concept Through derivations, it can be shown that the variance equals to the negative of the expectation of the second derivative of the likelihood function evaluated at the true value of the parameter. In other words, it is the average curvature of the likelihood functions at the true value of the parameter in the parameter space. Mathematics Begin with the identity (this is seen before in the expectation section) \\(\\begin{aligned}\\displaystyle 1 &amp;= \\int_{-\\infty}^{\\infty}{\\mathcal{L}(\\theta; x)} ~ ~ dx \\\\ &amp;= \\int_{-\\infty}^{\\infty}{\\mathcal{f}(x; \\theta)} ~ dx \\end{aligned}\\) The first derivative under the integral sign (this is seen before in the expectation section) \\(\\displaystyle 0 = \\int_{-\\infty}^{\\infty}{\\frac{\\partial\\mathcal{f}(x; \\theta)}{\\partial \\theta}} ~ dx\\) Which can be reexpressed as (this is seen before in the expectation section) \\(\\begin{aligned}\\displaystyle 0 &amp;= \\int_{-\\infty}^{\\infty}{\\frac{\\partial\\mathcal{f}(x; \\theta)}{\\partial \\theta \\mathcal{f}(x; \\theta)}\\mathcal{f}(x; \\theta)} ~ dx \\\\ &amp;= \\int_{-\\infty}^{\\infty}{\\frac{\\partial \\log \\mathcal{f}(x; \\theta)}{\\partial \\theta}\\mathcal{f}(x; \\theta)} ~ dx \\end{aligned}\\) Differentiate again (the second derivative) \\(\\begin{aligned}\\displaystyle 0 &amp;= \\int_{-\\infty}^{\\infty}{\\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2}} ~ f(x; \\theta)~dx + \\int_{-\\infty}^{\\infty}{\\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} ~ f(x; \\theta) ~ dx} \\\\ \\displaystyle 0 &amp;= \\int_{-\\infty}^{\\infty}{\\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2}} ~ f(x; \\theta)~dx + \\int_{-\\infty}^{\\infty}{\\left(\\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta}\\right)^2} ~ f(x; \\theta)~dx\\\\ \\displaystyle -\\int_{-\\infty}^{\\infty}{\\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2}} ~ f(x; \\theta)~dx &amp;= \\int_{-\\infty}^{\\infty}{\\left(\\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta}\\right)^2}~ f(x; \\theta)~dx \\end{aligned}\\) Expressed as expectations \\(\\displaystyle -\\int_{-\\infty}^{\\infty}{\\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2}} ~ f(x; \\theta)~dx = \\int_{-\\infty}^{\\infty}{\\left(\\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta}\\right)^2}~ f(x; \\theta)~dx \\\\ \\displaystyle -E\\left(\\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2}\\right) = E\\left[\\left(\\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta}\\right)^2\\right]\\) Hence, you can see that the second moment about the mean (\\(E\\left[\\left(\\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta}\\right)^2\\right]\\)) equals to the negative of the expectation of the curvature of the likelihood function evaluated at the true value of the parameter (\\(-E\\left(\\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2}\\right)\\)) Which expression is preferred? Usually the second definition of the variance is preferred as it is usually easier to compute than the first definition &amp;nbsp - Fisher Information - Concept (general) - A quantity that quantifies the amount of information that any datum/observation in a data/datum/sample carries about an unknown parameter of a statistical model of that random variable - The Fisher information is the variance of the score evaluated at 1 observation - Mathematics - \\(\\begin{aligned}\\displaystyle I_1(\\theta) &amp;= \\text{Var}(S) \\\\ \\displaystyle &amp;= \\text{Var}\\left[\\frac{\\partial \\mathcal{l}(\\theta; x)}{\\partial \\theta} \\right] \\\\ \\displaystyle &amp;= \\text{Var}\\left[\\frac{\\partial \\log f(x_i; \\theta)}{\\partial \\theta}\\right] \\end{aligned}\\) - Notes - \\(I_1(\\theta)\\) - Usually it is expressed as \\(I(\\theta)\\) but I expressed it as such to indicate that this is the Information from one observation - Information defined as the second moment of about the mean - Concept - As seen, the variance of the score can be defined in terms of the second moment of about the mean, since the Fisher Information is the variance of the score at the true value of the parameter, the Fisher Information can be interpreted the same way - Information defined as the curvature - Concept - As seen, the variance of the score can be defined in terms of the curvature of the likelihood function at the true value of the parameter, which means that Fisher information can be interpreted as the same way, which is pretty intuitive - Information can be expressed in terms of the negative of the expectation of the curvature of the likelihood function evaluated at the true value of the parameter - Hence, the greater the curvature of the likelihood function at the true value of the parameter, the more information the data/datum has about the unknown parameters of the model (in the extreme case when the likelihood function is at the highest at a single value of the parameter and 0 at all other values of the parameter, in other words, the likelihood is non-zero at a single point in the parameter space, the data/datum contains complete information about the parameter) - The smaller the curvature of the likelihood function at the true value of the parameter, the less information the data has about the unknown parameters of the model (in the extreme case when the likelihood function is flat, that is, when the likelihood is the same across values of the parameter, then the data/datum contains no information at all about the value of the parameter) - Mathematics - \\(\\begin{aligned}\\displaystyle I(\\theta) &amp;= \\text{Var}(S) \\\\ &amp;= -E\\left(\\frac{\\partial^2 \\log f(x; \\theta)}{\\partial \\theta^2}\\right)\\end{aligned}\\) - A multiple-observation case - Score - Mathematics - The likelihood function - \\(\\displaystyle \\mathcal{L}(\\theta; X) = \\prod_{i = 1}^{n}{f(x; \\theta)}\\) - The log likelihood function - \\(\\displaystyle \\mathcal{l}(\\theta; X) = \\log{\\mathcal{L}(\\theta; X)} = \\sum_{i = 1}^{n}{\\log{f(x; \\theta)}}\\) - Notes - The log of a product is the sum of the log of each factor in the product - The Score (the first derivative of the log likelihood function) (NOT SURE ABOUT THIS) - \\(\\begin{aligned} \\displaystyle S_n &amp;= \\frac{\\partial \\mathcal{l}(\\theta; X) }{\\partial \\theta} \\\\ \\displaystyle &amp;= \\frac{\\partial }{\\partial \\theta} \\mathcal{l}(\\theta; X) \\\\ \\displaystyle &amp;= \\frac{\\partial}{\\partial \\theta} \\sum_{i = 1}^{n}{\\log{f(x; \\theta)}} \\\\ \\displaystyle &amp;= \\sum_{i = 1}^{n}{\\frac{\\partial}{\\partial \\theta}\\log{f(x; \\theta)}} \\\\ \\displaystyle &amp;= \\sum_{i = 1}^{n}{\\frac{\\partial \\log{f(x; \\theta)}}{\\partial \\theta}} \\\\ \\displaystyle &amp;= \\sum_{i = 1}^{n}{S_1} \\\\ &amp;= nS_1 \\end{aligned}\\) - Note - The sum rule of differentiation - The derivative of a sum is the sum of the derivative of each term in the sum - Expectation of the Score - Mathematics - \\(\\begin{aligned}\\displaystyle \\text{E}(S_n) &amp;= \\text{E}\\left[\\frac{\\partial \\mathcal{l}(\\theta; \\textbf{x})}{\\partial \\theta}\\right] \\\\ &amp;= \\text{E}\\left[ \\sum_{i = 1}^{n}{\\frac{\\partial \\log{f(x_i; \\theta)}}{\\partial \\theta}}\\right]\\\\ &amp;= \\sum_{i = 1}^{n}{\\text{E}\\left[ \\frac{\\partial \\log{f(x_1; \\theta)}}{\\partial \\theta}\\right]} \\end{aligned}\\) - The expectation of each score is 0, hence: - \\(\\begin{aligned} \\text{E}(S_n) &amp;= \\sum_{i = 1}^{n}{0} \\\\ &amp;= 0\\end{aligned}\\) - Variance of the score - Variance defined as the second moment about the mean - Concept - The variance of the score in a sample with n observations is n times the information of any one observation in the sample - Mathematics - \\(\\begin{aligned} \\displaystyle \\text{Var}(S) &amp;= \\text{Var}\\left[ \\frac{\\partial \\mathcal{l}(\\theta; \\textbf{x})}{\\partial \\theta}\\right] \\\\ &amp;= \\text{Var}\\left[\\sum_{i = 1}^{n}{\\frac{\\partial \\log{f(x_i; \\theta)}}{\\partial \\theta}}\\right] \\\\ &amp;= \\sum_{i = 1}^{n}{\\text{Var}\\left[ \\frac{\\partial \\log{f(x_i; \\theta)}}{\\partial \\theta}\\right]} + \\sum_{j = 1}^{J}{\\sum_{i = 1}^{I}{\\text{Cov}\\left[ \\frac{\\partial \\log{f(x_i; \\theta)}}{\\partial \\theta} \\frac{\\partial \\log{f(x_j; \\theta)}}{\\partial \\theta}\\right]}} \\end{aligned}\\) - Notes - Bienaymé’s identity - Concept - The variance of the sum is the sum of the variance of each of the elements plus the sum of the covariance of each pair of the elements - If the random variables are uncorrelated, that is, they have a covariance of 0, then it simplies that the variance of the sum is the sum of the variance of the elements - Mathematics - $ ({i = 1}^{n}{x_i}) = {i = 1}^{n}{(x_i)} + {j = 1}^{J}{{i = 1}^{I}{( x_i, x_j)}} $ - Assuming that the scores are uncorrelated (having a correlation of 0) - \\(\\begin{aligned}\\displaystyle \\text{Var}\\left(S_n\\right) &amp;= \\sum_{i = 1}^{n}{\\text{Var}\\left[ \\frac{\\partial \\log{f(x_i; \\theta)}}{\\partial \\theta}\\right]} \\\\ &amp;= \\sum_{i = 1}^{n}{\\text{Var}\\left[ \\frac{\\partial \\log{f(x; \\theta)}}{\\partial \\theta}\\right]} \\\\ &amp;= n\\times\\text{Var}\\left[\\frac{\\partial \\log{f(x; \\theta)}}{\\partial \\theta}\\right] \\\\ &amp;= n\\text{Var}(S_1)\\end{aligned}\\) - Fisher Information - Concept - The Information in a sample with n observations is n times the Information from any one observation in the sample - Mathematics - \\(\\begin{aligned}\\displaystyle \\text{Var}\\left(S_n\\right) &amp;= n\\text{Var}(S_1) \\\\ I_n(\\theta) &amp;= nI_1(\\theta)\\end{aligned}\\) Variance of model parameters Concept The variance of a model parameter or the variance-covariance matrix of a vector of model parameters Under regularity conditions, the variance-covariance of model parameters is the inverse of the information weighted by n (sample size) (Rao, 1973, p.364; Rao Cramer lower bound) Mathematics For one parameter \\(\\begin{aligned} \\displaystyle \\text{Var}(\\hat\\theta) &amp;= \\sigma_{\\hat\\theta}^{2}= \\frac{1}{n}I^{-1}(\\hat\\theta) = \\frac{1}{nI(\\hat\\theta)} \\\\ \\displaystyle \\text{SE}(\\hat\\theta) &amp;= \\sqrt{\\sigma_{\\hat\\theta}^{2}} = \\sqrt{\\frac{1}{n}I^{-1}(\\hat\\theta)} = \\sqrt{\\frac{1}{nI(\\hat\\theta)}} \\end{aligned}\\) For multiple parameters \\(\\displaystyle \\textbf{Cov}(\\hat\\theta) = \\frac{1}{n}\\textbf{I}^{-1}(\\theta)\\) Notes This is a variance-covariance matrix, the variance for each of the model parameters are in the diagonal. To get the Standard Error of each of the model parameters, square the variance of the model parameters. NHST for MLE Wald test Likelihood ratio Score statistic Wald test (Wald, 1943) Concept Inferential Test statistic based on the (log) likelihood function The Wald statistic Mathematics \\(\\displaystyle W = \\frac{(\\hat\\theta - \\theta_0)^2}{\\sigma^2_{\\theta}}\\) Where $$ - The MLE \\(\\theta_0\\) - The value of the parameter under the null hypothesis \\(\\sigma^2_{\\theta}\\) The sampling variance of the population parameter under the alternative hypothesis Distribution of the Wald statistic Concept The Wald statistic has an approximate central Chi-squared distribution with df of 1 Mathematics \\(W \\sim \\chi^2(1)\\) z-transformed Wald statistic Concept The Wald statistic transformed into a z normal statistic Mathematics \\(z_W = \\sqrt{W}\\) Distribution of the z Wald statistic Concept The z Wald statistic has an approximate z normal distribution Mathematics \\(z_W \\sim \\mathcal{N}(df)\\) Likelihood ratio test (Wilks test) The likelihood ratio test statistic Concept The ratio of the likelihood under the alternative hypothesis and the likelihood under the null hypothesis The squared vertical deviation between the maximised likelihood under the alternative hypothesis and the likelihood of the null hypothesized value of the parameter under the alternative hypothesis Mathematics \\(\\displaystyle -2\\log \\Lambda = -2 \\log\\left[ \\frac{\\mathcal{L}{(\\theta_0)}}{\\mathcal{L}({\\theta_1})}\\right] = -2\\left[\\mathcal{L}{(\\theta_0)} - \\mathcal{L}{(\\theta_1)}\\right]\\) Where \\(L\\) - Maximised log-likelihood function \\(\\mathcal{L}{(\\theta_1)}\\) - The maximised likelihood of the value of the parameter given the data (under the alternative hypothesis) \\(\\mathcal{L}{(\\theta_0)}\\) - The likelihood of the null hypothesised value of the parameter in the likelihood function given the data (under the alternative hypothesis) Degrees of freedom of the likelihood ratio test statistic Concept The df is the difference in the dimensions of the parameter spaces under \\(H_0 \\cup H_1\\) and \\(h_0\\) The distribution of the likelihood ratio test statistic Concept Under certain regularity conditions, the null distribution of the likelihood ratio statistic tends towards a chi-squared distribution with 1 degrees of freedom as sample size increases Mathematics $^2(1) \\ or \\ -2 ^2(1) $ Score statistic (Lagrange multiplier test) Concept Based on the slope and expected curvature at the null hypothesised value of the parameter in the log-likelihood function The score statistic The z score statistic Concept The normal z version of the score statistic It is the ratio of the slope of the likelihood function of the parameter at the null hypothesised value of the parameter to the standard error of the slope at the null hypothesised value of the parameter Mathematics \\(\\displaystyle \\frac{\\mathcal{l&#39;}(\\theta_0)}{\\sqrt{I(\\theta_0)}}\\) Where \\(\\mathcal{l&#39;}(\\theta_0)\\) - The slope of the likelihood function of the parameter at the null hypothesised value of the parameter \\(\\sqrt{I(\\theta_0)}\\) - The standard error of the slope of the likelihood function of the parameter at the null hypothesised value of the parameter - The standard error depends on the curvature of the likelihood function of the parameter at the null hypothesised value of the parameter - The information evaluated at the null hypothesised value of the parameter Mathematics \\(\\displaystyle \\frac{\\mathcal{l&#39;}(\\theta_0)^2}{I(\\theta_0)}\\) Where \\(\\mathcal{l&#39;}\\) - The first derivative of the log likelihood function \\(\\mathcal{l&#39;}(\\theta_0)^2\\) - The slope at the null hypothesised value of the parameter in the log likelihood function (sub in the null hypothesised value of the parameter into the first derivative of the log likelihood function and you will get this) \\(I\\) - Information \\(I(\\theta_0)\\) - - The information at the null hypothesised value in the likelihood function - The variance of the slope of the likelihood function under the null hypothesis (those that are close to and around the maximum (slope of 0)) Notes Some textbooks (e.g. Hogg and McKean) say it’s like this \\(\\displaystyle \\frac{\\mathcal{L&#39;}(\\theta_0)^2}{nI(\\theta_0)}\\) Central Distribution of the score statistic Concept The score statistic under the null hypothesis has a central Chi-square distribution z transformed score statistic Concept The score statistic can be transformed into a z normal statistic under the null hypothesis that has an approximate central normal distribution Mathematics \\(\\displaystyle \\sqrt{\\frac{S(\\theta_0)^2}{I(\\theta_0)}}\\) Evaluating the three tests As \\(n\\) increases towards infinity, the three tests have asymptotic equivalences (Cox and Hinkley, 1974, Sec. 9.3). For small to moderate sample sizes, the likelihood-ratio and score tests tend to be more reliable than the Wald test, that is that they vaeh an actual error rate closer to the nominal level Maximum Likelihood Estimation for binomial Parameter MLE for the population parameter Concept The log likelihood of the kernal Mathematics The PMF of the binomial distribution \\(\\displaystyle P(y) = {n\\choose y} \\pi^y (1 - \\pi)^{n - y} ~~~, y = 0:n\\) The log likelihood function: \\(\\begin{aligned} \\displaystyle \\mathcal{l}(\\pi)&amp;=\\log{\\left[\\pi^{y}(1-\\pi)^{n-y}\\right]} \\\\ \\displaystyle &amp;= y\\log(\\pi)+(n-y)\\log(1-\\pi) \\end{aligned}\\) Notes Only the kernal of the PMF of the binomial distribution contains the parameters and relevant, therefore, only the kernal is used The first derivative of the likelihood function: \\(\\begin{aligned} \\displaystyle \\frac{\\partial{\\mathcal{l}(\\pi)}}{\\partial{\\pi}} \\displaystyle &amp;= \\frac{y}{\\pi} - \\frac{n - y}{1- \\pi} \\\\ \\displaystyle &amp;= \\frac{y - n\\pi}{\\pi(1-\\pi)} \\end{aligned}\\) Maximisation problem: set the derivative of the log likelihood function to 0 and find \\(\\pi\\) (the estimating equation) \\(\\begin{aligned} \\displaystyle \\frac{y - n\\pi}{\\pi(1-\\pi)} &amp;= 0 \\\\ \\displaystyle y - n\\pi &amp;= 0 \\\\ \\displaystyle \\hat\\pi &amp;= \\frac{y}{n}\\end{aligned}\\) Notes As you can see, this is the sample proportion of success for the \\(n\\) trials - This is the MLE of the population probability Information Mathematics \\(\\displaystyle I_{\\pi} =-E\\left(\\frac{\\partial^2 f}{\\partial \\pi^2}\\right) = E\\left[ \\frac{y}{\\pi^2} - \\frac{n-y}{(1-\\pi)^2}\\right] = \\frac{n}{\\pi(1-\\pi)}\\) Variance of \\(\\hat\\pi\\) Concept The variance of the parameter estimate in the distribution under the null hypothesis based on the ML method It is estimated from the variance of the sample Mathematics \\(\\begin{aligned} \\displaystyle \\sigma_{\\hat\\pi}^{2} &amp;= I^{-1} \\\\ \\displaystyle \\sigma_{\\hat\\pi}^{2} &amp;= -E\\left(\\frac{\\partial^2 f}{\\partial \\pi^2}\\right)^{-1} \\\\ \\displaystyle &amp;= E \\left[ \\frac{y}{\\pi^2} + \\frac{n-y}{(1-\\pi)^2} \\right]^{-1} \\\\ \\displaystyle &amp;= \\left[\\frac{n}{\\pi(1 - \\pi)}\\right]^{-1} \\\\ \\displaystyle &amp;= \\frac{\\pi(1-\\pi)}{n} \\end{aligned}\\) Standard error Mathematics \\(\\begin{aligned} \\displaystyle \\sigma_{\\hat\\pi} &amp;= \\sqrt{\\sigma_{\\hat\\pi}^{2}} \\\\ &amp;= \\sqrt{\\frac{\\pi(1-\\pi)}{n}} \\end{aligned}\\) Confidence Interval (Not yet developed) Wald confidence interval Concept The Wald confidence interval uses the normal approximation Mathematics \\(CI = \\hat\\pi ± z_{1-\\frac{\\alpha}{2}} \\times \\sigma_{\\hat\\pi}\\) Others A marked divergence in results of Wald and Likelihood ratio inference indicates that the distribution of the parameter may not be close to normality NHST of the binomial parameter \\(\\hat\\pi\\) Wald test The z Wald statistic Mathematics \\(\\displaystyle z_W = \\frac{\\hat\\pi - \\pi_0}{\\sigma_{\\hat\\pi}} = \\frac{\\hat\\pi - \\pi_0}{\\sqrt{\\frac{\\hat\\pi(1-\\hat\\pi)}{n}}}\\) Score statistic test Concept Since the Wald statistic uses the standard error evaluated at \\(\\hat\\pi\\) and the score statistic uses the standard error evaluated at \\(\\pi_0\\), the score statistic is preferred because it uses the actual null SE rather than an estimate of the null SE The score statistic Mathematics $ = $ Finding the slope of the likelihood function at the null hypothesised value of the parameter (\\(S(\\pi_0)^2\\)) The first derivative of the likelihood function for \\(\\pi\\) \\(\\displaystyle \\mathcal{l}(\\pi) = \\frac{y - n\\pi}{\\pi(1-\\pi)}\\) Sub in \\(\\pi_0\\) \\(\\displaystyle S(\\pi_0) = \\frac{y - n\\pi_0}{\\pi_0(1-\\pi_0)}\\) Square it $^2 $ Finding the SE of the slope under the null hypothesis (\\(I(\\pi_0)\\)) \\(\\begin{aligned}\\displaystyle I(\\pi_0) &amp;= -E\\left(\\frac{\\partial{}^2f}{\\partial{x_{1}^{2}}}\\right) \\\\ \\displaystyle &amp;= -E\\left(\\frac{\\partial{}^2 \\mathcal{L}(\\pi)}{\\partial{\\pi_0^{2}}}\\right) \\\\ \\displaystyle &amp;= \\frac{n}{\\pi_0 (1 - \\pi_0)} \\end{aligned}\\) Hence \\(\\displaystyle \\frac{S(\\pi_0)^2}{I(\\pi_0)} = \\frac{\\left[\\frac{y - n\\pi_0}{\\pi_0(1-\\pi_0)}\\right]^2}{\\frac{n}{\\pi_0 (1 - \\pi_0)}} = \\frac{(y - n\\pi_0)^2}{\\frac{\\pi_0(1-\\pi_0)}{n}} = \\frac{(\\hat\\pi - \\pi_0)^2}{\\frac{\\pi_0(1-\\pi_0)}{n}}\\) Continuity correction Concept A correction made when a discrete distribution is approximated by a continuous distribution The Generalised Linear Model Concept An extension of ordinary regression models to encompass non-normal distributions of the outcome variable/residuals There three components of GLM Random component Systematic component Link function Random component Identifies the outcome variable and its probability distribution Systematic component Specifies explanatory variables used in a linear combination Link function A function in Generalised Linear Models that maps the random with the systematic components Types of link function Identity link function Logit link function Other stuff Unbiasedness of an estimator Definition Let \\(x_1, x_2, x_3, \\cdots , x_n\\) be a sample on a random variable \\(X\\) with pdf \\(f(x; \\theta)\\). Let \\(S = S(x_1, x_2, x_3, \\cdots , x_n)\\) be a statistic. \\(S\\) is an unbiased estimator of the population parameter \\(\\theta\\) if \\(E(S) = \\theta\\) Cramér-Rao Lower Bound Concept The CRLB states that the variance of an estimator cannot be lower than the CRLB That is, the CRLB is the smallest possible variance of an estimator For unbiased or biased estimators Mathematics This is for the case when \\(\\hat\\theta\\) can be an unbiased or biased estimator of \\(\\theta\\), that is when the expectation of \\(\\hat\\theta\\) is not \\(\\theta\\) but some kind of function of this (e.g. \\(k(\\theta)\\)) Let \\(x_1, x_2, x_3, \\cdots, x_n\\) be idd with a pdf \\(f(x; \\theta)\\). Let \\(S = u(x_1, x_2, x_3, \\cdots, x_n)\\) be a sample statistic with a mean \\(E(S) = E[u(x_1, x_2, x_3, \\cdots, x_n)] = k(\\theta)\\). Under the first 4 regularity conditions: \\(\\displaystyle \\text{Var}(\\theta) ≥ \\frac{\\left[k&#39;(\\theta)\\right]^2}{n I(\\theta)}\\) For unbiased estimators Mathematics If \\(S = u(x_1, x_2, x_3, \\cdots, x_n)\\) is an unbiased estimator of \\(\\theta\\), so that \\(k(\\theta) = \\theta\\), then the Cramér-Rao inequality becomes: \\(\\displaystyle \\text{Var}(\\hat\\theta) ≥ \\frac{1}{I(\\theta)}\\) Proof Scenario Let \\(\\hat\\theta = u(X) = u(x_1, x_2, \\cdots , x_n)\\) be a sample statistic and an estimator of the true value of the parameter \\(\\theta\\). To generalise the proof to accommodate both unbiased and biased estimator, let’s assume that \\(\\hat\\theta\\) can be a biased or unbiased estimator, hence, let’s express the expectation of \\(\\hat\\theta\\) as \\(\\text{E}(\\hat\\theta) = g(\\theta)\\) (So that if \\(\\hat\\theta\\) is an unbiased estimator, \\(g()\\) would be an identity function). Let \\(f(x; \\theta)\\) be the pdf of the random variable \\(X\\) The log-likelihood function \\(\\mathcal{l}(\\theta; X)\\) The score \\(\\begin{aligned}\\displaystyle S &amp;= \\frac{\\partial \\log \\mathcal{f}(x; \\theta) }{\\partial \\theta} \\\\ &amp;= \\frac{1}{f(x; \\theta)} \\times \\frac{\\partial f(x; \\theta)}{\\partial \\theta} \\\\ &amp;= \\frac{\\partial f(x; \\theta)}{\\partial \\theta f(x; \\theta)}\\end{aligned}\\) Covariance of \\(S\\) and \\(\\hat\\theta\\) \\(\\displaystyle \\text{Cov}(S, \\hat\\theta) = \\text{E}\\left[ (S - \\text{E}(S)) (\\hat\\theta - \\text{E}(\\hat\\theta))\\right]=\\text{E}(S\\hat\\theta) - \\text{E}(S)\\text{E}(\\hat\\theta)\\) Notes The definition of the covariance of 2 random variables is: \\(\\displaystyle \\text{Cov}(X, Y) = \\text{E}\\left[ (X - \\text{E}(X)) (Y - \\text{E}(Y))\\right]=\\text{E}(XY) - \\text{E}(X)\\text{E}(Y)\\) Since \\(\\text{E}(S) = 0\\) (proof is in the Score section), the covariance simplifies to: \\(\\displaystyle \\text{Cov}(S, \\hat\\theta) =\\text{E}(S\\hat\\theta)\\) Expanding the expression \\(\\begin{aligned}\\displaystyle \\text{Cov}(S, \\hat\\theta) &amp;= \\text{E}(S\\hat\\theta) \\\\ &amp;= \\text{E}\\left[ \\left( \\frac{\\partial f(X; \\theta)}{\\partial \\theta f(X; \\theta)}\\right) \\hat\\theta\\right]\\end{aligned}\\) Expressing it as an integral \\(\\begin{aligned} \\displaystyle \\text{Cov}\\left( S, T\\right) &amp;= \\text{E}\\left[ \\left( \\frac{\\partial f(x; \\theta)}{\\partial \\theta f(X; \\theta)}\\right) \\hat\\theta\\right] \\\\ &amp;= \\int_{}^{}{\\frac{\\partial f(x; \\theta)}{\\partial \\theta f(X; \\theta)} u(x) f(x; \\theta) ~ dx} \\\\ &amp;= \\frac{\\partial}{\\partial \\theta}\\int_{}^{}{\\frac{f(x; \\theta)}{f(x; \\theta)}u(x) f(x; \\theta) ~ dx} \\\\ &amp;= \\frac{\\partial}{\\partial \\theta}\\int_{}^{}{u(x) f(x; \\theta) ~ dx} \\\\ &amp;= \\frac{\\partial}{\\partial \\theta}\\text{E}(\\hat\\theta) \\\\ &amp;= g&#39;(\\theta) \\end{aligned}\\) Since The Cauchy-Schwarz inequality proofed that \\(\\sqrt{\\text{Var}(X) \\text{Var}(Y)} ≥ |\\text{Cov(X, Y)}|\\) And as proofed, \\(\\text{Cov(X, Y)} = g&#39;(\\theta)\\) Therefore \\(\\begin{aligned} \\displaystyle \\sqrt{\\text{Var}(S) \\text{Var}(\\hat\\theta)} &amp;≥ |g&#39;(\\theta)| \\\\ \\text{Var}(S)\\text{Var}(\\hat\\theta) &amp;≥ \\left[g&#39;(\\theta)\\right]^2 \\\\ \\text{Var}(\\hat\\theta) &amp;≥ \\frac{\\left[g&#39;(\\theta)\\right]^2 }{\\text{Var}(S)} \\\\ \\text{Var}(\\hat\\theta) &amp;≥ \\frac{\\left[g&#39;(\\theta)\\right]^2 }{nI(\\theta)}\\end{aligned}\\) Efficient Estimator Concept A sample statistic is an efficient estimator of the population parameter \\(\\theta\\) only if the variance of the sample statistic is equal to the Cramér-Rao lower bound Efficiency Concept The efficiency of an estimator is quantified as the ratio of the Cramér-Rao lower bound to the actual variance of an unbiased estimator Mathematics \\(\\displaystyle \\text{Efficiency} = \\frac{\\textit{CRLB}}{\\text{Var}(\\theta)} \\\\ \\displaystyle e(\\hat\\theta_{1n}) = \\frac{\\frac{1}{I(\\theta_0)}}{\\sigma^2_{\\hat\\theta_{1n}}}\\) Interpretation According to the CRLB, the variance cannot be lower than the CRLB Hence, efficiency ranges from asymptotic 0 to 1 An efficiency of 1 means that the estimator is asymptotically efficient An efficiency smaller than 1 means that the estimator is inefficient The smaller the efficiency below 1 and towards 0, the less efficient is the estimator MLE and efficiency It can be shown that under regularity conditions, mles achieve an efficiency of 1 asymptotically. In other words, the variances of mles achieve their Cramér-Rao lower bounds asymptotically Regularity conditions of MLEs (DON’T FUCKING KNOW! Not developed) The variables are independent and identically distributed with density \\(f(y; \\theta)\\) The parameter space \\(\\Theta\\) is compact The value of the parameter is identified The likelihood function is continuous in \\(\\theta\\) \\(E_{\\theta_0} \\log f(Y; \\theta)\\) exists The log-likelihood function is such that \\(\\frac{1}{n}\\mathcal{L}(y; \\theta)\\) converges in probability to \\(E_{\\theta_0} \\log f(Y; \\theta)\\) uniformly in \\(\\theta \\in \\Theta\\) The pdf is at least twice differentiable as a function of \\(\\theta\\) Integration and differential operators are interchangeable - The order of integration and differentiation can be interchanged The information matrix exists and is non-singular The integral \\(\\int f(x; \\theta) dx\\) can be differentiated twice under the integral sign as a function of \\(\\theta\\) Additional regularity conditions The pdf is at least three times differentiable as a function of \\(\\theta\\). For all \\(\\theta \\in \\Omega\\), there exist a constant c and a function \\(M(x)\\) such that Properties of MLEs MLEs are asymptotically efficient MLEs are asymptotically normal MLEs are asymptotically consistent MLEs are asymptotically efficient Concept Under regularity conditions, MLEs are asymptotically efficient This means that the variance of an mle approaches the minimum variance (i.e. CRLB) as sample size increases towards infinity The efficiency of mles converges to 1 as \\(n \\rightarrow \\infty\\) MLEs are asymptotically normal Concept Under certain regularity conditions and correct model specification, MLEs are asymptotically normal The distribution of \\(\\hat\\theta_{mle}\\) approaches normal as sample size increases towards infinity MLEs are asymptotically consistent Concept Under regularity conditions 1-6, MLEs are asymptotically consistent This means that MLEs in the sampling distribution converge in probability to the true value of the parameter as sample size increases towards infinity $ _0 $ Variance of an MLE Concept The asymptotically consistent estimate of the variance Mathematics \\(\\displaystyle \\text{Var}(\\hat\\theta) = \\frac{1}{nI(\\theta_0)}\\) Estimation Concept As seen, the variance of an MLE is a function of the true population parameter value. However, this is often unknown, the estimate of this value is used instead. Mathematics \\(\\displaystyle \\text{Var}(\\hat\\theta) = \\frac{1}{nI(\\hat\\theta)}\\) Asymptotically consistent This estimate of the variance is asymptotically consistent It converges in probability to the true variance $I() I(_0) $ SE of an MLE Mathematics \\(\\displaystyle \\sqrt{\\frac{1}{nI(\\theta_0)}}\\) Estimation Mathematics \\(\\displaystyle \\sqrt{\\frac{1}{nI(\\hat\\theta)}}\\) 7.0.0.0.0.0.0.0.0.1 OTHERS Negative Binomial Distribution Concept Terms Complement of an event - The complement of an event is the probability of that event not occurring Others Score Mathematics The likelihood function \\(\\displaystyle \\mathcal{L}\\left( \\theta; x \\right) = \\prod_{i = 1}^{n = 3}{f(x_i; \\theta)} = f(x_1; \\theta) \\times f(x_2; \\theta) \\times f(x_3; \\theta)\\) The log likelihood \\(\\begin{aligned} \\displaystyle \\mathcal{l}(\\theta; \\textbf{x}) &amp;= \\log \\mathcal{L}\\left( \\theta; \\textbf{x} \\right) \\\\ &amp;= \\log \\prod_{i = 1}^{n = 3}{f(x_i; \\theta)} \\\\ &amp;= \\log{\\left[f(x_1; \\theta) \\times f(x_2; \\theta) \\times f(x_3; \\theta)\\right]} \\\\ &amp;= \\log{f(x_1; \\theta)} + \\log{f(x_2; \\theta)} + \\log{f(x_3; \\theta)} \\end{aligned}\\) Notes The log of the product is sum of the log of each factor in the product The score (the first derivative of the log likelihood) \\(\\begin{aligned} \\displaystyle S &amp;= \\frac{\\partial \\mathcal{l}(\\theta; \\textbf{x})}{\\partial \\theta} \\\\ &amp;= \\frac{\\partial}{\\partial\\theta}\\left[ \\log{f(x_1; \\theta)} + \\log{f(x_2; \\theta)} + \\log{f(x_3; \\theta)} \\right] \\\\ &amp;= \\frac{\\partial \\log{f(x_1; \\theta)}}{\\partial \\theta} + \\frac{\\partial \\log{f(x_2; \\theta)}}{\\partial \\theta} + \\frac{\\partial \\log{f(x_3; \\theta)}}{\\partial \\theta} \\end{aligned}\\) Notes The derivative of a sum is the sum of the derivative of each of the elements in the sum The expectation of the score Mathematics \\(\\begin{aligned} \\displaystyle E\\left(S \\right) &amp;= E\\left[\\frac{\\partial \\mathcal{l}(\\theta; \\textbf{x})}{\\partial \\theta}\\right] \\\\ &amp;= E\\left[\\frac{\\partial \\log{f(x_1; \\theta)}}{\\partial \\theta} + \\frac{\\partial \\log{f(x_2; \\theta)}}{\\partial \\theta} + \\frac{\\partial \\log{f(x_3; \\theta)}}{\\partial \\theta}\\right] \\\\ &amp;= E\\left[\\frac{\\partial \\log{f(x_1; \\theta)}}{\\partial \\theta}\\right] + E\\left[\\frac{\\partial \\log{f(x_2; \\theta)}}{\\partial \\theta}\\right]+ E\\left[\\frac{\\partial \\log{f(x_3; \\theta)}}{\\partial \\theta}\\right] \\end{aligned}\\) Notes The expectation of a sum is the sum of the expectation of each of the elements in the sum The expectation of the score is 0 Since any one of the expectations is 0 (as shown in the one-observation case), the sum of all the expectations is also 0 The variance of the score Mathematics Variance defined as the second moment about the mean \\(\\begin{aligned} \\displaystyle \\text{Var}(S) &amp;= \\text{Var}\\left[ \\frac{\\partial \\mathcal{l}(\\theta; \\textbf{x})}{\\partial \\theta}\\right] \\\\ \\displaystyle &amp;= \\text{Var}\\left[\\frac{\\partial \\log{f(x_1; \\theta)}}{\\partial \\theta} + \\frac{\\partial \\log{f(x_2; \\theta)}}{\\partial \\theta} + \\frac{\\partial \\log{f(x_3; \\theta)}}{\\partial \\theta}\\right] \\\\ &amp;= \\text{Var}\\left[\\frac{\\partial \\log{f(x_1; \\theta)}}{\\partial \\theta}\\right] + \\text{Var}\\left[\\frac{\\partial \\log{f(x_2; \\theta)}}{\\partial \\theta}\\right]+ \\text{Var}\\left[\\frac{\\partial \\log{f(x_3; \\theta)}}{\\partial \\theta}\\right] + 2\\sum_{j = 1}^{n = 3}{\\sum_{i = 1}^{n = 3}{\\text{Cov}\\left(\\frac{\\partial \\log{f(x_i; \\theta)}}{\\partial \\theta}, \\frac{\\partial \\log{f(x_j; \\theta)}}{\\partial \\theta}\\right)}} \\end{aligned}\\) Assuming that observations are uncorrelated: \\(\\begin{aligned} \\displaystyle \\text{Var}(S) &amp;= \\text{Var}\\left[\\frac{\\partial \\log{f(x_1; \\theta)}}{\\partial \\theta}\\right] + \\text{Var}\\left[\\frac{\\partial \\log{f(x_2; \\theta)}}{\\partial \\theta}\\right]+ \\text{Var}\\left[\\frac{\\partial \\log{f(x_3; \\theta)}}{\\partial \\theta}\\right] \\\\ &amp;= 3\\times \\text{Var}\\left[\\frac{\\partial \\log{f(x; \\theta)}}{\\partial \\theta}\\right] \\\\ &amp;= n\\times\\text{Var}\\left[\\frac{\\partial \\log{f(x; \\theta)}}{\\partial \\theta}\\right]\\end{aligned}\\) Fisher Information Concept The Information from a sample with 3 observations is 3 times the Information from any single observation Mathematics \\(\\displaystyle \\text{Var}(S) = \\text{Var}\\left[ \\frac{\\partial \\mathcal{l}(\\theta; \\textbf{x})}{\\partial \\theta}\\right]\\) \\(\\begin{aligned}I_3(\\theta) &amp;= 3 \\times \\text{Var}\\left[\\frac{\\partial \\log{f(x; \\theta)}}{\\partial \\theta}\\right] \\\\ I_3(\\theta) &amp;= 3 I_1(\\theta)\\end{aligned}\\) The expectation of the score Concept Under certain regularity conditions, the expectation of the score evaluated at the true value of \\(\\theta\\) is 0 Imagine you know the true value of the parameter, you then take many samples, each time you take a sample, you calculate the score with respect to the true value of the parameter, after doing this for each of the samples, you will have many scores, each coming from each sample, the mean of these scores will be 0, that is, they are clustered around 0 Mathematics \\(\\displaystyle E(S) = E\\left( \\frac{\\partial f(x_i; \\theta)}{\\partial \\theta}\\right) = 0\\) Proof that the expectation of the score is 0 Begin with the identity \\(\\displaystyle 1 = \\int_{-\\infty}^{\\infty}{\\mathcal{L}(\\theta; x)} ~ ~ dx = \\int_{-\\infty}^{\\infty}{\\mathcal{f}(x; \\theta)} ~ dx\\) The first derivative under the integral sign \\(\\displaystyle 0 = \\int_{-\\infty}^{\\infty}{\\frac{\\partial\\mathcal{f}(x; \\theta)}{\\partial \\theta}} ~ dx\\) Which can be reexpressed as \\(\\displaystyle 0 = \\int_{-\\infty}^{\\infty}{\\frac{\\partial\\mathcal{f}(x; \\theta)}{\\partial \\theta \\mathcal{f}(x; \\theta)}\\mathcal{f}(x; \\theta)} ~ dx = \\int_{-\\infty}^{\\infty}{\\frac{\\partial \\log \\mathcal{f}(x; \\theta)}{\\partial \\theta}\\mathcal{f}(x; \\theta)} ~ dx = E\\left[ \\frac{\\partial \\log \\mathcal{f}(x; \\theta)}{\\partial \\theta}\\right]\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
