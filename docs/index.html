<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Statistics</title>
  <meta name="description" content="Statistics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistics" />
  
  
  

<meta name="author" content="Samuel Mak" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="generalised-least-squares.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a >Section</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="2" data-path="generalised-least-squares.html"><a href="generalised-least-squares.html"><i class="fa fa-check"></i><b>2</b> Generalised Least Squares</a></li>
<li class="chapter" data-level="3" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>3</b> Mathematics</a></li>
<li class="chapter" data-level="4" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>4</b> ANOVA</a></li>
<li class="chapter" data-level="5" data-path="derivation.html"><a href="derivation.html"><i class="fa fa-check"></i><b>5</b> Derivation</a></li>
<li class="chapter" data-level="6" data-path="multivariate-analysis.html"><a href="multivariate-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Analysis</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Statistics</h1>
<p class="author"><em>Samuel Mak</em></p>
<p class="date"><em>Last Updated: 10-Oct-2022 | 17:51:57</em></p>
</div>
<div id="the-linear-regression-model" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> The Linear Regression Model<a href="index.html#the-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Mathematical model</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>A model that describes the relationship between variables with a mathematical function/expression</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Linear Model</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>A model that describes the relationship between variables with the linear function</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y = mx + c\)</span></li>
</ul></li>
<li><strong>Assumption of the linear model</strong>
<ul>
<li><strong>Linearity</strong> - The relationship between the variables of interest is linear or best described by a linear function</li>
<li><strong>Additivity</strong> - The relationship between the variables of interest is best described as a linear combination</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of estimating the relationship between variables of interest</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Linear Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of estimating the relationship between variables of interest with a linear model (fitting a straight line to the data)</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Curvilinear Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of estimating the relationship between variables of interest with a curvilinear model (fitting a curve to the data)</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Simple Linear Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Linear regression with one predictor</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><strong>Population Model</strong>
<ul>
<li><span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}X_{i,1} + \epsilon_{i}\)</span>
<ul>
<li><strong>Where</strong>
<ul>
<li><span class="math inline">\(y_{i}\)</span>
<ul>
<li>The value of observation <em>i</em></li>
</ul></li>
<li><span class="math inline">\(\beta_{p}\)</span>
<ul>
<li>Regression coefficients/parameters</li>
</ul></li>
<li><span class="math inline">\(\beta_{0}\)</span>
<ul>
<li>The intercept</li>
<li>The predicted value of the outcome when the value of predictor 1 (<span class="math inline">\(X_{1}\)</span>) is 0</li>
</ul></li>
<li><span class="math inline">\(\beta_{1}\)</span>
<ul>
<li>Regression coefficient/weight for predictor 1</li>
<li>Represents the magnitude and direction of the relationship between predictor 1 and the outcome as the change in the value of the outcome variable for every unit change in the value of predictor 1 (it is the gradient/slope of the regression line)</li>
</ul></li>
<li><span class="math inline">\(X_{i,1}\)</span>
<ul>
<li>The value of predictor 1 for observation <em>i</em></li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}\)</span>
<ul>
<li>Error for observation <em>i</em></li>
<li>Error - The absolute deviation/distance/difference between the observed value and the expected/predicted/fitted value for observation <em>i</em></li>
<li>This quantifies the error in prediction at the observation level</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Sample Model</strong>
<ul>
<li><span class="math inline">\(y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i,1} + e_{i}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><strong>Population Model</strong>
<ul>
<li><span class="math inline">\(\begin{aligned}\mathbf{Y} &amp;= \mathbf{X}\boldsymbol{\beta} + \mathbf{E} \\ \begin{bmatrix} y_{i} \\ y_{i} \\ y_{3} \\ \vdots \\ y_{n}\end{bmatrix} &amp;= \begin{bmatrix} 1 &amp; x_{1} \\ 1 &amp; x_{2} \\ 1 &amp; x_{3} \\ \vdots &amp; \vdots \\ 1 &amp; x_{n}\end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \end{bmatrix} + \begin{bmatrix} \epsilon_{1} \\ \epsilon_{2} \\ \epsilon_{3} \\ \vdots \\ \epsilon_{n}\end{bmatrix}\end{aligned}\)</span>
<ul>
<li><strong>Where</strong>
<ul>
<li><span class="math inline">\(\mathbf{Y}\)</span>
<ul>
<li>A column vector that contains the values for the outcome variable for each of the observations</li>
</ul></li>
<li><span class="math inline">\(\mathbf{X}\)</span>
<ul>
<li>The model matrix/design matrix</li>
<li>The model matrix contains
<ul>
<li>Column vector <span class="math inline">\(\mathbf{U}\)</span>
<ul>
<li>A column vector of 1s one for each observation. The column of 1s corresponds to <span class="math inline">\(\beta_{0}\)</span> in the <span class="math inline">\(\boldsymbol{\beta}\)</span> matrix, which means that the intercept is the same for every observation.</li>
</ul></li>
<li>Column vector of <em>x</em> values
<ul>
<li>A column vector that contains the values of predictor variable 1 for each of the observations</li>
</ul></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span>
<ul>
<li>A column vector that contains the regression coefficient/weight for each of regression parameters</li>
</ul></li>
<li><span class="math inline">\(\mathbf{E}\)</span>
<ul>
<li>A column vector that contains the error of each of the observations</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Methods of Estimation</strong>
</p>
<ul>
<li>Ordinary Least Squares (OLS)</li>
<li>Generalised Least Squares (GLS)</li>
<li>Maximum Likelihood (ML)</li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Ordinary Least Squares</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Estimates the regression line by fitting a line that minimises the model sum of squared residuals (the sum of the squared difference/distance/deviation between each of the observed values and the corresponding predicted/fitted values)</li>
<li><span class="math inline">\(\min{\sum_{i=1}^{n}{e^{2}_{i}}} = \min{\sum_{i=1}^{n}({y_{i} - \hat\beta_{0} - \hat\beta_{1}x_{i,1}}})^2\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\hat\beta_{0} = \bar{y} - \hat\beta_{1}\bar{x}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})y_{i}}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}\)</span>
<ul>
<li>Note
<ul>
<li>I think these two equations are rather boring, looking at how they are derived is much more interesting</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Derivation</strong>
<ul>
<li>Start off with the linear model with the outcome variable as a function of the weights
<ul>
<li><span class="math inline">\(y_i = \hat\beta_0 + \hat\beta_{1}x_{i,1} + e_i\)</span></li>
</ul></li>
<li>Make the residual as the subject (express the residual as a function of the weights)
<ul>
<li><span class="math inline">\(e_i = y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1}\)</span></li>
</ul></li>
<li>Square both sides
<ul>
<li><span class="math inline">\(e_{i}^{2} = (y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2\)</span></li>
</ul></li>
<li>Sum both sides
<ul>
<li><span class="math inline">\(\begin{aligned}\sum_{i=1}^{n}{e_{i}^{2}} &amp;= \sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2} \\ S(\beta_0,\beta_1) &amp;= \sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2}\end{aligned}\)</span></li>
<li><img src="image/ols_1.png" style="width:30.0%" /></li>
</ul></li>
<li>Find the values of <span class="math inline">\(\hat\beta_{0}\)</span> and <span class="math inline">\(\hat\beta_{1}\)</span> that minimises <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span>
<ul>
<li>There is a point at which the sum of squared residuals is at the minimum</li>
<li>The <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span> function is a quadratic and it has a minimum that represents the minimal sum of squared residuals</li>
<li>The coordinates of the minimum are the values of <span class="math inline">\(\hat\beta_{0}\)</span> and <span class="math inline">\(\hat\beta_{1}\)</span> that give you the least sum of squared residuals (or minimises <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span>)</li>
<li>Hence, find the values of the coordinate of the minimum of the quadratic function</li>
<li>To find each of the values of the coordinate of the minimum of the function <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span>, the partial derivatives of the function <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span> with respect to each of the <span class="math inline">\(\hat\beta\)</span>s are set to 0 (because the minimum has a slope of 0)</li>
<li>Expression for <span class="math inline">\(\hat\beta_0\)</span>
<ul>
<li><span class="math inline">\(\frac{\partial{S}}{\partial{\hat\beta_{0}}}=-2\sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})}\)</span></li>
</ul></li>
<li>Expression for <span class="math inline">\(\hat\beta_1\)</span>
<ul>
<li><span class="math inline">\(\frac{\partial{S}}{\partial{\hat\beta_{1}}}=-2\sum_{i=1}^{n}{x_{i,1}(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})}\)</span></li>
</ul></li>
<li>Solve the above simultaneous equations</li>
<li>Solving the above simultaneous equations would result in
<ul>
<li><span class="math inline">\(\hat\beta_{0} = \frac{\sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{y_{i}} - \sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{x_{i,1}y_{i}}}{n\sum_{i=1}^{n}{x_{i,1}^2}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{n\sum_{i=1}^{n}{x_{i,1}y_i} - \sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{y_{i}}}{n\sum_{i=1}^{n}{x_{i,1}^2}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
</ul></li>
<li>Solving the simultaneous equations would also result in
<ul>
<li>For <span class="math inline">\(\hat\beta_{0}\)</span>
<ul>
<li><span class="math inline">\(\hat\beta_{0} = \bar{y} - \hat\beta_{1}\bar{x}\)</span></li>
</ul></li>
<li>For <span class="math inline">\(\hat\beta_{1}\)</span>
<ul>
<li><span class="math inline">\(\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})(y_{i} - \bar{y})}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})y_{i}}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{Cov(x,y)}{s_{x}^{2}}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{S_{xx}}{S_{xx}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(\boldsymbol{\beta} = (\mathbf{X&#39;X)^{-1}X&#39;Y}\)</span></li>
</ul></li>
<li><strong>Derivation (Matrix)</strong>
<ul>
<li><span class="math inline">\(S(\boldsymbol{\hat\beta}) = ||\mathbf{u}||^2 = ||\mathbf{Y-\hat{Y}}||^2 = ||\mathbf{Y-X\boldsymbol{\beta}}||^2\)</span></li>
<li><em>note</em> - Matrix derivation not complete</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Variance of <span class="math inline">\(\beta\)</span></strong>
</p>
<ul>
<li>Variance of <span class="math inline">\(\beta\)</span>
<ul>
<li><strong>Variance of <span class="math inline">\(\beta_{0}\)</span></strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\text{Var}(\hat\beta_{0}) = \sigma_{\hat\beta_{0}}^{2} = \frac{\sigma^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\text{SE}_{\hat\beta_{0}} = \sigma_{\hat\beta_{0}} = \sqrt{\frac{\sigma^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}}\)</span></li>
</ul></li>
<li><strong>Estimation from the sample</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li><span class="math inline">\(\sigma^{2}\)</span> is unknown</li>
<li>Since <span class="math inline">\(\sigma^{2}\)</span> is the variance of the population model (the average model sum of squared errors), it is reasonable to use the variance of the sample model (<span class="math inline">\(s^2\)</span>) as the estimator</li>
<li>It has been shown that <span class="math inline">\(s^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^{2}\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\hat{\text{Var}(\hat\beta_{0})} = \hat\sigma_{\hat\beta_{0}}^{2} = \frac{s^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\hat{\text{SE}}_{\hat\beta_{0}} = \hat\sigma_{\hat\beta_{0}} = \sqrt{\frac{s^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Variance of <span class="math inline">\(\beta_{1}\)</span></strong>
<ul>
<li><strong>Population</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\sigma_{\hat\beta_{1}^{2}} = \frac{n\sigma^{2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}- (\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\text{Var}(\hat\beta_{1}) = \sigma_{\hat\beta_{1}^{2}} = \frac{n\sigma^{2}}{n\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\text{Var}(\hat\beta_{1}) = \sigma_{\hat\beta_{1}}^{2} = \frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\text{SE}_{\hat\beta_{1}} = \sigma_{\hat\beta_{1}} = \sqrt{\frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Estimation from the sample</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\hat{\text{Var}(\hat\beta_{1})} = \hat\sigma_{\hat\beta_{1}}^{2} = \frac{s^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\hat{\text{SE}}_{\hat\beta_{1}} = \hat\sigma_{\hat\beta_{1}} = \sqrt{\frac{s^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Assumptions of the Linear Models</strong>
</p>
<ul>
<li>Linearity</li>
<li>Additivity</li>
<li>Normality of the sampling distribution</li>
<li>Uncorrelated errors</li>
<li>Heteroscedasticity</li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Linearity</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The relationship between each of the predictor variables and the outcome is linear (or best described as linear)</li>
<li>This is the most important assumption of linear models because even if all other assumptions are met, the model is invalid because the description of the process you want to model is wrong</li>
</ul></li>
<li><strong>Treatment</strong>
<ul>
<li>Transformation</li>
<li>Segmented regression</li>
<li>Fit non-linear models</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Additivity</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The combined effect of multiple predictors in a multiple regression is best described as a linear combination of the predictors</li>
<li>This, along with linearity, is the most important assumptions of linear models</li>
</ul></li>
<li><strong>Treatment</strong>
<ul>
<li>Transformation</li>
<li>Segmented regression</li>
<li>Fit non-linear models</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Uncorrelated Errors</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The errors in the population model are uncorrelated with each other</li>
<li>In other words, there is no autocorrelation/serial correlation</li>
<li>The distribution of errors at given value of x is uncorrelated with the distribution of errors at another value of x</li>
<li>It is important to note that the errors are not individual errors, they are theoretical distribution of errors given x, in other words, for each vale of x, there is a distribution of errors, and the observed residuals given x you have are sampled from the distribution of errors given the same x (see <a href="https://stats.stackexchange.com/questions/103439/how-to-calculate-the-covariance-between-two-observations-of-the-same-variable">this short description on CrossValidated</a>)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\text{Cov}(e_{i}, e_{j}) = 0\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\boldsymbol\Phi = \Sigma_{e_{i}e_{j}} = \sigma^{2}\mathbf{I} = \begin{bmatrix} \sigma_{1,1}^{2} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma_{2,2}^{2} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; \sigma_{3,3}^{2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma_{i,j}^{2} \end{bmatrix}\)</span>
<ul>
<li><strong><em>Note</em></strong>
<ul>
<li>All the off-diagonals are 0s, indicating that all errors are uncorrelated with each other</li>
<li>The variances are unconstrained, hence, they can vary</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Effects of correlated errors on OLS estimation</strong>
<ul>
<li><strong>Biased SE</strong>
<ul>
<li>The standard mathematical expression of the estimate of the variance of the sampling distribution (<span class="math inline">\(\frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span>) is a biased estimate of the true variance of the sampling distribution
<ul>
<li>If errors are positively correlated, the standard mathematical expression of the estimate of the variance of the sampling distribution (<span class="math inline">\(\frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span>) will underestimate the true value (a negative bias), this means that the true sampling variance and SE are larger than estimated, this means that OLS is less efficient/optimal than other estimators, this follows that all statistical inferences involving the SE will be biased (e.g. underestimating the confidence intervals, overestimating the p-value in significance tests, higher Type I error etc.)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Assessing Autocorrelation</strong>
<ul>
<li>See the autocorrelation section</li>
</ul></li>
<li><strong>Treatment</strong>
<ul>
<li>Autocorrelation consistent standard errors</li>
<li>Cluster robust standard errors</li>
<li>Use other estimators</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Normality of the Sampling Distribution</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The sampling distribution of the sample estimates is normally distributed</li>
<li>A normally distributed sampling distribution is desirable because it can be modelled easily with the normal distribution curve and inferential statistics can be easily derived (which may be easier than if it has a non-normal distribution, e.g. Pearson’s r)</li>
</ul></li>
<li><strong>Effects of violation of the assumption</strong>
<ul>
<li>All the standard mathematical expressions and procedures for estimating the inferential statistics and uncertainty of the sample estimate are based on the assumption that the sampling distribution is normal or at least approximately normal. If the true sampling distribution is not normal, then all the inferential statistics and estimates of uncertainty of the sample estimate are incorrect</li>
</ul></li>
<li><strong>Assessing Normality of the sampling distribution</strong>
<ul>
<li>Distribution of model residuals
<ul>
<li><strong>Description</strong>
<ul>
<li>Assess whether the model residuals are normally distributed</li>
</ul></li>
<li><strong>Explanation</strong>
<ul>
<li>In general, in a linear combination of variables, if the variables in the linear combination are random, independent, and normally distributed (their sampling distribution), then the linear combination itself is also normally distributed (the sampling distribution of the linear combination is normal)</li>
<li>Each of the regression parameters can be expressed as a linear combination
<ul>
<li><span class="math inline">\(\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_i - \bar{x} )y_i}}{\sum_{i=1}^{n}{(x_i - \bar{x})^2}} = \sum_{i=1}^{n}{\frac{x_i - \bar{x}}{\sum_{i=1}^{n}{(x_i - \bar{x})^2}}y_i}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\frac{x_i - \bar{x}}{\sum_{i=1}^{n}{(x_i - \bar{x})^2}}\)</span> is treated as a constant and <span class="math inline">\(y_i\)</span> is treated a variable</li>
</ul></li>
</ul></li>
</ul></li>
<li>Hence, for the linear combination (<span class="math inline">\(\hat\beta_1\)</span>) to be normally distributed, the random variable <span class="math inline">\(y_i\)</span> has to be independent and normally distributed</li>
<li>Since independence and normality of the errors imply independence and normality of the random variable <span class="math inline">\(y_i\)</span>, normality of errors can be used to indicate that the linear combination (<span class="math inline">\(\hat\beta_1\)</span>) is normally distributed</li>
<li>And since we don’t have the errors in hand, we use the model residuals</li>
</ul></li>
<li><strong>Assessing normality</strong>
<ul>
<li>See the ‘Assessing Normality’ section</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The Central Limit Theorem and the assumption of normality</strong>
<ul>
<li>The distribution of the residuals is not important if sample size is “sufficiently” large according to the Central Limit Theorem. (The CLT states that the sampling distribution is approximately normal if sample size is large and can be assumed to be normal if sample size is sufficiently large (n &gt; 30) regardless of the shape of the population distribution)</li>
<li>Hence, there is much less to worry about the non-normality of the sampling distribution if the sample size is “sufficiently” large</li>
</ul></li>
<li><strong>Just some side notes</strong>
<ul>
<li><span class="math inline">\(y_i\)</span> also needs to be normally distributed, if y is not normally distributed, then the scores of y need to be transformed such that it’s sampling distribution is normal or approximately normal (I am still not sure about this part)</li>
</ul></li>
<li><strong>Treatment</strong>
<ul>
<li>Use robust/non-parametric methods</li>
<li>Transformation</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Heteroscedasticity</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>Aka - Homogeneity of variance or constant variance</li>
<li>The variance of the distribution of errors given x is the identical between values of x in the population model</li>
<li>The variance of the distribution of errors given x is a constant - It is a fixed value and does not vary across values of x in the population model</li>
<li>Some sources say that the variance of errors given x does not depend on x (e.g. Rice, 2006; Field, 2023)</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/heteroscedasticity_visualisation.png" style="width:30.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\text{Var}(\epsilon_{i}) = \sigma^2\)</span></li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(\boldsymbol\Phi = E(\epsilon\epsilon&#39;|X) = \Sigma_{\epsilon\epsilon} = \begin{bmatrix} \sigma^{2} &amp; \phi_{1,2} &amp; \phi_{1,3} &amp; \cdots &amp; \phi_{1,j} \\ \phi_{2,1} &amp; \sigma^{2} &amp; \phi_{2,3} &amp; \cdots &amp; \phi_{2,j} \\ \phi_{3,1} &amp; \phi_{3,2} &amp; \sigma^{2} &amp; \cdots &amp; \phi_{3,j} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \phi_{i,1} &amp; \phi_{i,2} &amp; \phi_{i,3} &amp;\cdots &amp; \sigma^{2} \end{bmatrix}\)</span>
<ul>
<li><strong><em>Notes</em></strong>
<ul>
<li>The variances in the diagonals are all <span class="math inline">\(\sigma^2\)</span></li>
<li>The covariances in the off-diagonals can vary (the covariances are not the condition here)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Effects of Homoscedasticity</strong>
<ul>
<li><strong>Biased estimation of sampling variance</strong>
<ul>
<li>The mathematical expression for the estimate of the sampling variance assumes homoscedasticity (see derivation) (or rather, requires homoscedasticity to be unbiased). If there is heteroscedasticity, then the sampling variance estimated using the same standard mathematical expression will be biased, in that the estimated value is not the same as the true value, this in turn means that the parameter estimates are not optimal due to increased sampling variation and there are other estimators that will be mor efficient than OLS, the biased sampling variance estimate will affect the SE estimate, and will affect all statistical inferences using the sampling
variance (e.g. confidence intervals, test statistic, p-values etc.)</li>
</ul></li>
<li>Confidence intervals can be “extremely inaccurate” when ignoring heteroscedasticity (Wilcox, 2010)</li>
<li>But this assumption matters only if group sizes are unequal</li>
</ul></li>
<li><strong>Assessing Homo/Heteroscedasticity</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The assumption applies to the population model, it is best to assess heteroscedasticity in the population through heteroscedasticity in the sample model</li>
</ul></li>
<li><strong>Ways to assess heteroscedasticity</strong>
<ul>
<li>Plots</li>
<li>Statistical tests</li>
</ul></li>
<li><strong>Plots</strong>
<ul>
<li><strong>Residual Plots</strong>
<ul>
<li>Residuals vs fitted plot</li>
</ul></li>
<li><strong>Residuals vs fitted plot</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The residuals are plotted against the fitted values</li>
<li>A plot that shows the vertical distribution of residuals in the y-axis for each of the fitted values in the x-axis</li>
<li>Basically it is the scatter plot but detrended</li>
<li>It can be based on raw or standardised scores</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/het_resid_vs_fitted.png" style="width:50.0%" /></li>
</ul></li>
<li><details>
<summary>
<img src="image/r.svg" width = 5% style=" margin-top: 0px; margin-bottom: 0px; padding-top: 1px;"/>
</summary>
<pre style=" margin-top: 10px"><code>
## Base R <br>
model <- lm(formula = outcome ~ predictor, data = data) <br>
&nbsp
plot(model, which = 1) 
</code></pre>
</details></li>
</ul></li>
</ul></li>
<li><strong>Statistical Tests</strong>
<ul>
<li><strong>Statistical Tests</strong>
<ul>
<li>Levene’s test</li>
<li>Brown-Forsythe test</li>
<li>Hartley’s Fmax</li>
</ul></li>
<li><strong>Levene’s tests</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Tests the null hypothesis that the variances in different groups are equal</li>
<li>It is a one-way ANOVA with absolute deviation as the outcome variable and the groups/values of x as the predictor variable</li>
<li>It tests whether the absolute deviation varies between groups/values of x</li>
</ul></li>
<li><strong>Original Levene’s test</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\textit{W} = \frac{\sum_{i=1}^{n}{n_{g}(|\bar{d}|_{g} - |\bar{d}|_{G})^2}}{\sum_{i=1}^{n}{n_{g}(|{d}|_{i} - |\bar{d}|_{g})^2}} \times \frac{n-k}{k-1}\)</span>
<ul>
<li><strong><em>Where</em></strong></li>
<li><span class="math inline">\(g\)</span> - Denotes group</li>
<li><span class="math inline">\(G\)</span> - Denotes grand</li>
<li><span class="math inline">\(d_i = y_i - \bar{y}_{g}\)</span> - A deviation means the difference between a score and mean of the group in which it belongs</li>
<li><strong><em>Note</em></strong>
<ul>
<li>Levene’s test statistic is expressed as such but it can be interpreted as the F-statistic</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Brown-Forsythe test</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Similar to Levene’s test but uses the median in the absolute deviation instead (<span class="math inline">\(d_i = y_i - \tilde{y}_{g}\)</span>)</li>
</ul></li>
</ul></li>
<li><strong>Levene’s test using trimmed mean</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Similar to Levene’s test but uses the trimmed mean instead of the mean</li>
</ul></li>
</ul></li>
<li><strong>Comparing the tests</strong>
<ul>
<li>Trimmed mean performed best when the underlying data followed a Cauchy distribution and the median performed best when the data followed a chi-squared distribution with 4 degrees of freedom (sharply skewed distribution) (Brown &amp; Forsythe, 1974; extracted from <a href="https://en.wikipedia.org/wiki/Levene%27s_test">Wiki</a>)</li>
</ul></li>
</ul></li>
<li><strong>Hartley’s Fmax (Pearson &amp; Harley, 1954)</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>It is the ratio of the variances between the group with the biggest variance and the group with the smallest variance</li>
<li>It then conducts a null hypothesis significance test for the ratio</li>
<li>The Fmax isn’t used very often, so it is harder to find a critical value table</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\textit{F}_{max} = \frac{\max{s_{g}^{2}}}{\min{s_{g}^{2}}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\max{s_{g}^{2}}\)</span> - The largest group variance among all the groups</li>
<li><span class="math inline">\(\min{s_{g}^{2}}\)</span> - The smallest group variance among all the groups<br />
</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Cochrane’s C test</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>It tests whether the variance of a particular group is exceptionally larger than the rest of the group (it is an upper tail test)</li>
<li>The test statistic is the ratio of the variane of a group of interest and the overall variance (of all of the groups)</li>
<li>It then conducts a null hypothesis significance test on the ratio</li>
<li>It is used to test for variance outlier</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\textit{C}_{g} = \frac{s_{g}^{2}}{\sum_{i=1}^{n}{s_{g}^{2}}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(s_{g}^{2}\)</span> - The variance of a particular group</li>
<li><span class="math inline">\(\sum_{i=1}^{n}{s_{g}^{2}}\)</span> - The sum of all the variances (the total variance)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Null Hypothesis Significance Test</strong>
<ul>
<li><strong>Hypothesis</strong>
<ul>
<li><span class="math inline">\(\textit{H}_{0}:\)</span> All variances are equal</li>
<li><span class="math inline">\(\textit{H}_{1}:\)</span> At least one group variance is significantly larger than the other variances</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Don’t use these tests</strong>
<ul>
<li><strong>Sample size</strong>
<ul>
<li>In large samples, even trivial heteroscedasticity can produce significant Levene’s result</li>
<li>In small samples, Levene’s test has inadequate power to detect heteroscedasticity</li>
</ul></li>
<li><strong>Group size equality</strong>
<ul>
<li>Homoscedasticity tests work best when group sample sizes are equal (when heteroscedasticity does not matter as mentioned before) and does not work well when sample sizes are unequal (when heteroscedasticity matters)</li>
</ul></li>
<li><strong>Adjust for it instead…</strong>
<ul>
<li>There are robust methods for heteroscedasticity, hence, no need to drill on testing heteroscedasticity</li>
</ul></li>
<li>Read Zimmerman (2004) about the problems with heteroscedasticity tests…</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Remedies</strong>
<ul>
<li>Heteroscedasticity-Consistent Standard Errors</li>
<li>Weighted Least Squares</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Heteroscedasticity-Consistent Standard Error</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>Aka
<ul>
<li>Heteroscedasticity-Robust Standard Errors</li>
<li>Robust Standard Errors</li>
<li>Eicker-Huber-White standard errors</li>
<li>Huber-White Standard Errors</li>
<li>White Standard Errors</li>
<li>Sandwich Estimator (because the matrix expression looks like a sandwich)</li>
</ul></li>
</ul></li>
<li><strong>Versions</strong>
<ul>
<li>HC1</li>
<li>HC2</li>
<li>HC3</li>
<li>HC4</li>
</ul></li>
<li><strong>HC0</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The first version of the HC standard error</li>
</ul></li>
<li><strong>Mathematical desciption</strong>
<ul>
<li>In the mathematical expression of the estimate of the sampling variance of the beta parameters, the variance of errors given x is not treated as fixed/a constant, rather, it relaxes the assumption of homoscedasticity and treat the variance of errors given x are different at different values of x</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><strong>Population Model</strong>
<ul>
<li><span class="math inline">\(\textit{Var}(\hat\beta_{1}) = \frac{1}{(\sum_{i=1}^{n}{(x_i - \bar{x})^2})^2} \times \sum_{i=1}^{n}{(x_i - \bar{x})^2 \textit{Var}(\epsilon_i|X)}\)</span></li>
</ul></li>
<li><strong>Sample Estimatte</strong>
<ul>
<li><span class="math inline">\(\textit{Var}(\hat\beta_{1}) = \frac{1}{(\sum_{i=1}^{n}{(x_i - \bar{x})^2})^2} \times \sum_{i=1}^{n}{(x_i - \bar{x})^2 e_{i}^{2}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(e_{i}^{2}\)</span> - Squared residual for observation i (it is basically the variance of residual given x, but we usually only have 1 observation given x)</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(\boldsymbol{\hat\beta_1} = (\mathbf{X&#39;X})^{-1}(\sum_{i=1}^{n}{e_{i}^{2}x&#39;_{i}x_{i}})(\mathbf{X&#39;X})^{-1}\)</span></li>
</ul></li>
<li><strong>Evaluation</strong>
<ul>
<li>Simulations show that it is better than the normal estimation when sample size is large but worse when sample size is small</li>
</ul></li>
</ul></li>
<li><strong>Which version is the best?</strong>
<ul>
<li>HC3 is better than HC0 and HC2 (Long &amp; Ervin, 2000)</li>
<li>HC4 appears to be more robust than HC3 when there are influential cases and non-normal errors (See Hayes and Cai, 2007 for a review)</li>
</ul></li>
<li><strong>Heteroscedastic-Autocorrelation-consistent variance estimation</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Variance estimation robust to heteroscedasticity and autocorrelation</li>
<li>They are extensions of the HC standard error</li>
</ul></li>
<li><strong>Types</strong>
<ul>
<li>Newey and West (1987)</li>
</ul></li>
</ul></li>
<li><strong>Cluster Robust Standard Errors</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Standard Errors estimation that accounts for heteroscedasticity and correlated errors</li>
<li>Based on the concept that data points can be clustered in a certain way</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\textit{Var}(\hat\beta_{1}) = \frac{1}{(\sum_{i=i}^{n}{(x_{i} - \bar{x})^2})^2} \times \sum_{i=1}^{n}{\textit{Var}(\epsilon_{i}(x_{i} - \bar{x}))} + 2\sum_{j=1}^{n}{\sum_{i=1}^{n}{(x_{i} - \bar{x})(x_{j} - \bar{x})(\epsilon{}_{i}\epsilon{}_{j})}}\)</span></li>
<li><span class="math inline">\(\textit{Var}(\hat\beta_{1}) = \frac{1}{(\sum_{i=i}^{n}{(x_{i} - \bar{x})^2})^2} \times \sum_{j=1}^{n}{\sum_{i=1}^{n}{\textit{Cov}((x_{i} - \bar{x})\epsilon{}_{i}, (x_{j} - \bar{x})\epsilon{}_{j})}}\)</span></li>
<li><span class="math inline">\(\textit{Var}(\hat\beta_{1}) = \frac{1}{(\sum_{i=i}^{n}{(x_{i} - \bar{x})^2})^2} \times \sum_{j=1}^{n}{\sum_{i=1}^{n}{(x_{i} - \bar{x})(x_{j} - \bar{x})(\epsilon{}_{i}\epsilon{}_{j})}}\)</span></li>
<li><span class="math inline">\(\textit{Var}(\hat\beta_{1}) = \frac{1}{(\sum_{i=i}^{n}{(x_{i} - \bar{x})^2})^2} \times \sum_{j=1}^{n}{\sum_{i=1}^{n}{(x_{i} - \bar{x})(x_{j} - \bar{x})(\epsilon{}_{i}\epsilon{}_{j})}}1[A]\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(1[A]\)</span>
<ul>
<li><span class="math inline">\(A\)</span> is an event in which i and j are in the same cluster</li>
<li><span class="math inline">\(1[A]\)</span> - A indicator function that indicates whether 2 errors belong to the same cluster</li>
<li>The indicator function <span class="math inline">\(1[A]\)</span> equals 1 if event A happens (i and j are in the same cluster)</li>
<li>The indicator function <span class="math inline">\(1[A]\)</span> equals 0 if event A does not happen (i ad j are not in the same cluster)</li>
<li>This results in a block matrix where the diagonal blocks are the error variance-covariance submatrices and the off-diagonal blocks are all 0s (assuming that the clusters are uncorrelated with each other) - For this reason, it is robust for arbitrary within cluster error correlation but not between cluster error correlation</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(\textbf{V} = (\textbf{X}&#39;\textbf{X})^{-1} \textbf{X}&#39;\boldsymbol\Phi \textbf{X}(\textbf{X}&#39;\textbf{X})^{-1}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\boldsymbol\Phi\)</span> - Error Variance-Covariance matrix</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>The Bootstrap (Efron &amp; Tibshirani, 1993)</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>Estimates the properties of the sampling distribution empirically from the sample data</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Autocorrelation</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>Aka serial correlation</li>
<li>Errors given x are correlated with errors in other values of x</li>
<li>Autocorrelation is often described with respect to time because autocorrelation usually occurs in time series data (correlated across time)</li>
</ul></li>
<li><strong>Reasons for autocorrelation</strong>
<ul>
<li><strong>Temporal autocorrelation</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Autocorrelation across time time</li>
</ul></li>
<li><strong>Examples</strong>
<ul>
<li>Repeated measures designs</li>
<li>Time series</li>
<li>Longitudinal designs</li>
<li>Growth models</li>
</ul></li>
</ul></li>
<li><strong>Spatial autocorrelation</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Autocorrelation due to space</li>
</ul></li>
<li><strong>Examples</strong>
<ul>
<li>Clustered data</li>
</ul></li>
</ul></li>
<li><strong>Ignoring covariates</strong>
<ul>
<li>If a covariate is not included in the model, the errors will capture the effect of that covariate</li>
<li>The error is not random - There are systematic process in the errors that are mistreated as random or ignored</li>
</ul></li>
<li><strong>Model misspecification</strong>
<ul>
<li>E.g. ignoring linearity</li>
</ul></li>
<li><strong>Measurement error in the independent variable</strong></li>
</ul></li>
<li><strong>Types of serial correlation</strong>
<ul>
<li>First-order serial correlation</li>
</ul></li>
<li><strong>First-order serial correlation</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The correlation between errors at any one point of x (<span class="math inline">\(e_{x}\)</span>) and errors at the point of x that is 1 unit less (<span class="math inline">\(e_{x-1}\)</span>)</li>
<li>Note that t is often used instead of x, but I am keeping things general here</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><strong>Positive first-order serial correlation</strong>
<ul>
<li><img src="image/positive_first_order_serial_correlation.png" /></li>
</ul></li>
<li><strong>Negative first-order serial correlation</strong>
<ul>
<li><img src="image/negative_first_order_serial_correlation.png" /></li>
</ul></li>
</ul></li>
<li><strong>Mathematical concept</strong>
<ul>
<li>Tests testing for first-order serial correlation usually assess whether errors given <span class="math inline">\(x\)</span> can be predicted by errors a unit of x that is 1 unit smaller (<span class="math inline">\(x - 1\)</span>)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(e_{t} = \rho{}e_{x-1} + u_x\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\rho{}\)</span> - First-order serial correlation coefficient</li>
<li>The process for the error term is called the first-order autoregressive process (AR1)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Tests for first-order serial correlation</strong>
<ul>
<li>Durbin-Watson test</li>
<li>Breusch-Godfrey test</li>
</ul></li>
<li><strong>Durbin-Watson test</strong>
<ul>
<li><strong>The Durbin-Watson statistic</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(d = \frac{\sum_{t=2}^{n}{(e_{t} - e_{t-1})^2}}{\sum_{t=1}^{n}{e_{t}^{2}}}\)</span></li>
</ul></li>
<li><strong>Interpretation</strong>
<ul>
<li><span class="math inline">\(0 ≤ d ≤ 4\)</span></li>
<li><span class="math inline">\(d = 2\)</span> - No serial correlation</li>
<li><span class="math inline">\(d &lt; 2\)</span> - Positive serial correlation (<span class="math inline">\(d &lt; 1\)</span> is interpreted as serious positive serial correlation and thus concerning)</li>
<li><span class="math inline">\(d &gt; 2\)</span> - Negative serial correlation</li>
</ul></li>
</ul></li>
<li><strong>Significance testing</strong>
<ul>
<li>Statistical significance of the Durbin-Watson statistic us often sort through assessing whether the observed Durbin-Watson statistic is inside or outside the critical boundary in the sampling distribution under the null hypothesis</li>
<li>The critical boundary is a function of sample size and the number of variables</li>
</ul></li>
</ul></li>
<li><strong>Breusch-Godfrey test (Breusch &amp; Godfrey, 1978)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka auxiliary regression (A regression for supplementary purposes)</li>
<li>Tests the extent to which the errors given x can be predicted by the predictor in the model and the model error at a value of x that is 1 less, model error at a value of x that is 2 less, model error at a value of x that is 3 less, and so on (these are the lagged variables, and the number of lagged variables is defined by the researcher, but some people may include lagged variables until the errors in that auxiliary regression is no longer autocorrelated)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(e_{t} = \beta_{0} + \beta_{1}x_i + \beta_{2}e_{t-1} + \beta_{3}e_{t-2} + \beta_{4}e_{t-3} + \cdots + \beta_{p+1}e_{t-p} + u_i\)</span></li>
</ul></li>
<li><strong>Null Hypothesis Significance Test</strong>
<ul>
<li><strong>The Breusch-Godfrey test statistic</strong>
<ul>
<li><strong>Mathematics (wiki)</strong>
<ul>
<li><span class="math inline">\(nR^2\)</span></li>
</ul></li>
</ul></li>
<li><strong>Distribution of the Breusch-Godfrey test statistic</strong>
<ul>
<li><span class="math inline">\(nR^2 \sim{} \chi_{p}^{2}\)</span>
<ul>
<li><strong>Warning</strong>
<ul>
<li>A source (zed statistics) said it is <span class="math inline">\((n-p)R^2 \sim{} \chi_{p}^{2}\)</span></li>
<li>But they are very similar, so I think it’s fine</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hypotheses</strong>
<ul>
<li><span class="math inline">\(H_0:\)</span> There is no autocorrelation</li>
<li><span class="math inline">\(H_1:\)</span> There is autocorrelation in the lagged variables in the model</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Assessing Normality</strong>
</p>
<ul>
<li><strong>Ways to assess normality</strong>
<ul>
<li>Plots</li>
<li>Statistical Tests</li>
</ul></li>
<li><strong>Plots</strong>
<ul>
<li><strong>Types of plots</strong>
<ul>
<li>Frequency Distribution plot (e.g. histogram)</li>
<li>Theoretical vs Observed distribution plots</li>
</ul></li>
<li><strong>Theoretical vs Observed Distribution plots</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>A plot with the distributional points of a particular distribution of interest and the distributional points of the data</li>
<li>It visualises how well the distributional point of each of the data points fit their corresponding distributional point a particular distribution of interest (e.g. a normal distribution)</li>
<li>The corresponding distributional point is the expected value that the observed score should have in a particular distribution of interest</li>
</ul></li>
<li><strong>Types</strong>
<ul>
<li>P-P plot</li>
<li>Q-Q plot</li>
</ul></li>
<li><strong>P-P plot</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>A plot with the probability of each of the points of a particular distribution of interest against the probability of each of the data points in the data (relative to the whole data)</li>
</ul></li>
<li><strong>P-P plot for the normal distribution</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A plot with the probability of each of the points of a normal distribution against the probability of each of the data points in the data (relative to the whole data)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Q-Q plot</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>A plot with the quantile or z-score of each of the points of a particular distribution of interest against the quantile or z-score of each of the data points in the data (relative to the whole data)</li>
</ul></li>
<li><strong>Detrending (Thode, 2002)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><strong>Detrending</strong> - Transforming the plots such that the line showing the theoretical distributional values is horizontal and not diagonal</li>
<li>The diagonal arrangement of Q-Q and P-P plots can lead to visual bias, detrending the plots can reduce visual bias
<strong>Visualisation</strong>
<ul>
<li><img src="image/qqplot1.1.png" style="width:30.0%" /></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Q-Q plot for the normal distribution</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A plot with the quantile or z-score of each of the points of a normal distribution against the quantile or z-score of each of the data points in the data (relative to the whole data)</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/qqplot1.png" style="width:30.0%" /></li>
</ul></li>
<li><details>
<summary>
<img src="image/r.svg" width = 3% style=" margin-top: 0px; margin-bottom: 0px; padding-top: 1px;"/>
</summary>
<pre style=" margin-top: 10px"><code>
# Q-Q plot (general) <br>
ggplot2::ggplot(data = iris, aes(sample = Sepal.Length)) + <br>
&nbsp&nbsp qqplotr::stat_qq_point(size = 1, alpha = 0.6, detrend = FALSE) + <br>
&nbsp&nbsp qqplotr::stat_qq_line(colour = "#008898", detrend = FALSE) + <br>
&nbsp&nbsp qqplotr::stat_qq_band(fill = "#008898", alpha = 0.3, detrend = FALSE) + <br>
&nbsp&nbsp labs(x = "Theoretical Quantile", y = "Observed Quantile") +<br>
&nbsp&nbsp theme_minimal()<br>
&nbsp
# Q-Q plot for Model residual <br>
plot(model_object, which = 2)
</code></pre>
</details></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Statistical Tests</strong>
<ul>
<li><strong>Statistical Tests</strong>
<ul>
<li>Kolmogorov-Smirnov test</li>
<li>Shapiro-Wilk test</li>
</ul></li>
<li><strong>Kolmogorov-Smirnov test</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Tests the null hypothesis that the scores are normally distributed</li>
<li>It compares the scores to a normally distributed set of scores with the same mean and standard deviation</li>
</ul></li>
</ul></li>
<li><strong>The problem with statistical tests</strong>
<ul>
<li><strong>Statistical significance does not necessarily mean practical importance</strong>
<ul>
<li>Statistical significance tells how real an effect is but does not tell the practical importance/size of the effect</li>
</ul></li>
<li><strong>It doesn’t work when you need it, but works when you don’t need it (that’s what she said)</strong>
<ul>
<li>The smaller the sample size, the lower the power of statistical tests, the less likely normality problems are detected by these statistical tests, however, this is when the assumption of normality is more of a concern</li>
<li>The larger the sample size, the higher the power of statistical tests, the more likely normality problems are detected by these statistical tests, however, this is when the assumption of normality is less of a concern due to the CLT</li>
</ul></li>
<li><strong>Robust methods are the way to go</strong>
<ul>
<li>Violation of the normality assumption can be adjusted, hence, we don’t have to drill on statistical tests (comparing non-robust result with robust result as a sensitivity analysis may be a better option)</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Robust Techniques</strong>
</p>
<ul>
<li><strong>Robust Techniques</strong>
<ul>
<li>Trimming</li>
<li>Winsorizing</li>
<li>The Bootstrap</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Robust Estimators</strong>
</p>
<ul>
<li><strong>Robust Estimators</strong>
<ul>
<li>M-estimators</li>
<li>L-estimators</li>
<li>S-estimators</li>
<li>R-estimators</li>
</ul></li>
<li><strong>Extremum estimator (<span class="math inline">\(\hat\theta\)</span>)</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li><strong>Extremum Estimator</strong> - A class of estimators in which the parameters are estimated through maximisation or minimisation of a certain obective function that depends on the data</li>
<li>The theory of extremum estimators was developed by Amemiya (1985)</li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>M-estimators</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li><strong>M-estimators</strong> - Extremum estimators for which the obejct function is a sample average</li>
</ul></li>
<li><strong>History</strong>
<ul>
<li>These are called M-estimators because Maximum likelihood is one of the first of these estimators, hence, Huber (1981, p. 43) called these estimators as M-estimators to denote that these are “maximum likelihood-type” estimators</li>
</ul></li>
<li><strong>Examples</strong>
<ul>
<li>Least squares - Because the estimator is defined as a minimum of the sum of the squared errors/residuals</li>
<li>Maximum Likelihood- Because it maximises the likelihood function over the parameter space</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Trimming</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>Deleting scores from the extremes</li>
<li>Robust methods tend to use trimming (e.g. trimmed mean, the median, M-estimators, etc.)</li>
<li>Trimming can be baesd on percentage or standard deviation of the scores relative to the data</li>
<li>Standard deviation-based trimming is a bad idea (Field, 2023)</li>
</ul></li>
<li><strong>Percentage-based trimming</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Deleting certain percentage of data points from each of the extremes before any statistic is calculated</li>
</ul></li>
<li><strong>10% trimming</strong>
<ul>
<li>Deleting 10% of the data points from each of the extremes</li>
<li>If the sample size is 100, then 10 of the data points at the higher end (because 0% of 100 is 10) and 10 data points at the lower end will be deleted</li>
</ul></li>
<li><strong>The median is a trimmed mean</strong>
<ul>
<li>The median is the mean when all but the middle data point are trimmed</li>
</ul></li>
<li><strong>A trimmed statistic</strong>
<ul>
<li>A statistic that is calculated on the trimmed data (e.g. trimmed mean)</li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Winsorizing</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>Arranging the data in ascending order and replacing values outside a threshold boundary (outside of which is considered extremes of the data) woth the value at the threshold boundary</li>
</ul></li>
<li><strong>Defining the threshold boundary </strong>
<ul>
<li><strong>Percentage-based threshold boundary</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The threshold boundary is determined by a percentage of the data
<strong>Example</strong></li>
<li><strong>The data</strong>
<ul>
<li>9, 19, 22, 4, 8, 21, 9, 10, 0, 0, 1, 17, 4, 19, 13, 11, 0, 5, 3, 4</li>
</ul></li>
<li><strong>Arrange in ascending order</strong>
<ul>
<li>0, 0, 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 21, 22</li>
</ul></li>
<li><strong>Winsorize</strong>
<ul>
<li><strong>5% Winsorization</strong>
<ul>
<li>The bottom 5% and upper 5% of the data is replaced</li>
<li>5% of a data with n = 20 is 1</li>
<li>Hence, the bottom first data point is replaced with the value after it, and the upper first data point is replaced with the value before it</li>
<li><mark>0</mark>, 0, 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, 21, <mark>21</mark></li>
</ul></li>
<li><strong>10% Winsorization</strong>
<ul>
<li>The bottom 10% and upper 10% of the data is replaced</li>
<li>10% of a data with n = 20 is 2</li>
<li>Hence, the bottom 2 data points are replaced with the value after the second data point, the upper 2 data points are replaced with the value before the boundary</li>
<li><mark>0, 0 </mark>, 0, 1, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, 19, 19, <mark>19, 19</mark></li>
</ul></li>
<li><strong>20% Winsorization</strong>
<ul>
<li>The data points in the bottom 20% and those in the upper 20% of the data is replaced<br />
</li>
<li>20% of the data of N = 20 is 4</li>
<li>Hence the bottom 4 data points are replace with the value at the lower boundary, the upper 4 data poitns are replaced with the value at the upper boundaryy</li>
<li><mark>3, 3, 3, 3 </mark>, 3, 3, 4, 4, 5, 8, 9, 9, 10, 11, 13, 17, <mark>17, 17, 17, 17</mark></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Standard-deviation based threshold boundary</strong>
<ul>
<li>The threshold boundary is determined by the standard deviation of the data</li>
<li>The scores at the are replaced by the unstandardised score that has a standardised score at the boundary (not repeating the actual score at the boundary as in trimming)</li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Theil-Sen Estimator</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>Robust regarding outliers among the predictor variables</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\hat\beta_{p} = \tilde{\beta_{p,ij}}\)</span> (need to double check)</li>
</ul></li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>

<a href="generalised-least-squares.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/samuel-mak/statistics/edit/master/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/samuel-mak/statistics/blob/master/index.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub", "bookdownproj.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
