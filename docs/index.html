<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Statistics</title>
  <meta name="description" content="Statistics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistics" />
  
  
  

<meta name="author" content="Samuel Mak" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="the-pool-of-tears.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a >Section</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="2" data-path="the-pool-of-tears.html"><a href="the-pool-of-tears.html"><i class="fa fa-check"></i><b>2</b> The pool of tears</a></li>
<li class="chapter" data-level="3" data-path="a-caucus-race-and-a-long-tale.html"><a href="a-caucus-race-and-a-long-tale.html"><i class="fa fa-check"></i><b>3</b> A caucus-race and a long tale</a></li>
<li class="chapter" data-level="4" data-path="a-very-statistical-day.html"><a href="a-very-statistical-day.html"><i class="fa fa-check"></i><b>4</b> A very statistical day</a></li>
<li class="chapter" data-level="5" data-path="derivations.html"><a href="derivations.html"><i class="fa fa-check"></i><b>5</b> Derivations</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Statistics</h1>
<p class="author"><em>Samuel Mak</em></p>
<p class="date"><em>Last Updated: 2022-09-11 02:54:22</em></p>
</div>
<div id="the-linear-regression-model" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> The Linear Regression Model<a href="index.html#the-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Mathematical model</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>A model that describes the relationship between variables with a mathematical function/expression</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Linear Model</strong>
</p>
<ul>
<li><strong>Description</strong>
<ul>
<li>A model that describes the relationship between variables with the linear function</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y = mx + c\)</span></li>
</ul></li>
<li><strong>Assumption of the linear model</strong>
<ul>
<li><strong>Linearity</strong> - The relationship between the variables of interest is linear or best described by a linear function</li>
<li><strong>Additivity</strong> - The relationship between the variables of interest is best described as a linear combination</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of estimating the relationship between variables of interest</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Linear Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of estimating the relationship between variables of interest with a linear model (fitting a straight line to the data)</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Curvilinear Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of estimating the relationship between variables of interest with a curvilinear model (fitting a curve to the data)</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Simple Linear Regression</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Linear regression with one predictor</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><strong>Population Model</strong>
<ul>
<li><span class="math inline">\(y_{i} = \beta_{0} + \beta_{1}X_{i,1} + \epsilon_{i}\)</span>
<ul>
<li><strong>Where</strong>
<ul>
<li><span class="math inline">\(y_{i}\)</span>
<ul>
<li>The value of observation <em>i</em></li>
</ul></li>
<li><span class="math inline">\(\beta_{p}\)</span>
<ul>
<li>Regression coefficients/parameters</li>
</ul></li>
<li><span class="math inline">\(\beta_{0}\)</span>
<ul>
<li>The intercept</li>
<li>The predicted value of the outcome when the value of predictor 1 (<span class="math inline">\(X_{1}\)</span>) is 0</li>
</ul></li>
<li><span class="math inline">\(\beta_{1}\)</span>
<ul>
<li>Regression coefficient/weight for predictor 1</li>
<li>Represents the magnitude and direction of the relationship between predictor 1 and the outcome as the change in the value of the outcome variable for every unit change in the value of predictor 1 (it is the gradient/slope of the regression line)</li>
</ul></li>
<li><span class="math inline">\(X_{i,1}\)</span>
<ul>
<li>The value of predictor 1 for observation <em>i</em></li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}\)</span>
<ul>
<li>Error for observation <em>i</em></li>
<li>Error - The absolute deviation/distance/difference between the observed value and the expected/predicted/fitted value for observation <em>i</em></li>
<li>This quantifies the error in prediction at the observation level</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Sample Model</strong>
<ul>
<li><span class="math inline">\(y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i,1} + e_{i}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><strong>Population Model</strong>
<ul>
<li><span class="math inline">\(\begin{aligned}\mathbf{Y} &amp;= \mathbf{X}\boldsymbol{\beta} + \mathbf{E} \\ \begin{bmatrix} y_{i} \\ y_{i} \\ y_{3} \\ \vdots \\ y_{n}\end{bmatrix} &amp;= \begin{bmatrix} 1 &amp; x_{1} \\ 1 &amp; x_{2} \\ 1 &amp; x_{3} \\ \vdots &amp; \vdots \\ 1 &amp; x_{n}\end{bmatrix} \begin{bmatrix} \beta_{0} \\ \beta_{1} \end{bmatrix} + \begin{bmatrix} \epsilon_{1} \\ \epsilon_{2} \\ \epsilon_{3} \\ \vdots \\ \epsilon_{n}\end{bmatrix}\end{aligned}\)</span>
<ul>
<li><strong>Where</strong>
<ul>
<li><span class="math inline">\(\mathbf{Y}\)</span>
<ul>
<li>A column vector that contains the values for the outcome variable for each of the observations</li>
</ul></li>
<li><span class="math inline">\(\mathbf{X}\)</span>
<ul>
<li>The model matrix/design matrix</li>
<li>The model matrix contains
<ul>
<li>Column vector <span class="math inline">\(\mathbf{U}\)</span>
<ul>
<li>A column vector of 1s one for each observation. The column of 1s corresponds to <span class="math inline">\(\beta_{0}\)</span> in the <span class="math inline">\(\boldsymbol{\beta}\)</span> matrix, which means that the intercept is the same for every observation.</li>
</ul></li>
<li>Column vector of <em>x</em> values
<ul>
<li>A column vector that contains the values of predictor variable 1 for each of the observations</li>
</ul></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span>
<ul>
<li>A column vector that contains the regression coefficient/weight for each of regression parameters</li>
</ul></li>
<li><span class="math inline">\(\mathbf{E}\)</span>
<ul>
<li>A column vector that contains the error of each of the observations</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Methods of Estimation</strong>
</p>
<ul>
<li>Ordinary Least Squares (OLS)</li>
<li>Generalised Least Squares (GLS)</li>
<li>Maximum Likelihood (ML)</li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Ordinary Least Squares</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Estimates the regression line by fitting a line that minimises the model sum of squared residuals (the sum of the squared difference/distance/deviation between each of the observed values and the corresponding predicted/fitted values)</li>
<li><span class="math inline">\(\min{\sum_{i=1}^{n}{e^{2}_{i}}} = \min{\sum_{i=1}^{n}({y_{i} - \hat\beta_{0} - \hat\beta_{1}x_{i,1}}})^2\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\hat\beta_{0} = \bar{y} - \hat\beta_{1}\bar{x}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})y_{i}}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}\)</span>
<ul>
<li>Note
<ul>
<li>I think these two equations are rather boring, looking at how they are derived is much more interesting</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Derivation</strong>
<ul>
<li>Start off with the linear model with the outcome variable as a function of the weights
<ul>
<li><span class="math inline">\(y_i = \hat\beta_0 + \hat\beta_{1}x_{i,1} + e_i\)</span></li>
</ul></li>
<li>Make the residual as the subject (express the residual as a function of the weights)
<ul>
<li><span class="math inline">\(e_i = y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1}\)</span></li>
</ul></li>
<li>Square both sides
<ul>
<li><span class="math inline">\(e_{i}^{2} = (y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2\)</span></li>
</ul></li>
<li>Sum both sides
<ul>
<li><span class="math inline">\(\begin{aligned}\sum_{i=1}^{n}{e_{i}^{2}} &amp;= \sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2} \\ S(\beta_0,\beta_1) &amp;= \sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})^2}\end{aligned}\)</span></li>
<li><img src="image/ols_1.png" style="width:30.0%" /></li>
</ul></li>
<li>Find the values of <span class="math inline">\(\hat\beta_{0}\)</span> and <span class="math inline">\(\hat\beta_{1}\)</span> that minimises <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span>
<ul>
<li>There is a point at which the sum of squared residuals is at the minimum</li>
<li>The <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span> function is a quadratic and it has a minimum that represents the minimal sum of squared residuals</li>
<li>The coordinates of the minimum are the values of <span class="math inline">\(\hat\beta_{0}\)</span> and <span class="math inline">\(\hat\beta_{1}\)</span> that give you the least sum of squared residuals (or minimises <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span>)</li>
<li>Hence, find the values of the coordinate of the minimum of the quadratic function</li>
<li>To find each of the values of the coordinate of the minimum of the function <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span>, the partial derivatives of the function <span class="math inline">\(S(\hat\beta_{0}, \hat\beta_{1})\)</span> with respect to each of the <span class="math inline">\(\hat\beta\)</span>s are set to 0 (because the minimum has a slope of 0)</li>
<li>Expression for <span class="math inline">\(\hat\beta_0\)</span>
<ul>
<li><span class="math inline">\(\frac{\partial{S}}{\partial{\hat\beta_{0}}}=-2\sum_{i=1}^{n}{(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})}\)</span></li>
</ul></li>
<li>Expression for <span class="math inline">\(\hat\beta_1\)</span>
<ul>
<li><span class="math inline">\(\frac{\partial{S}}{\partial{\hat\beta_{1}}}=-2\sum_{i=1}^{n}{x_{i,1}(y_i - \hat\beta_{0} - \hat\beta_{1}x_{i,1})}\)</span></li>
</ul></li>
<li>Solve the above simultaneous equations</li>
<li>Solving the above simultaneous equations would result in
<ul>
<li><span class="math inline">\(\hat\beta_{0} = \frac{\sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{y_{i}} - \sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{x_{i,1}y_{i}}}{n\sum_{i=1}^{n}{x_{i,1}^2}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{n\sum_{i=1}^{n}{x_{i,1}y_i} - \sum_{i=1}^{n}{x_{i,1}}\sum_{i=1}^{n}{y_{i}}}{n\sum_{i=1}^{n}{x_{i,1}^2}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
</ul></li>
<li>Solving the simultaneous equations would also result in
<ul>
<li>For <span class="math inline">\(\hat\beta_{0}\)</span>
<ul>
<li><span class="math inline">\(\hat\beta_{0} = \bar{y} - \hat\beta_{1}\bar{x}\)</span></li>
</ul></li>
<li>For <span class="math inline">\(\hat\beta_{1}\)</span>
<ul>
<li><span class="math inline">\(\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})(y_{i} - \bar{y})}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})y_{i}}}{\sum_{i=n}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{Cov(x,y)}{s_{x}^{2}}\)</span></li>
<li><span class="math inline">\(\hat\beta_{1} = \frac{S_{xx}}{S_{xx}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Mathematics (Matrix)</strong>
<ul>
<li><span class="math inline">\(\boldsymbol{\beta} = (\mathbf{X&#39;X)^{-1}X&#39;Y}\)</span></li>
</ul></li>
<li><strong>Derivation (Matrix)</strong>
<ul>
<li><span class="math inline">\(S(\boldsymbol{\hat\beta}) = ||\mathbf{u}||^2 = ||\mathbf{Y-\hat{Y}}||^2 = ||\mathbf{Y-X\boldsymbol{\beta}}||^2\)</span></li>
<li><em>note</em> - Matrix derivation not complete</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Variance of <span class="math inline">\(\beta\)</span></strong>
</p>
<ul>
<li>Variance of <span class="math inline">\(\beta\)</span>
<ul>
<li><strong>Variance of <span class="math inline">\(\beta_{0}\)</span></strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\text{Var}(\hat\beta_{0}) = \sigma_{\hat\beta_{0}}^{2} = \frac{\sigma^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\text{SE}_{\hat\beta_{0}} = \sigma_{\hat\beta_{0}} = \sqrt{\frac{\sigma^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}}\)</span></li>
</ul></li>
<li><strong>Estimation from the sample</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li><span class="math inline">\(\sigma^{2}\)</span> is unknown</li>
<li>Since <span class="math inline">\(\sigma^{2}\)</span> is the variance of the population model (the average model sum of squared errors), it is reasonable to use the variance of the sample model (<span class="math inline">\(s^2\)</span>) as the estimator</li>
<li>It has been shown that <span class="math inline">\(s^2\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^{2}\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\hat{\text{Var}(\hat\beta_{0})} = \hat\sigma_{\hat\beta_{0}}^{2} = \frac{s^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\hat{\text{SE}}_{\hat\beta_{0}} = \hat\sigma_{\hat\beta_{0}} = \sqrt{\frac{s^{2}\sum_{i=1}^{n}{x_{i,1}^2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}-(\sum_{i=1}^{n}{x_{i,1}})^2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Variance of <span class="math inline">\(\beta_{1}\)</span></strong>
<ul>
<li><strong>Population</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\sigma_{\hat\beta_{1}^{2}} = \frac{n\sigma^{2}}{n\sum_{i=1}^{n}{x^{2}_{i,1}}- (\sum_{i=1}^{n}{x_{i,1}})^2}\)</span></li>
<li><span class="math inline">\(\text{Var}(\hat\beta_{1}) = \sigma_{\hat\beta_{1}^{2}} = \frac{n\sigma^{2}}{n\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\text{Var}(\hat\beta_{1}) = \sigma_{\hat\beta_{1}}^{2} = \frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\text{SE}_{\hat\beta_{1}} = \sigma_{\hat\beta_{1}} = \sqrt{\frac{\sigma^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Estimation from the sample</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\hat{\text{Var}(\hat\beta_{1})} = \hat\sigma_{\hat\beta_{1}}^{2} = \frac{s^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}\)</span></li>
<li><span class="math inline">\(\hat{\text{SE}}_{\hat\beta_{1}} = \hat\sigma_{\hat\beta_{1}} = \sqrt{\frac{s^{2}}{\sum_{i=1}^{n}{(x_{i,1} - \bar{x})^2}}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Assumptions of the Linear Models</strong>
</p>
<ul>
<li>Linearity</li>
<li>Additivity</li>
<li>Normality of the sampling distribution</li>
<li>Uncorrelated errors</li>
<li>Heteroscedasticity</li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Linearity</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The relationship between each of the predictor variables and the outcome is linear (or best described as linear)</li>
<li>This is the most important assumption of linear models because even if all other assumptions are met, the model is invalid because the description of the process you want to model is wrong</li>
</ul></li>
<li><strong>Treatment</strong>
<ul>
<li>Transformation</li>
<li>Segmented regression</li>
<li>Fit non-linear models</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Additivity</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The combined effect of multiple predictors in a multiple regression is best described as a linear combination of the predictors</li>
<li>This, along with linearity, is the most important assumptions of linear models</li>
</ul></li>
<li><strong>Treatment</strong>
<ul>
<li>Transformation</li>
<li>Segmented regression</li>
<li>Fit non-linear models</li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Uncorrelated Errors</strong>
</p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The errors in the population model are uncorrelated with each other</li>
<li>In other words, there is no autocorrelation/serial correlation</li>
<li>The distribution of errors at given value of x is uncorrelated with the distribution of errors at another value of x</li>
<li>It is important to note that the errors are not individual errors, they are theoretical distribution of errors given x, in other words, for each vale of x, there is a distribution of errors, and the observed residuals given x you have are sampled from the distribution of errors given the same x (see <a href="https://stats.stackexchange.com/questions/103439/how-to-calculate-the-covariance-between-two-observations-of-the-same-variable">this short description on CrossValidated</a>)</li>
</ul></li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>

<a href="the-pool-of-tears.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/samuel-mak/statistics/edit/master/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/samuel-mak/statistics/blob/master/index.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
