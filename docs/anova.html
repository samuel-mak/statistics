<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 ANOVA | Statistics</title>
  <meta name="description" content="Chapter 4 ANOVA | Statistics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 ANOVA | Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 ANOVA | Statistics" />
  
  
  

<meta name="author" content="Samuel Mak" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mathematics.html"/>
<link rel="next" href="derivation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a >Section</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="2" data-path="generalised-least-squares.html"><a href="generalised-least-squares.html"><i class="fa fa-check"></i><b>2</b> Generalised Least Squares</a></li>
<li class="chapter" data-level="3" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>3</b> Mathematics</a></li>
<li class="chapter" data-level="4" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>4</b> ANOVA</a></li>
<li class="chapter" data-level="5" data-path="derivation.html"><a href="derivation.html"><i class="fa fa-check"></i><b>5</b> Derivation</a></li>
<li class="chapter" data-level="6" data-path="multivariate-analysis.html"><a href="multivariate-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Analysis</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="anova" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> ANOVA<a href="anova.html#anova" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>ANOVA</strong>
</p>
<ul>
<li><strong>Analysis of Variance (ANOVA)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Under the null hypothesis, it assumes that each of the groups is an independent random sample from the population</li>
<li>ANOVA involves assessing the agreement between two estimators of the population variance</li>
</ul></li>
</ul></li>
<li><strong>The Statistical Model of ANOVA</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The value of observation <span class="math inline">\(i\)</span> on outcome variable <span class="math inline">\(y\)</span> is the grand mean plus the effect of the grouping variable it is in plus random error</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{i,k} = \mu + \tau_{k} + \epsilon_{i, k}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y_{i,k}\)</span> - The value of the outcome variable <span class="math inline">\(y\)</span> of observation <span class="math inline">\(i\)</span> in group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\mu\)</span> - The overall/grand mean of the outcome variable <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(\tau_{k}\)</span>
<ul>
<li>The nonrandom effect of the group</li>
<li>The difference between the mean of group <span class="math inline">\(k\)</span> and the grand mean (<span class="math inline">\(\mu\)</span>)</li>
<li>Since they are normalised, <span class="math inline">\(\sum_{k=1}^{k}{\tau_k} = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(\mu + \beta_{k}\)</span> - The expected value (predicted value) of observation <span class="math inline">\(y_{i,k}\)</span></li>
<li><span class="math inline">\(\epsilon_{i,k}\)</span>
<ul>
<li>Random error</li>
<li>The error term is the difference between an observed value (<span class="math inline">\(y_{i,k}\)</span>) and its expected value (<span class="math inline">\(\mu + \beta_{k}\)</span>) (or <span class="math inline">\(\epsilon_{i,k} = y_{i,k} - \mu - \tau_{k}\)</span>)</li>
<li>Assuming that <span class="math inline">\(y_{ik}\)</span> is a random independent normal variable with <span class="math inline">\(E(y_{ik}) = 0\)</span> and <span class="math inline">\(Var(y_{ik}) = 0\)</span>, since the error term is the difference between <span class="math inline">\(y_{i,k}\)</span>, a normally distributed random variable with variance of <span class="math inline">\(\sigma^2\)</span>, and its mean, this follows that the errors are also a random independent normal variable with <span class="math inline">\(E(\epsilon_{ik}) = 0\)</span> (because subtracting each of the errors by its mean will normalize it to 0) and <span class="math inline">\(Var(\epsilon_{i,k}) = \sigma^2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Estimating the population variance <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li><strong>Estimators of <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li><span class="math inline">\(MS_W\)</span></li>
<li><span class="math inline">\(MS_B\)</span></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_W\)</span> estimator</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(SS_W\)</span> and <span class="math inline">\(MS_W\)</span> are statistics derived from multiple independent random samples, hence, <span class="math inline">\(SS_W\)</span> and <span class="math inline">\(MS_W\)</span> are independent random variables</li>
<li>The expectation of <span class="math inline">\(MS_W\)</span> is <span class="math inline">\(\sigma^2\)</span></li>
<li>Hence, the observed <span class="math inline">\(MS_W\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_B\)</span> estimator</strong>
<ul>
<li><strong>The distribution of <span class="math inline">\(MS_B\)</span></strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(SS_B\)</span> and <span class="math inline">\(MS_B\)</span> are statistics derived from multiple independent random samples, hence, <span class="math inline">\(SS_B\)</span> and <span class="math inline">\(MS_B\)</span> are independent random variables</li>
<li><span class="math inline">\(MS_B\)</span> is an independent random variable that has a Chi-squared distribution with a mean of <span class="math inline">\(\sigma^2 + (\frac{1}{k-1})\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span> and degrees if freedom of</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle E\left( MS_B \right) = \sigma^2 + \left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span></li>
<li><strong><em>Notes</em></strong>
<ul>
<li>See proof below</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Implication of <span class="math inline">\(MS_B\)</span> as an estimator</strong>
<ul>
<li>Under the null hypothesis, that is when all <span class="math inline">\(\tau_ks = 0\)</span>, this quantity <span class="math inline">\(\left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span> is <span class="math inline">\(0\)</span>, therefore, the observed <span class="math inline">\(MS_B\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
<li>If the null hypothesis is false, that is when at least one of the <span class="math inline">\(\tau_ks â‰  0\)</span>, this quantity <span class="math inline">\(\left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span> will be larger than <span class="math inline">\(0\)</span>, hence, <span class="math inline">\(MS_B\)</span> is a positively biased estimator of <span class="math inline">\(\sigma^2\)</span> (it will tend to overestimate <span class="math inline">\(\sigma^2\)</span>)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The test statistic: The F ratio</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The null hypothesis of ANOVA (<span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3 = \cdots = \mu_k\)</span>) is tested by the extent of positive bias of the <span class="math inline">\(MS_B\)</span> estimator through comparing the <span class="math inline">\(MS_B\)</span> estimator to the <span class="math inline">\(MS_W\)</span> estimator as a ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span></li>
<li>In other words, the ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span> is the test statistic for the null hypothesis</li>
<li>This statistic is called the <span class="math inline">\(F\)</span> ratio (or <span class="math inline">\(F\)</span> statistic) after Fisher</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = \frac{MS_B}{MS_W}\)</span></li>
</ul></li>
<li><strong>Interpretation</strong>
<ul>
<li>If there are no or little positive bias of <span class="math inline">\(MS_B\)</span>, the <span class="math inline">\(F\)</span> statistic is close to 1</li>
<li>The greater the positive bias of <span class="math inline">\(MS_B\)</span>, the larger the <span class="math inline">\(F\)</span> statistic is above 1</li>
</ul></li>
<li><strong>The distribution of the F-statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Since <span class="math inline">\(MS_B\)</span> is <span class="math inline">\(SS_B\)</span> (a chi-squared-distributed independent random variable) divided by its degrees of freedom, and <span class="math inline">\(MS_W\)</span> is <span class="math inline">\(SS_W\)</span> (another chi-squared-distributed random variable) divided by its degrees of freedom, the ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span> has a Central <span class="math inline">\(F\)</span> distribution, that is an <span class="math inline">\(F\)</span> distribution with a degrees of freedom for the numerator of <span class="math inline">\(v_1 = k-1\)</span> and a degrees of freedom for the denominator of <span class="math inline">\(v_2 = N - k\)</span> for the denominator</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F_{central} \sim F\left(k - 1, N - k \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The F test</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of assessing the probability of an F statistic as extreme as or more extreme than the observed F statistic in the Central F distribution</li>
</ul></li>
</ul></li>
<li><strong>NHST for ANOVA</strong>
<ul>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>There are no difference between the means</li>
<li>The samples are from the same population</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3 = \cdots = \mu_k\)</span> or <span class="math inline">\((\tau_k - \bar{\tau})^2 = 0\)</span></li>
</ul></li>
</ul></li>
<li><strong>Alternative hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>At least one of the samples is likely from another population</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(H_1: \mu_1 â‰  \mu_2 â‰  \mu_3 â‰  \cdots â‰  \mu_k\)</span> or <span class="math inline">\((\tau_k - \bar{\tau})^2 &gt; 0\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>NHST</strong>
<ul>
<li>The null hypothesis is tested by an upper-tailed <span class="math inline">\(F\)</span> test</li>
<li>The null hypothesis is tested by assessing the probability of getting an F-statistic equal to or larger than the observed <span class="math inline">\(F\)</span> statistic in the Central <span class="math inline">\(F\)</span> distribution</li>
<li>The F test for 2 samples is equivalent to the t test</li>
</ul></li>
</ul></li>
<li><strong><span class="math inline">\(Proof of E(MS_B) = \sigma^2\)</span></strong>
<ul>
<li>$ MS_B = ( ) <em>{k=1}^{k}{n_k ({y}</em>{k} - {y} )^2}$
<ul>
<li>Since:
<ul>
<li><span class="math inline">\(\begin{aligned} \displaystyle \bar{y}_{k} &amp;= \frac{1}{n_k}\sum_{i = 1}^{n_k}{y_{ik}} \\ \displaystyle &amp;= \frac{1}{n_k}\sum_{i = 1}^{n_k}{\left( \mu + \tau_k + \epsilon_{ik} \right)} \\ \displaystyle &amp;= \mu + \tau_k + \frac{1}{n_k}\sum_{i = 1}^{n_k}{\epsilon_{ik}} \\ \displaystyle &amp;= \mu + \tau_k + \bar{\epsilon}_k \end{aligned}\)</span></li>
<li>$
<span class="math display">\[\begin{aligned} \displaystyle \bar{y} &amp;= \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{y_{ik}}} \\ \displaystyle &amp;=  \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{\left( \mu + \tau_k + \epsilon_{ik} \right)}} \\ \displaystyle &amp;=  \mu + \frac{1}{N}\sum_{k = 1}^{k}{n_k \tau_k} + \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{\epsilon_{ik}}} \\ \displaystyle &amp;= \mu + \bar{\tau} + \bar{\epsilon} \end{aligned}\]</span>
$</li>
</ul></li>
</ul></li>
<li>$ MS_B = ( ) <em>{k=1}^{k}{n_k ({y}</em>{k} - {y} )^2} \ MS_B = ( ) <em>{k=1}^{k}{n_k ^2} \ MS_B = ( ) </em>{k=1}^{k}{n_k ^2} \ MS_B = ( ) <em>{k=1}^{k}{n_k } \ MS_B = ( ) </em>{k=1}^{k}{ } \ MS_B = ( ) _{k=1}<sup>{k}{n_k(<em>k - {})^2} + ( )</em>{k=1}</sup>{k}{2n_k(_k - {})({}<em>k} - {}) + ( )</em>{k=1}^{k}{n_k({}_k - {})^2} \ E(MS_B) = ( ) E+ ( ) E+ ( ) E$
<ul>
<li>Since <span class="math inline">\(\tau_k\)</span> and thus <span class="math inline">\(\bar{\tau}\)</span> are constants and <span class="math inline">\(E(\epsilon_{ik}) = E(\bar\epsilon_{i}) = E(\bar\epsilon) = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(\displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_k - \bar{\epsilon})^2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_{k}^{2} - 2\bar{\epsilon}_{k}\bar{\epsilon} + \bar{\epsilon}^{2})}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k\bar{\epsilon}_{k}^{2} - 2n_k\bar{\epsilon}_{k}\bar{\epsilon} + n_k\bar{\epsilon}^{2}}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{(n_k\bar{\epsilon}_{k}^{2})} - 2N\bar{\epsilon}^{2} + N\bar{\epsilon}^{2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{(n_k\bar{\epsilon}_{k}^{2})} - N\bar{\epsilon}^{2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{n_k E(\bar{\epsilon}_{k}^{2})} - NE(\bar{\epsilon}^{2})\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{n_k \frac{\sigma^2}{n_k}} - N\frac{\sigma^2}{N}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{\sigma^2} - \sigma^2\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) (k{\sigma^2} - \sigma^2) \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) (k-1)\sigma^2 \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \sigma^2 \\ \displaystyle E(MS_B) = \sigma^2 + \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Other stuff</strong>
</p>
<ul>
<li><strong>The T and F</strong>
<ul>
<li>The squared independent random variable with a t-distribution with <span class="math inline">\(v\)</span> degree of freedom has an F distribution with degrees of freedom 1 of 1 and degree of freedom 2 of <span class="math inline">\(v\)</span></li>
<li><span class="math inline">\(t^2 = F\)</span></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>The F statistic</strong>
</p>
<ul>
<li><strong>The F statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The <span class="math inline">\(F\)</span> statistic is the ratio of a independent random variable with a Chi-squared distribution divided by its degrees of freedom to another independent random variable with a Chi-squared distribution divided by its degrees of freedom</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = \frac{\frac{S_1}{df_1}}{\frac{S_2}{df_2}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(S_1\)</span> is an independent random variable with a Chi-squared distribution with degrees of freedom of <span class="math inline">\(df_1\)</span></li>
<li><span class="math inline">\(S_2\)</span> is another independent random variable with a Chi-squared distribution with degrees of freedom of <span class="math inline">\(df_2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The F-distribution: The distribution of the F statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The <span class="math inline">\(F\)</span> statistic has an <span class="math inline">\(F\)</span> distribution</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li>I am not including the PDF maths here</li>
</ul></li>
</ul></li>
<li><strong>Null hypothesis</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(H_0: \tau_1 = \tau_2 = \tau_3 = \cdots = \tau_k = 0\)</span> (<span class="math inline">\(\bar{\tau} = 0\)</span>)</li>
</ul></li>
</ul></li>
<li><strong>The Central <span class="math inline">\(F\)</span> Distribution</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The Central F is the ratio of 2 independent chi-squared random variables divided by their degrees of freedom</li>
</ul></li>
</ul></li>
<li><strong>Sum of Squared Deviations</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka Sum of Squares</li>
<li>A measure of variation</li>
<li>The Sum of Squared Deviation is the sum of all the squared deviations where a deviation is the difference between an observed value and its expected value</li>
<li>It is the standardized version of the variance</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle SS = \sum_{i=1}^{n}{\left( y_i - E(y)\right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> is a variable</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Operationalising variability</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>ANOVA partitions the Total Sum of Squares of the outcome variable into 2 components:
<ul>
<li>The Model Sum of Squares</li>
<li>The Error Sum of Squares</li>
</ul></li>
</ul></li>
<li><strong>Total Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The Sum of Squares that represents the total amount of variation in the outcome variable</li>
<li>It is the sum of all the squared deviations between each observed value and the grand mean</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SST.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} = {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} \end{alignat*}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> represents group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(i\)</span> represents observation <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(y_{i,k}\)</span> - Observation <span class="math inline">\(i\)</span> in group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\bar{y}\)</span> - The grand mean</li>
</ul></li>
<li><strong><em>Notes</em></strong>
<ul>
<li>Alternatively, it can be expressed simply as:
<ul>
<li><span class="math inline">\(\displaystyle \sum_{n=1}^{n}{(y_i - \bar{y})^2}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Total degrees of freedom</strong>
<ul>
<li><span class="math inline">\(\begin{aligned}df_T &amp;= df_M + df_E \\ df_T &amp;= N-1 \end{aligned}\)</span></li>
</ul></li>
<li><strong>Mean Total Sum of Squares (Total Variance)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Total Variance in the outcome variable</li>
<li>The mean amount of variation in the outcome variable</li>
<li>The average amount by squared deviation between each observation and the grand mean</li>
<li>It is the standardised version of the Total Sum of Squares
<ul>
<li>Because the Total Sum of Squares is the sum of all the squared deviations, it depends on the degrees of freedom. Finding the mean, specifically, dividing the Total Sum of Squares by the degrees of freedom will standardise the quantity</li>
</ul></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_T = \frac{SS_T}{df_T}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Model Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Represents the amount of variation in the outcome variable that is explained by the model or attributable to the predictors in the model (systematic variation)</li>
<li>It is the sum of all the squared deviations between the predicted value of each observation (which is the mean of the group to which the observation belongs) and the grand mean</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SSM.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\({SS_{M}} = {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(n_k\)</span> - Group size</li>
<li><span class="math inline">\(\bar{y}_k\)</span> - The mean of group $k$0</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Model degrees of freedom</strong>
<ul>
<li><span class="math inline">\(df_M = k -1\)</span></li>
</ul></li>
<li><strong>Mean Model Sum of Squares (Variance Explained)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Variance in the outcome variable explained by the model</li>
<li>The mean amount of variation in the outcome variable explained by the model</li>
<li>The average amount of squared deviation between the predicted value of each observation and the grand mean</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_M = \frac{SS_M}{df_M}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Error Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It represents the amount of variation in the outcome variable that is not explained by the model or attributable to the predictors in the model and is assumed to be due to random error</li>
<li>It is the sum of all the squared deviations between each observed value and the mean of the group to which it belongs</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SSR.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\({SS_{E}} = {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2}}\)</span>
<ul>
<li><strong><em>Notes</em></strong>
<ul>
<li>Alternatively, it is the sum of the variance of each group times the degrees of freedom of that group (so you get the error sum of squares of that group)
<ul>
<li><span class="math inline">\(\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Degrees of freedom</strong>
<ul>
<li><span class="math inline">\(df_e = N-k\)</span></li>
</ul></li>
<li><strong>Mean Error Sum of Squares (Variance Unexplained/Error Variance)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Variance in the outcome variable not explained by the model and is assumed to be due to random error</li>
<li>The mean amount of variation in the outcome variable not explained by the model</li>
<li>The average amount of squared deviation between the predicted value of each observation and its predicted value</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_E = \frac{SS_E}{df_e}\)</span>
<ul>
<li>Alternatively, it is just the error variance
<ul>
<li><span class="math inline">\(\displaystyle MS_E = s^2 = \frac{1}{n-k}\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The Model as a whole</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} &amp;= {SS_{M}} + {SS_{E}} \\ {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} &amp;= {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2} + {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2} }} \end{alignat*}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>ANOVA</strong>
</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="mathematics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="derivation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/samuel-mak/statistics/edit/master/04-Linear_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/samuel-mak/statistics/blob/master/04-Linear_Regression.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub", "bookdownproj.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
