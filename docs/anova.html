<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 ANOVA | Statistics</title>
  <meta name="description" content="Chapter 4 ANOVA | Statistics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 ANOVA | Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 ANOVA | Statistics" />
  
  
  

<meta name="author" content="Samuel Mak" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mathematics.html"/>
<link rel="next" href="derivation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a >Section</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="2" data-path="generalised-least-squares.html"><a href="generalised-least-squares.html"><i class="fa fa-check"></i><b>2</b> Generalised Least Squares</a></li>
<li class="chapter" data-level="3" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>3</b> Mathematics</a></li>
<li class="chapter" data-level="4" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>4</b> ANOVA</a></li>
<li class="chapter" data-level="5" data-path="derivation.html"><a href="derivation.html"><i class="fa fa-check"></i><b>5</b> Derivation</a></li>
<li class="chapter" data-level="6" data-path="multivariate-analysis.html"><a href="multivariate-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Analysis</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="anova" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> ANOVA<a href="anova.html#anova" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>ANOVA</strong>
</p>
<ul>
<li><strong>Analysis of Variance (ANOVA)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Under the null hypothesis, it assumes that each of the groups is an independent random sample from the population</li>
<li>ANOVA involves assessing the agreement between two estimators of the population variance</li>
</ul></li>
</ul></li>
<li><strong>The Statistical Model of ANOVA</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The value of observation <span class="math inline">\(i\)</span> on outcome variable <span class="math inline">\(y\)</span> is the grand mean plus the effect of the grouping variable it is in plus random error</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{i,k} = \mu + \tau_{k} + \epsilon_{i, k}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y_{i,k}\)</span> - The value of the outcome variable <span class="math inline">\(y\)</span> of observation <span class="math inline">\(i\)</span> in population <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\mu\)</span> - The overall/grand mean of the outcome variable <span class="math inline">\(y\)</span> (the mean of the superpopulation)</li>
<li><span class="math inline">\(\tau_{k}\)</span>
<ul>
<li>The deviation of population <span class="math inline">\(k\)</span> from the overall mean (<span class="math inline">\(\mu\)</span>)</li>
<li>The nonrandom effect of the group</li>
<li>Group or sample <span class="math inline">\(k\)</span> is assumed to be sampled from this population <span class="math inline">\(k\)</span></li>
<li>Since they are normalised, <span class="math inline">\(\sum_{k=1}^{k}{\tau_k} = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(\mu + \tau_{k}\)</span>
<ul>
<li>The mean of population <span class="math inline">\(k\)</span></li>
<li>The expected value of observation <span class="math inline">\(i\)</span> in population <span class="math inline">\(k\)</span> (<span class="math inline">\(y_{i,k}\)</span>)</li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i,k}\)</span>
<ul>
<li>The random error for observation <span class="math inline">\(i\)</span> in population <span class="math inline">\(k\)</span></li>
<li>The difference between an observed value (<span class="math inline">\(y_{i,k}\)</span>) and its expected value (<span class="math inline">\(\mu + \tau_{k}\)</span>)</li>
<li>Assuming that <span class="math inline">\(y_{ik}\)</span> is a random independent normal variable with <span class="math inline">\(E(y_{ik}) = 0\)</span> and <span class="math inline">\(Var(y_{ik}) = 0\)</span>, since the error term is the difference between <span class="math inline">\(y_{i,k}\)</span>, a normally distributed random variable with variance of <span class="math inline">\(\sigma^2\)</span>, and its mean, this follows that the errors are also a random independent normal variable with <span class="math inline">\(E(\epsilon_{ik}) = 0\)</span> (because subtracting each of the errors by its mean will normalize it to 0) and <span class="math inline">\(Var(\epsilon_{i,k}) = \sigma^2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hypothesis Testing for ANOVA</strong>
<ul>
<li><strong>Conceptual Hypotheses in ANOVA</strong>
<ul>
<li><strong>Null hypothesis</strong>
<ul>
<li><span class="math inline">\(H_0:\)</span> All populations are the same (are from the same superpopulation)</li>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 = \cdots = \mu_k\)</span></li>
</ul></li>
<li><strong>Alternative hypothesis</strong>
<ul>
<li><span class="math inline">\(H_1:\)</span> The populations are different</li>
<li><span class="math inline">\(H_0: \mu_1 ≠ \mu_2 ≠ \cdots ≠ \mu_k 0\)</span></li>
</ul></li>
</ul></li>
<li><strong>Operationalising the null hypothesis to be tested</strong>
<ul>
<li><strong>General Concept</strong>
<ul>
<li>The operationalisation of the ANOVA null hypothesis is based on the agreement/disaggreement between 2 estimators of the variance of the superpopulation (<span class="math inline">\(\sigma^@\)</span>) (or it’s called common variance)</li>
</ul></li>
<li><strong>Estimating the population variance <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li><strong>There are 2 estimators of <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li><span class="math inline">\(MS_W\)</span></li>
<li><span class="math inline">\(MS_B\)</span></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_W\)</span> estimator</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(SS_W\)</span>, and thus also <span class="math inline">\(MS_W\)</span>, are statistics derived from multiple independent random samples, hence, <span class="math inline">\(SS_W\)</span>, and also <span class="math inline">\(MS_W\)</span>, are independent random variables</li>
<li>And the expectation of <span class="math inline">\(MS_W\)</span> is <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(E(MS_W) = \sigma^{2}\)</span></li>
</ul></li>
<li><strong>Implications for <span class="math inline">\(MS_W\)</span> as an estimator of <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li>Hence, the observed <span class="math inline">\(MS_W\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_B\)</span> estimator</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(SS_B\)</span> and <span class="math inline">\(MS_B\)</span> are statistics derived from multiple independent random samples, hence, <span class="math inline">\(SS_B\)</span> and <span class="math inline">\(MS_B\)</span> are independent random variables</li>
<li><span class="math inline">\(MS_B\)</span> is an independent random variable that has a Chi-squared distribution with an expectation of <span class="math inline">\(\sigma^2 + (\frac{1}{k-1})\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span><br />
</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle E\left( MS_B \right) = \sigma^2 + \left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span>
<ul>
<li><strong><em>Notes</em></strong>
<ul>
<li>See proof below</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Implication for <span class="math inline">\(MS_B\)</span> as an estimator of <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li>Under the null hypothesis, that is when all <span class="math inline">\(\tau_ks = 0\)</span>, this linear combination <span class="math inline">\(\left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span> is <span class="math inline">\(0\)</span>, therefore, <span class="math inline">\(\displaystyle E\left( MS_B \right) = \sigma^2 + 0\)</span>, which means that the observed <span class="math inline">\(MS_B\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
<li>If the null hypothesis is false, that is when at least one of the <span class="math inline">\(\tau_ks ≠ 0\)</span>, the linear combination of <span class="math inline">\(\tau_ks\)</span> (<span class="math inline">\(\left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span>) will be larger than <span class="math inline">\(0\)</span>, hence, <span class="math inline">\(MS_B\)</span> is a positively biased estimator of <span class="math inline">\(\sigma^2\)</span> (it will tend to overestimate <span class="math inline">\(\sigma^2\)</span>)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The test statistic: The F ratio</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The conceptual null hypothesis of ANOVA (<span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3 = \cdots = \mu_k\)</span>) is formally operationalised as <span class="math inline">\(H_0: \sum_{k=1}^{k}{\tau_k^2} = 0\)</span> and this is tested by comparing the <span class="math inline">\(MS_B\)</span> estimator to the <span class="math inline">\(MS_W\)</span> estimator as a ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span></li>
<li>This ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span> is the test statistic for the null hypothesis of ANOVA and this test statistic is called the <span class="math inline">\(F\)</span> ratio (or <span class="math inline">\(F\)</span> statistic)<br />
</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = \frac{MS_B}{MS_W}\)</span></li>
</ul></li>
<li><strong>Interpretation</strong>
<ul>
<li>If the null hypothesis is true, that is <span class="math inline">\(\sum_{k=1}^{k}{\tau_k^2} = 0\)</span>, the expected value of <span class="math inline">\(E(MS_B)\)</span> is just the random variation in the superpopulation (<span class="math inline">\(E(MS_B) = \sigma^2\)</span>), and <span class="math inline">\(F = \frac{E(MS_B)}{E(MS_W)} = \frac{\sigma^2}{\sigma^2} = 1\)</span>. Using the sample <span class="math inline">\(MS_B\)</span> and <span class="math inline">\(MS_W\)</span> as estimators of <span class="math inline">\(E(MS_B)\)</span> and <span class="math inline">\(E(MS_B)\)</span> respectively would yield an <span class="math inline">\(F\)</span> statistic that is close to 1</li>
<li>If the null hypothesis is false, that is when at least one of the <span class="math inline">\(\tau_ks &gt; 0\)</span> (<span class="math inline">\(\sum_{k=1}^{k}{\tau_k^2} &gt; 0\)</span>), in other words, when at least one of the populations is different, the expected value <span class="math inline">\(E(MS_B)\)</span> consists of both the random variation in the superpopulation and the overall variation induced by the groups (<span class="math inline">\(E(MS_B) = \sigma^2 + \left(\frac{1}{k-1} \right)\sum_{k=1}^{k}{n_k\tau_k^2}\)</span>), then <span class="math inline">\(F = \frac{E(MS_B)}{E(MS_W)} = \frac{\sigma^2 + \left(\frac{1}{k-1} \right)\sum_{k=1}^{k}{n_k\tau_k^2}}{\sigma^2}\)</span>, <em>where</em> <span class="math inline">\(\left(\frac{1}{k-1} \right)\sum_{k=1}^{k}{n_k\tau_k^2} &gt; 0\)</span>, the <span class="math inline">\(F\)</span> statistic will be larger than 1</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>NHST</strong>
<ul>
<li>The null hypothesis of ANOVA is thus tested by an Upper-Tail <span class="math inline">\(F\)</span> test</li>
</ul></li>
</ul></li>
<li><strong>The distribution of the F-statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Since <span class="math inline">\(MS_B\)</span> is <span class="math inline">\(SS_B\)</span> (a chi-squared-distributed independent random variable) divided by its degrees of freedom, and <span class="math inline">\(MS_W\)</span> is <span class="math inline">\(SS_W\)</span> (another chi-squared-distributed random variable) divided by its degrees of freedom, the ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span> has a Central <span class="math inline">\(F\)</span> distribution, that is an <span class="math inline">\(F\)</span> distribution with a degrees of freedom for the numerator of <span class="math inline">\(v_1 = k-1\)</span> and a degrees of freedom for the denominator of <span class="math inline">\(v_2 = N - k\)</span> for the denominator</li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F_{central} \sim F\left(k - 1, N - k \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The F test</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of assessing the probability of an F statistic as extreme as or more extreme than the observed F statistic in the Central F distribution</li>
</ul></li>
</ul></li>
<li><strong>NHST for ANOVA</strong>
<ul>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>There are no difference between the means</li>
<li>The samples are from the same population</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3 = \cdots = \mu_k\)</span> or <span class="math inline">\((\tau_k - \bar{\tau})^2 = 0\)</span></li>
</ul></li>
</ul></li>
<li><strong>Alternative hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>At least one of the samples is likely from another population</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(H_1: \mu_1 ≠ \mu_2 ≠ \mu_3 ≠ \cdots ≠ \mu_k\)</span> or <span class="math inline">\((\tau_k - \bar{\tau})^2 &gt; 0\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>NHST</strong>
<ul>
<li>The null hypothesis is tested by an upper-tailed <span class="math inline">\(F\)</span> test</li>
<li>The null hypothesis is tested by assessing the probability of getting an F-statistic equal to or larger than the observed <span class="math inline">\(F\)</span> statistic in the Central <span class="math inline">\(F\)</span> distribution</li>
<li>The F test for 2 samples is equivalent to the t test</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(H_0: \sum_{k=1}^{k}{\tau_k^2} = 0\)</span> (operationally)
- <strong>Hypothesis Testing for ANOVA</strong></p>
<ul>
<li><strong>Total Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The Sum of Squares that represents the total amount of variation in the outcome variable</li>
<li>It is the sum of all the squared deviations between each observed value and the grand mean</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SST.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} = {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} \end{alignat*}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> represents group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(i\)</span> represents observation <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(y_{i,k}\)</span> - Observation <span class="math inline">\(i\)</span> in group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\bar{y}\)</span> - The grand mean</li>
</ul></li>
<li><strong><em>Notes</em></strong>
<ul>
<li>Alternatively, it can be expressed simply as:
<ul>
<li><span class="math inline">\(\displaystyle \sum_{n=1}^{n}{(y_i - \bar{y})^2}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Total degrees of freedom</strong>
<ul>
<li><span class="math inline">\(\begin{aligned}df_T &amp;= df_M + df_E \\ df_T &amp;= N-1 \end{aligned}\)</span></li>
</ul></li>
<li><strong>Mean Total Sum of Squares (Total Variance)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Total Variance in the outcome variable</li>
<li>The mean amount of variation in the outcome variable</li>
<li>The average amount by squared deviation between each observation and the grand mean</li>
<li>It is the standardised version of the Total Sum of Squares</li>
<li>Because the Total Sum of Squares is the sum of all the squared deviations, it depends on the degrees of freedom. Finding the mean, specifically, dividing the Total Sum of Squares by the degrees of freedom will standardise the quantity</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_T = \frac{SS_T}{df_T}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>ANOVA and variance partitioning</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>ANOVA partitions the Total Sum of Squares of the outcome variable into 2 components:
<ul>
<li>The Model Sum of Squares</li>
<li>The Error Sum of Squares</li>
</ul></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} &amp;= {SS_{M}} + {SS_{E}} \\ {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} &amp;= {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2} + {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2} }} \end{alignat*}\)</span></li>
</ul></li>
<li><strong>Between/Model Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The amount of variation between groups/samples/populations</li>
<li>In an experimental design, it represents the amount of variation in the outcome variable that is induced or explained by the model in the model (systematic variation)</li>
<li>It is the sum of all the squared deviations between the predicted value of each observation (which is the mean of the group to which the observation belongs) and the grand mean</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SSM.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\({SS_{B}} = {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(n_k\)</span> - Group size</li>
<li><span class="math inline">\(\bar{y}_k\)</span> - The mean of group $k$0</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Model degrees of freedom</strong>
<ul>
<li><span class="math inline">\(df_B = k - 1\)</span></li>
</ul></li>
<li><strong>Mean Model Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Between-group/sample variance - The variance between groups
<ul>
<li>In a 2-group/sample design, it is the variance between 2 groups</li>
</ul></li>
<li>Variance in the outcome variable explained by the model</li>
<li>The mean amount of variation in the outcome variable explained by the model</li>
<li>The average amount of squared deviation between the predicted value of each observation and the grand mean</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_M = \frac{SS_M}{df_M}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Error Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It represents the amount of variation in the outcome variable that is not explained by the model or attributable to the predictors in the model and is assumed to be due to random error</li>
<li>It is the sum of all the squared deviations between each observed value and the mean of the group to which it belongs</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SSR.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\({SS_{E}} = {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2}}\)</span>
<ul>
<li><strong><em>Notes</em></strong>
<ul>
<li>Alternatively, it is the sum of the variance of each group times the degrees of freedom of that group (so you get the error sum of squares of that group)
<ul>
<li><span class="math inline">\(\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Degrees of freedom</strong>
<ul>
<li><span class="math inline">\(df_e = N-k\)</span></li>
</ul></li>
<li><strong>Mean Error Sum of Squares (Variance Unexplained/Error Variance)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Variance in the outcome variable not explained by the model and is assumed to be due to random error</li>
<li>The mean amount of variation in the outcome variable not explained by the model</li>
<li>The average amount of squared deviation between the predicted value of each observation and its predicted value</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_E = \frac{SS_E}{df_e}\)</span>
<ul>
<li>Alternatively, it is just the error variance
<ul>
<li><span class="math inline">\(\displaystyle MS_E = s^2 = \frac{1}{n-k}\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The Model as a whole</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} &amp;= {SS_{M}} + {SS_{E}} \\ {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} &amp;= {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2} + {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2} }} \end{alignat*}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Proof of <span class="math inline">\(E(MS_B) = \sigma^2\)</span></strong>
<ul>
<li><span class="math inline">\(\displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left(\bar{y}_{k} - \bar{y} \right)^2}\)</span>
<ul>
<li>Since:
<ul>
<li><span class="math inline">\(\begin{aligned} \displaystyle \bar{y}_{k} &amp;= \frac{1}{n_k}\sum_{i = 1}^{n_k}{y_{ik}} \\ \displaystyle &amp;= \frac{1}{n_k}\sum_{i = 1}^{n_k}{\left( \mu + \tau_k + \epsilon_{ik} \right)} \\ \displaystyle &amp;= \mu + \tau_k + \frac{1}{n_k}\sum_{i = 1}^{n_k}{\epsilon_{ik}} \\ \displaystyle &amp;= \mu + \tau_k + \bar{\epsilon}_k \end{aligned}\)</span></li>
<li><span class="math inline">\(\begin{aligned} \displaystyle \bar{y} &amp;= \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{y_{ik}}} \\ \displaystyle &amp;= \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{\left( \mu + \tau_k + \epsilon_{ik} \right)}} \\ \displaystyle &amp;= \mu + \frac{1}{N}\sum_{k = 1}^{k}{n_k \tau_k} + \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{\epsilon_{ik}}} \\ \displaystyle &amp;= \mu + \bar{\tau} + \bar{\epsilon} \end{aligned}\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left(\bar{y}_{k} - \bar{y} \right)^2} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left[(\mu + \tau_k + \bar{\epsilon}_k) - (\mu + \bar{\tau} + \bar{\epsilon}) \right]^2} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left[(\tau_k - \bar{\tau}) + (\bar{\epsilon}_k - \bar{\epsilon}) \right]^2} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left[(\tau_k - \bar{\tau})^2 + 2(\tau_k - \bar{\tau})(\bar{\epsilon}_k - \bar{\epsilon}) + (\bar{\epsilon}_k - \bar{\epsilon})^2 \right]} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{ \left[n_k(\tau_k - \bar{\tau})^2 + 2n_k(\tau_k - \bar{\tau})(\bar{\epsilon}_k - \bar{\epsilon}) + n_k(\bar{\epsilon}_k - \bar{\epsilon})^2 \right]} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right)\sum_{k=1}^{k}{2n_k(\tau_k - \bar{\tau})(\bar{\epsilon}_k} - \bar{\epsilon}) + \left( \frac{1}{k - 1}\right)\sum_{k=1}^{k}{n_k(\bar{\epsilon}_k - \bar{\epsilon})^2} \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) E\left[\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\right] + \left( \frac{1}{k - 1}\right) E\left[\sum_{k=1}^{k}{2n_k(\tau_k - \bar{\tau})(\bar{\epsilon}_k} - \bar{\epsilon})\right] + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_k - \bar{\epsilon})^2}\right]\)</span>
<ul>
<li>Since:
<ul>
<li><span class="math inline">\(\tau_k\)</span> and thus <span class="math inline">\(\bar{\tau}\)</span> are constants and <span class="math inline">\(E(\epsilon_{ik}) = E(\bar\epsilon_{i}) = E(\bar\epsilon) = 0\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_k - \bar{\epsilon})^2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_{k}^{2} - 2\bar{\epsilon}_{k}\bar{\epsilon} + \bar{\epsilon}^{2})}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k\bar{\epsilon}_{k}^{2} - 2n_k\bar{\epsilon}_{k}\bar{\epsilon} + n_k\bar{\epsilon}^{2}}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{(n_k\bar{\epsilon}_{k}^{2})} - 2N\bar{\epsilon}^{2} + N\bar{\epsilon}^{2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{(n_k\bar{\epsilon}_{k}^{2})} - N\bar{\epsilon}^{2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{n_k E(\bar{\epsilon}_{k}^{2})} - NE(\bar{\epsilon}^{2})\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{n_k \frac{\sigma^2}{n_k}} - N\frac{\sigma^2}{N}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{\sigma^2} - \sigma^2\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) (k{\sigma^2} - \sigma^2) \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) (k-1)\sigma^2 \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \sigma^2 \\ \displaystyle E(MS_B) = \sigma^2 + \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span>
<ul>
<li>Since:
<ul>
<li><span class="math inline">\(\bar{\tau} = 0\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\displaystyle E(MS_B) = \sigma^2 + \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k\tau_k^2}\)</span></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Other stuff</strong>
</p>
<ul>
<li><strong>The T and F</strong>
<ul>
<li>The squared independent random variable with a t-distribution with <span class="math inline">\(v\)</span> degree of freedom has an F distribution with degrees of freedom 1 of 1 and degree of freedom 2 of <span class="math inline">\(v\)</span></li>
<li><strong>My interpretation (Read with caution)</strong>
<ul>
<li>t and F are ratios of the variation due to the model to the natural random variation in the distribution</li>
<li>t is the ratio of the variation due to the model in terms of difference in their expected values to the natural random variation in the distribution (the distribution being the sampling distribution; so they look at the expected values)</li>
<li>F is the ratio of the variation due to the model in terms of individual scores to the natural random variation in the distribution (the distribution being the population distribution; so they look at individual scores rather than expected values)</li>
</ul></li>
<li><span class="math inline">\(t^2 = F\)</span></li>
</ul></li>
<li><strong>Variance of a linear combination of random variables</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\begin{aligned} \displaystyle y &amp;= \sum_{i = 1}^{n}{a_ix_i} = a_1x_1 + a_2x_2 + a_3x_3 + \cdots + a_nx_n \\ \displaystyle \text{Var}(y) &amp;= \sum_{i=1}^{n}{a_i^2\text{Var}(x_i)} + 2\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(a_i, a_j)}} \\ \displaystyle \text{Var}(y) &amp;= \text{Var}(x_i)\sum_{i=1}^{n}{a_i^2} + 2\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(a_i, a_j)}} \end{aligned}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li>The double sum is over all pairs <span class="math inline">\((i,j)\)</span> with <span class="math inline">\(i &lt; j\)</span></li>
</ul></li>
<li><strong><em>Ref</em></strong>
<ul>
<li>Wackerly, Mendenhall and Scheaffer</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Applications of this fact</strong>
<ul>
<li><strong>Sampling variance/standard error</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The sampling variance or standard error of a linear function/combination of random variables can be found using this fact</li>
<li>Most statistics are a linear combination of random variables (e.g. sample mean, sample beta, etc.)</li>
</ul></li>
<li><strong>Using the fact to derive the sampling variance of the mean</strong>
<ul>
<li>Let <span class="math inline">\(\bar{y_i}\)</span> be a sample mean</li>
<li>The sample mean is a linear function
<ul>
<li><span class="math inline">\(\bar{y_i} = \frac{x_1 + x_2 + x_3 + \cdots + x_n}{n} \\ \bar{y_i} = \frac{1}{n}x_1 + \frac{1}{n}x_2 + \frac{1}{n}x_3 + \cdots + \frac{1}{n}x_n \\ \bar{y}_i = \sum_{i = 1}^{n}{\frac{1}{n}x_i} \\ \displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)\sum_{i = 1}^{n}{\left(\frac{1}{n}\right)^2} + 2\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(x_i, x_j)}}\)</span></li>
</ul></li>
<li>Assuming that <span class="math inline">\(x_i\)</span> are independent, which implies that <span class="math inline">\(\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(x_i, x_j)}} = 0\)</span>
<ul>
<li><span class="math inline">\(\displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)\sum_{i = 1}^{n}{\left(\frac{1}{n}\right)^2} \\ \displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)n{\left(\frac{1}{n}\right)^2} \\ \displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)\frac{1}{n} \\ \displaystyle \text{Var}(\bar{y_i}) = \frac{\sigma^2}{n} \\ \displaystyle \text{sd}(\bar{y_i}) = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>The F statistic</strong>
</p>
<ul>
<li><strong>The F statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The <span class="math inline">\(F\)</span> statistic is the ratio of a independent random variable with a Chi-squared distribution divided by its degrees of freedom to another independent random variable with a Chi-squared distribution divided by its degrees of freedom</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = \frac{\frac{S_1}{df_1}}{\frac{S_2}{df_2}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(S_1\)</span> is an independent random variable with a Chi-squared distribution with degrees of freedom of <span class="math inline">\(df_1\)</span></li>
<li><span class="math inline">\(S_2\)</span> is another independent random variable with a Chi-squared distribution with degrees of freedom of <span class="math inline">\(df_2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The F-distribution: The distribution of the F statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The <span class="math inline">\(F\)</span> statistic has an <span class="math inline">\(F\)</span> distribution</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li>I am not including the PDF maths here</li>
</ul></li>
</ul></li>
<li><strong>Sum of Squared Deviations</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka Sum of Squares</li>
<li>A measure of variation</li>
<li>The Sum of Squared Deviation is the sum of all the squared deviations where a deviation is the difference between an observed value and its expected value</li>
<li>It is the standardized version of the variance</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle SS = \sum_{i=1}^{n}{\left( y_i - E(y)\right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> is a variable</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>ANOVA as a Linear Model</strong>
</p>
<ul>
<li><p><strong>Introduction</strong></p>
<ul>
<li>ANOVA can be implemented in a Linear Model</li>
</ul></li>
<li><p><strong>2-sample ANOVA as a Linear Model</strong></p>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{ik} = \beta_{0} + \beta_{k}x_k + \epsilon_{ik}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y_{ik}\)</span>
<ul>
<li>Observation <span class="math inline">\(i\)</span> in group/sample <span class="math inline">\(k\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{0}\)</span>
<ul>
<li>The reference group/sample - The group/sample against which each of the other samples are compared</li>
<li>If there is a control group, it would make the most sense if the control group is used as the reference group</li>
</ul></li>
<li><span class="math inline">\(\beta_{k}\)</span>
<ul>
<li>The difference between the mean of sample/group <span class="math inline">\(k\)</span> and <span class="math inline">\(\beta_0\)</span></li>
<li><span class="math inline">\(\beta_k = \mu_k - \mu_0\)</span></li>
<li>Represents the effect of level <span class="math inline">\(k\)</span> of the categorical predictor variable relative to the reference group</li>
</ul></li>
<li><span class="math inline">\(x_k\)</span>
<ul>
<li>A dummy or indicator variable for <span class="math inline">\(\beta_k\)</span></li>
<li><span class="math inline">\(x_k = \begin{cases} 1 ~~~ \text{if the observation is from the group} ~ k \\ 0 ~~~ \text{if the observation is not from the group}~k \end{cases}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{0} + \beta_{k}x_k\)</span>
<ul>
<li>The expected value of group <span class="math inline">\(k\)</span></li>
<li>This is equivalent to <span class="math inline">\(\mu_k\)</span></li>
</ul></li>
<li><span class="math inline">\(\epsilon_{ik}\)</span>
<ul>
<li>The random error of observation <span class="math inline">\(i\)</span> in group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\epsilon_{ik}\)</span> is a random independent normal variable with <span class="math inline">\(E(\epsilon_{ik}) = 0\)</span> and <span class="math inline">\(Var(\epsilon_{ik}) = \sigma^2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>NHST</strong></p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>To test the null hypothesis that the <span class="math inline">\(\beta_1 = \mu_1 - \mu_0 = 0\)</span></li>
<li>For this 2-sample test, you can use a t-statistic or an F statistic</li>
</ul></li>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null hypothesis</strong>
<ul>
<li>Beta 1 is 0</li>
<li><span class="math inline">\(H_0: \beta_1 = 0\)</span></li>
</ul></li>
</ul></li>
<li><strong>t test</strong>
<ul>
<li><strong>The t statistic</strong>
<ul>
<li><span class="math inline">\(t = \frac{\hat\mu_1 - \hat\mu_0}{\text{sd}(\hat\mu_1 - \hat\mu_0)} = \frac{\hat\beta_1}{\text{sd}(\hat\beta)}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\text{sd}(\hat\beta)\)</span> is the standard error of the <span class="math inline">\(\hat\beta_1\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>F test</strong>
<ul>
<li><strong>The F-statistic</strong>
<ul>
<li><span class="math inline">\(F = \frac{MS_B}{MS_E}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Bootstrap</strong></p>
<ul>
<li>For robust confidence intervals and p-values</li>
</ul></li>
<li><p><strong>Non-parametric F</strong></p>
<ul>
<li>Kruskal-Wallis test</li>
</ul></li>
<li><p><strong>Robust F</strong></p>
<ul>
<li>Welch’s F</li>
</ul></li>
<li><p><strong>3-sample ANOVA as a Linear Model</strong></p></li>
<li><p><strong>Mann-Whitney test</strong></p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka Wilcoxon rank sum test</li>
<li>Non-parametric test for difference between 2 independent samples</li>
<li>Observations from both samples are grouped together and ranked in ascending order</li>
<li>The test statistic the rank sum (sum of the ranks) of one of the samples (this statistic is called R)</li>
<li>Statistical significance is assessed by finding the probability of the observed statistic in the probability distribution of the R test statistic under the null hypothesis</li>
</ul></li>
<li><strong>Procedure</strong>
<ul>
<li>Group all observations from both samples together and rank the observations in ascending order</li>
<li>Calculate the sample R statistic (the R statistic is the sum of the ranks of those observations in either one of the 2 groups)</li>
<li>Find the probability distribution of R under the null hypothesis</li>
<li>Find or estimate the probability of the observed R in the probability distribution of R under the null hypothesis</li>
</ul></li>
<li><strong>Finding the probability distribution of R under the null hypothesis</strong>
<ul>
<li>Find all possible combination of ranks (the number of combination of ranks should be <span class="math inline">\(n\choose r\)</span> where n is the total number of ranks and r is the number of ranks in the group of interest)</li>
<li>Under the null hypothesis, each of all possible combination of ranks have an equal chance, hence, each of all possible combination occurs only once</li>
<li>Find the R of each of all possible combinations of ranks</li>
<li>The distribution of the set of Rs found is the distribution of R under the null hypothesis</li>
</ul></li>
<li><strong>Example</strong>
<ul>
<li>The raw data:
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;">
group
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Control
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Control
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
4
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Experimental
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
1
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Experimental
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
3
</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li>Group all observations from both samples together and rank the observations in ascending order
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;">
Rank Combinations
</th>
<th style="text-align:right;">
W
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 2
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
3
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
4
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
3, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
7
</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li>Calculate the sample W statistic
<ul>
<li>The W statistic is the sum of the ranks of those observations in either one of the 2 groups</li>
<li>Here, the Control group is chosen. The W statistic is 7 (<span class="math inline">\(W = 7\)</span>)</li>
</ul></li>
<li>Find all possible rank combinations for either one of groups and calculate the W statistic for each of the rank combinations</li>
<li>Find the probability distribution of W under the null hypothesis
<ul>
<li>Find all possible rank combinations for either one of the groups</li>
<li>Find the probability distribution of the rank combinations under the null hypothesis
<ul>
<li>Under the null hypothesis, each of the rank combination has an equal chance. Hence, each of the rank combination occur only once.</li>
</ul></li>
<li>Calculate the W statistic for each of the rank combinations.
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;">
Rank Combinations
</th>
<th style="text-align:right;">
W
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 2
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
3
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
4
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
3, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
7
</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li>The probability distribution of the resultant set of W statistics is the probability distribution of W under the null hypothesis
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
W
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
3
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
4
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
5
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
6
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
7
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
Freq
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
2
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
</tr>
<tr>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
prob
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.333
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul></li>
<li>Find the probability of the observed W statistic in the probability distribution under the null hypothesis
<ul>
<li>As mentioned, <span class="math inline">\(W = 7\)</span></li>
<li><span class="math inline">\(P(W = 7) = 0.167\)</span></li>
</ul></li>
</ul></li>
<li><strong>Critical regions</strong>
<ul>
<li>The critical regions can be found in published tables</li>
<li>If the probability of the observed W statistic falls outside the critical boundary, then it is statistically significant</li>
</ul></li>
<li><strong>Ties</strong>
<ul>
<li>If there are ties in the data, then the ranks for the ties will be the same and it will be the mean of the supposed ranks of the ties</li>
<li><strong>Example</strong>
<ul>
<li>If the data is 10.5, 11.2, 15.7, 15.7, 15.7, 15.7, 16.2, 18.5</li>
<li>The ranks would be 1, 2, 3, 4.5, 4.5, 4.5, 4.5, 8, 9</li>
</ul></li>
</ul></li>
</ul></li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="mathematics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="derivation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/samuel-mak/statistics/edit/master/04-Linear_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/samuel-mak/statistics/blob/master/04-Linear_Regression.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub", "bookdownproj.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
