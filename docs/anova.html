<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 ANOVA | Statistics</title>
  <meta name="description" content="Chapter 4 ANOVA | Statistics" />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 ANOVA | Statistics" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 ANOVA | Statistics" />
  
  
  

<meta name="author" content="Samuel Mak" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mathematics.html"/>
<link rel="next" href="derivation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a >Section</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> The Linear Regression Model</a></li>
<li class="chapter" data-level="2" data-path="generalised-least-squares.html"><a href="generalised-least-squares.html"><i class="fa fa-check"></i><b>2</b> Generalised Least Squares</a></li>
<li class="chapter" data-level="3" data-path="mathematics.html"><a href="mathematics.html"><i class="fa fa-check"></i><b>3</b> Mathematics</a></li>
<li class="chapter" data-level="4" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>4</b> ANOVA</a></li>
<li class="chapter" data-level="5" data-path="derivation.html"><a href="derivation.html"><i class="fa fa-check"></i><b>5</b> Derivation</a></li>
<li class="chapter" data-level="6" data-path="multivariate-analysis.html"><a href="multivariate-analysis.html"><i class="fa fa-check"></i><b>6</b> Multivariate Analysis</a></li>
<li class="chapter" data-level="7" data-path="catgorical-outcomes.html"><a href="catgorical-outcomes.html"><i class="fa fa-check"></i><b>7</b> Catgorical outcomes</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="anova" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> ANOVA<a href="anova.html#anova" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>ANOVA</strong>
</p>
<ul>
<li><strong>Analysis of Variance (ANOVA)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Under the null hypothesis, it assumes that each of the groups is an independent random sample from the population</li>
<li>ANOVA involves assessing the agreement between two estimators of the population variance</li>
</ul></li>
</ul></li>
<li><strong>The Statistical Model of ANOVA</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The value of observation <span class="math inline">\(i\)</span> on outcome variable <span class="math inline">\(y\)</span> is the grand mean plus the effect of the grouping variable it is in plus random error</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{i,k} = \mu + \tau_{k} + \epsilon_{i, k}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y_{i,k}\)</span> - The value of the outcome variable <span class="math inline">\(y\)</span> of observation <span class="math inline">\(i\)</span> in population <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\mu\)</span> - The overall/grand mean of the outcome variable <span class="math inline">\(y\)</span> (the mean of the superpopulation)</li>
<li><span class="math inline">\(\tau_{k}\)</span>
<ul>
<li>The deviation of population <span class="math inline">\(k\)</span> from the overall mean (<span class="math inline">\(\mu\)</span>)</li>
<li>The nonrandom effect of the group</li>
<li>Group or sample <span class="math inline">\(k\)</span> is assumed to be sampled from this population <span class="math inline">\(k\)</span></li>
<li>Since they are normalised, <span class="math inline">\(\sum_{k=1}^{k}{\tau_k} = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(\mu + \tau_{k}\)</span>
<ul>
<li>The mean of population <span class="math inline">\(k\)</span></li>
<li>The expected value of observation <span class="math inline">\(i\)</span> in population <span class="math inline">\(k\)</span> (<span class="math inline">\(y_{i,k}\)</span>)</li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i,k}\)</span>
<ul>
<li>The random error for observation <span class="math inline">\(i\)</span> in population <span class="math inline">\(k\)</span></li>
<li>The difference between an observed value (<span class="math inline">\(y_{i,k}\)</span>) and its expected value (<span class="math inline">\(\mu + \tau_{k}\)</span>)</li>
<li>Assuming that <span class="math inline">\(y_{ik}\)</span> is a random independent normal variable with <span class="math inline">\(E(y_{ik}) = 0\)</span> and <span class="math inline">\(Var(y_{ik}) = 0\)</span>, since the error term is the difference between <span class="math inline">\(y_{i,k}\)</span>, a normally distributed random variable with variance of <span class="math inline">\(\sigma^2\)</span>, and its mean, this follows that the errors are also a random independent normal variable with <span class="math inline">\(E(\epsilon_{ik}) = 0\)</span> (because subtracting each of the errors by its mean will normalize it to 0) and <span class="math inline">\(Var(\epsilon_{i,k}) = \sigma^2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Hypothesis Testing for ANOVA</strong>
<ul>
<li><strong>Conceptual Hypotheses in ANOVA</strong>
<ul>
<li><strong>Null hypothesis</strong>
<ul>
<li><span class="math inline">\(H_0:\)</span> All populations are the same (are from the same superpopulation)</li>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 = \cdots = \mu_k\)</span></li>
</ul></li>
<li><strong>Alternative hypothesis</strong>
<ul>
<li><span class="math inline">\(H_1:\)</span> The populations are different</li>
<li><span class="math inline">\(H_0: \mu_1 ≠ \mu_2 ≠ \cdots ≠ \mu_k 0\)</span></li>
</ul></li>
</ul></li>
<li><strong>Operationalising the null hypothesis to be tested</strong>
<ul>
<li><strong>General Concept</strong>
<ul>
<li>The operationalisation of the ANOVA null hypothesis is based on the agreement/disaggreement between 2 estimators of the variance of the superpopulation (<span class="math inline">\(\sigma^@\)</span>) (or it’s called common variance)</li>
</ul></li>
<li><strong>Estimating the population variance <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li><strong>There are 2 estimators of <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li><span class="math inline">\(MS_W\)</span></li>
<li><span class="math inline">\(MS_B\)</span></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_W\)</span> estimator</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(SS_W\)</span>, and thus also <span class="math inline">\(MS_W\)</span>, are statistics derived from multiple independent random samples, hence, <span class="math inline">\(SS_W\)</span>, and also <span class="math inline">\(MS_W\)</span>, are independent random variables</li>
<li>And the expectation of <span class="math inline">\(MS_W\)</span> is <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(E(MS_W) = \sigma^{2}\)</span></li>
</ul></li>
<li><strong>Implications for <span class="math inline">\(MS_W\)</span> as an estimator of <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li>Hence, the observed <span class="math inline">\(MS_W\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_B\)</span> estimator</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li><span class="math inline">\(SS_B\)</span> and <span class="math inline">\(MS_B\)</span> are statistics derived from multiple independent random samples, hence, <span class="math inline">\(SS_B\)</span> and <span class="math inline">\(MS_B\)</span> are independent random variables</li>
<li><span class="math inline">\(MS_B\)</span> is an independent random variable that has a Chi-squared distribution with an expectation of <span class="math inline">\(\sigma^2 + (\frac{1}{k-1})\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span><br />
</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle E\left( MS_B \right) = \sigma^2 + \left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span>
<ul>
<li><strong><em>Notes</em></strong>
<ul>
<li>See proof below</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Implication for <span class="math inline">\(MS_B\)</span> as an estimator of <span class="math inline">\(\sigma^2\)</span></strong>
<ul>
<li>Under the null hypothesis, that is when all <span class="math inline">\(\tau_ks = 0\)</span>, this linear combination <span class="math inline">\(\left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span> is <span class="math inline">\(0\)</span>, therefore, <span class="math inline">\(\displaystyle E\left( MS_B \right) = \sigma^2 + 0\)</span>, which means that the observed <span class="math inline">\(MS_B\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
<li>If the null hypothesis is false, that is when at least one of the <span class="math inline">\(\tau_ks ≠ 0\)</span>, the linear combination of <span class="math inline">\(\tau_ks\)</span> (<span class="math inline">\(\left(\frac{1}{k-1}\right)\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span>) will be larger than <span class="math inline">\(0\)</span>, hence, <span class="math inline">\(MS_B\)</span> is a positively biased estimator of <span class="math inline">\(\sigma^2\)</span> (it will tend to overestimate <span class="math inline">\(\sigma^2\)</span>)</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The test statistic: The F ratio</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The conceptual null hypothesis of ANOVA (<span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3 = \cdots = \mu_k\)</span>) is formally operationalised as <span class="math inline">\(H_0: \sum_{k=1}^{k}{\tau_k^2} = 0\)</span> and this is tested by comparing the <span class="math inline">\(MS_B\)</span> estimator to the <span class="math inline">\(MS_W\)</span> estimator as a ratio of <span class="math inline">\(\frac{MS_B}{\sigma^2}\)</span> to <span class="math inline">\(\frac{MS_W}{\sigma^2}\)</span></li>
<li>This ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span> is the test statistic for the null hypothesis of ANOVA and this test statistic is called the <span class="math inline">\(F\)</span> ratio (or <span class="math inline">\(F\)</span> statistic)<br />
</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = \frac{MS_B}{MS_W}\)</span></li>
</ul></li>
<li><strong>Interpretation</strong>
<ul>
<li>If the null hypothesis is true, that is <span class="math inline">\(\sum_{k=1}^{k}{\tau_k^2} = 0\)</span>, the expected value of <span class="math inline">\(E(MS_B)\)</span> is just the random variation in the superpopulation (<span class="math inline">\(E(MS_B) = \sigma^2\)</span>), and <span class="math inline">\(F = \frac{E(MS_B)}{E(MS_W)} = \frac{\sigma^2}{\sigma^2} = 1\)</span>. Using the sample <span class="math inline">\(MS_B\)</span> and <span class="math inline">\(MS_W\)</span> as estimators of <span class="math inline">\(E(MS_B)\)</span> and <span class="math inline">\(E(MS_B)\)</span> respectively would yield an <span class="math inline">\(F\)</span> statistic that is close to 1</li>
<li>If the null hypothesis is false, that is when at least one of the <span class="math inline">\(\tau_ks &gt; 0\)</span> (<span class="math inline">\(\sum_{k=1}^{k}{\tau_k^2} &gt; 0\)</span>), in other words, when at least one of the populations is different, the expected value <span class="math inline">\(E(MS_B)\)</span> consists of both the random variation in the superpopulation and the overall variation induced by the groups (<span class="math inline">\(E(MS_B) = \sigma^2 + \left(\frac{1}{k-1} \right)\sum_{k=1}^{k}{n_k\tau_k^2}\)</span>), then <span class="math inline">\(F = \frac{E(MS_B)}{E(MS_W)} = \frac{\sigma^2 + \left(\frac{1}{k-1} \right)\sum_{k=1}^{k}{n_k\tau_k^2}}{\sigma^2}\)</span>, <em>where</em> <span class="math inline">\(\left(\frac{1}{k-1} \right)\sum_{k=1}^{k}{n_k\tau_k^2} &gt; 0\)</span>, the <span class="math inline">\(F\)</span> statistic will be larger than 1</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>NHST</strong>
<ul>
<li>The null hypothesis of ANOVA is thus tested by an Upper-Tail <span class="math inline">\(F\)</span> test</li>
</ul></li>
</ul></li>
<li><strong>The distribution of the F-statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Since <span class="math inline">\(MS_B\)</span> is <span class="math inline">\(SS_B\)</span> (a chi-squared-distributed independent random variable) divided by its degrees of freedom, and <span class="math inline">\(MS_W\)</span> is <span class="math inline">\(SS_W\)</span> (another chi-squared-distributed random variable) divided by its degrees of freedom, the ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span> has a Central <span class="math inline">\(F\)</span> distribution, that is an <span class="math inline">\(F\)</span> distribution with a degrees of freedom for the numerator of <span class="math inline">\(v_1 = k-1\)</span> and a degrees of freedom for the denominator of <span class="math inline">\(v_2 = N - k\)</span> for the denominator</li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F_{central} \sim F\left(k - 1, N - k \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The F test</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of assessing the probability of an F statistic as extreme as or more extreme than the observed F statistic in the Central F distribution</li>
</ul></li>
</ul></li>
<li><strong>NHST for ANOVA</strong>
<ul>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>There are no difference between the means</li>
<li>The samples are from the same population</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3 = \cdots = \mu_k\)</span> or <span class="math inline">\((\tau_k - \bar{\tau})^2 = 0\)</span></li>
</ul></li>
</ul></li>
<li><strong>Alternative hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>At least one of the samples is likely from another population</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(H_1: \mu_1 ≠ \mu_2 ≠ \mu_3 ≠ \cdots ≠ \mu_k\)</span> or <span class="math inline">\((\tau_k - \bar{\tau})^2 &gt; 0\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>NHST</strong>
<ul>
<li>The null hypothesis is tested by an upper-tailed <span class="math inline">\(F\)</span> test</li>
<li>The null hypothesis is tested by assessing the probability of getting an F-statistic equal to or larger than the observed <span class="math inline">\(F\)</span> statistic in the Central <span class="math inline">\(F\)</span> distribution</li>
<li>The F test for 2 samples is equivalent to the t test</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(H_0: \sum_{k=1}^{k}{\tau_k^2} = 0\)</span> (operationally)
- <strong>Hypothesis Testing for ANOVA</strong></p>
<ul>
<li><strong>Total Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The Sum of Squares that represents the total amount of variation in the outcome variable</li>
<li>It is the sum of all the squared deviations between each observed value and the grand mean</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SST.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} = {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} \end{alignat*}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(k\)</span> represents group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(i\)</span> represents observation <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(y_{i,k}\)</span> - Observation <span class="math inline">\(i\)</span> in group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\bar{y}\)</span> - The grand mean</li>
</ul></li>
<li><strong><em>Notes</em></strong>
<ul>
<li>Alternatively, it can be expressed simply as:
<ul>
<li><span class="math inline">\(\displaystyle \sum_{n=1}^{n}{(y_i - \bar{y})^2}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Total degrees of freedom</strong>
<ul>
<li><span class="math inline">\(\begin{aligned}df_T &amp;= df_M + df_E \\ df_T &amp;= N-1 \end{aligned}\)</span></li>
</ul></li>
<li><strong>Mean Total Sum of Squares (Total Variance)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Total Variance in the outcome variable</li>
<li>The mean amount of variation in the outcome variable</li>
<li>The average amount by squared deviation between each observation and the grand mean</li>
<li>It is the standardised version of the Total Sum of Squares</li>
<li>Because the Total Sum of Squares is the sum of all the squared deviations, it depends on the degrees of freedom. Finding the mean, specifically, dividing the Total Sum of Squares by the degrees of freedom will standardise the quantity</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_T = \frac{SS_T}{df_T}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>ANOVA and variance partitioning</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>ANOVA partitions the Total Sum of Squares of the outcome variable into 2 components:
<ul>
<li>The Model Sum of Squares</li>
<li>The Error Sum of Squares</li>
</ul></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} &amp;= {SS_{M}} + {SS_{E}} \\ {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} &amp;= {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2} + {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2} }} \end{alignat*}\)</span></li>
</ul></li>
<li><strong>Between/Model Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The amount of variation between groups/samples/populations</li>
<li>In an experimental design, it represents the amount of variation in the outcome variable that is induced or explained by the model in the model (systematic variation)</li>
<li>It is the sum of all the squared deviations between the predicted value of each observation (which is the mean of the group to which the observation belongs) and the grand mean</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SSM.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\({SS_{B}} = {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(n_k\)</span> - Group size</li>
<li><span class="math inline">\(\bar{y}_k\)</span> - The mean of group $k$0</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(SS_B\)</span></strong>
<ul>
<li><span class="math inline">\(\frac{SS_B}{\sigma^2} \sim \chi()\)</span></li>
</ul></li>
<li><strong>Model degrees of freedom</strong>
<ul>
<li><span class="math inline">\(df_B = K - 1\)</span></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(SS_B\)</span></strong>
<ul>
<li><span class="math inline">\(\displaystyle \frac{SS_B}{\sigma^2} \sim \chi \left(K - 1 \right)\)</span></li>
</ul></li>
<li><strong>Mean Model Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Between-group/sample variance - The variance between groups
<ul>
<li>In a 2-group/sample design, it is the variance between 2 groups</li>
</ul></li>
<li>Variance in the outcome variable explained by the model</li>
<li>The mean amount of variation in the outcome variable explained by the model</li>
<li>The average amount of squared deviation between the predicted value of each observation and the grand mean</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_M = \frac{SS_M}{df_M}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Error Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It represents the amount of variation in the outcome variable that is not explained by the model or attributable to the predictors in the model and is assumed to be due to random error</li>
<li>It is the sum of all the squared deviations between each observed value and the mean of the group to which it belongs</li>
</ul></li>
<li><strong>Visualisation</strong>
<ul>
<li><img src="image/anova_SSR.png" style="width:35.0%" /></li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\({SS_{E}} = {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2}}\)</span>
<ul>
<li><strong><em>Notes</em></strong>
<ul>
<li>Alternatively, it is the sum of the variance of each group times the degrees of freedom of that group (so you get the error sum of squares of that group)
<ul>
<li><span class="math inline">\(\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Degrees of freedom</strong>
<ul>
<li><span class="math inline">\(df_e = N-k\)</span></li>
</ul></li>
<li><strong>Distribution of <span class="math inline">\(\frac{SS_W}{\sigma^2}\)</span></strong>
<ul>
<li><span class="math inline">\(\displaystyle \frac{SS_W}{\sigma^2} \sim \chi \left(N - K \right)\)</span></li>
</ul></li>
<li><strong>Mean Error Sum of Squares (Variance Unexplained/Error Variance)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Variance in the outcome variable not explained by the model and is assumed to be due to random error</li>
<li>The mean amount of variation in the outcome variable not explained by the model</li>
<li>The average amount of squared deviation between the predicted value of each observation and its predicted value</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_E = \frac{SS_E}{df_e}\)</span>
<ul>
<li>Alternatively, it is just the error variance
<ul>
<li><span class="math inline">\(\displaystyle MS_E = s^2 = \frac{1}{n-k}\sum_{k=1}^{k}{(n_k-1)s_{k}^{2}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The Model as a whole</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle \begin{alignat*}{10} {SS_{T}} &amp;= {SS_{M}} + {SS_{E}} \\ {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y})^2}}} &amp;= {\sum_{k = 1}^{k}{n_k(\bar{y}_k - \bar{y})^2} + {\sum_{k=1}^{k}{\sum_{i=1}^{n_k}{(y_{i,k} - \bar{y}_k})^2} }} \end{alignat*}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Proof of <span class="math inline">\(E(MS_B) = \sigma^2\)</span></strong>
<ul>
<li><span class="math inline">\(\displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left(\bar{y}_{k} - \bar{y} \right)^2}\)</span>
<ul>
<li>Since:
<ul>
<li><span class="math inline">\(\begin{aligned} \displaystyle \bar{y}_{k} &amp;= \frac{1}{n_k}\sum_{i = 1}^{n_k}{y_{ik}} \\ \displaystyle &amp;= \frac{1}{n_k}\sum_{i = 1}^{n_k}{\left( \mu + \tau_k + \epsilon_{ik} \right)} \\ \displaystyle &amp;= \mu + \tau_k + \frac{1}{n_k}\sum_{i = 1}^{n_k}{\epsilon_{ik}} \\ \displaystyle &amp;= \mu + \tau_k + \bar{\epsilon}_k \end{aligned}\)</span></li>
<li><span class="math inline">\(\begin{aligned} \displaystyle \bar{y} &amp;= \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{y_{ik}}} \\ \displaystyle &amp;= \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{\left( \mu + \tau_k + \epsilon_{ik} \right)}} \\ \displaystyle &amp;= \mu + \frac{1}{N}\sum_{k = 1}^{k}{n_k \tau_k} + \frac{1}{N}\sum_{k = 1}^{k}{\sum_{i = 1}^{n_k}{\epsilon_{ik}}} \\ \displaystyle &amp;= \mu + \bar{\tau} + \bar{\epsilon} \end{aligned}\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left(\bar{y}_{k} - \bar{y} \right)^2} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left[(\mu + \tau_k + \bar{\epsilon}_k) - (\mu + \bar{\tau} + \bar{\epsilon}) \right]^2} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left[(\tau_k - \bar{\tau}) + (\bar{\epsilon}_k - \bar{\epsilon}) \right]^2} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k \left[(\tau_k - \bar{\tau})^2 + 2(\tau_k - \bar{\tau})(\bar{\epsilon}_k - \bar{\epsilon}) + (\bar{\epsilon}_k - \bar{\epsilon})^2 \right]} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{ \left[n_k(\tau_k - \bar{\tau})^2 + 2n_k(\tau_k - \bar{\tau})(\bar{\epsilon}_k - \bar{\epsilon}) + n_k(\bar{\epsilon}_k - \bar{\epsilon})^2 \right]} \\ \displaystyle MS_B = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right)\sum_{k=1}^{k}{2n_k(\tau_k - \bar{\tau})(\bar{\epsilon}_k} - \bar{\epsilon}) + \left( \frac{1}{k - 1}\right)\sum_{k=1}^{k}{n_k(\bar{\epsilon}_k - \bar{\epsilon})^2} \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) E\left[\sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\right] + \left( \frac{1}{k - 1}\right) E\left[\sum_{k=1}^{k}{2n_k(\tau_k - \bar{\tau})(\bar{\epsilon}_k} - \bar{\epsilon})\right] + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_k - \bar{\epsilon})^2}\right]\)</span>
<ul>
<li>Since:
<ul>
<li><span class="math inline">\(\tau_k\)</span> and thus <span class="math inline">\(\bar{\tau}\)</span> are constants and <span class="math inline">\(E(\epsilon_{ik}) = E(\bar\epsilon_{i}) = E(\bar\epsilon) = 0\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_k - \bar{\epsilon})^2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k(\bar{\epsilon}_{k}^{2} - 2\bar{\epsilon}_{k}\bar{\epsilon} + \bar{\epsilon}^{2})}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{n_k\bar{\epsilon}_{k}^{2} - 2n_k\bar{\epsilon}_{k}\bar{\epsilon} + n_k\bar{\epsilon}^{2}}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{(n_k\bar{\epsilon}_{k}^{2})} - 2N\bar{\epsilon}^{2} + N\bar{\epsilon}^{2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) E\left[ \sum_{k=1}^{k}{(n_k\bar{\epsilon}_{k}^{2})} - N\bar{\epsilon}^{2}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{n_k E(\bar{\epsilon}_{k}^{2})} - NE(\bar{\epsilon}^{2})\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{n_k \frac{\sigma^2}{n_k}} - N\frac{\sigma^2}{N}\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) \left[\sum_{k=1}^{k}{\sigma^2} - \sigma^2\right] \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) (k{\sigma^2} - \sigma^2) \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \left( \frac{1}{k - 1}\right) (k-1)\sigma^2 \\ \displaystyle E(MS_B) = \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2} + \sigma^2 \\ \displaystyle E(MS_B) = \sigma^2 + \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k(\tau_k - \bar{\tau})^2}\)</span>
<ul>
<li>Since:
<ul>
<li><span class="math inline">\(\bar{\tau} = 0\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\displaystyle E(MS_B) = \sigma^2 + \left( \frac{1}{k - 1}\right) \sum_{k=1}^{k}{n_k\tau_k^2}\)</span></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Other stuff</strong>
</p>
<ul>
<li><strong>The T and F</strong>
<ul>
<li>The squared independent random variable with a t-distribution with <span class="math inline">\(v\)</span> degree of freedom has an F distribution with degrees of freedom 1 of 1 and degree of freedom 2 of <span class="math inline">\(v\)</span></li>
<li><strong>My interpretation (Read with caution)</strong>
<ul>
<li>t and F are ratios of the variation due to the model to the natural random variation in the distribution</li>
<li>t is the ratio of the variation due to the model in terms of difference in their expected values to the natural random variation in the distribution (the distribution being the sampling distribution; so they look at the expected values)</li>
<li>F is the ratio of the variation due to the model in terms of individual scores to the natural random variation in the distribution (the distribution being the population distribution; so they look at individual scores rather than expected values)</li>
</ul></li>
<li><span class="math inline">\(t^2 = F\)</span></li>
</ul></li>
<li><strong>Variance of a linear combination of random variables</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\begin{aligned} \displaystyle y &amp;= \sum_{i = 1}^{n}{a_ix_i} = a_1x_1 + a_2x_2 + a_3x_3 + \cdots + a_nx_n \\ \displaystyle \text{Var}(y) &amp;= \sum_{i=1}^{n}{a_i^2\text{Var}(x_i)} + 2\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(a_i, a_j)}} \\ \displaystyle \text{Var}(y) &amp;= \text{Var}(x_i)\sum_{i=1}^{n}{a_i^2} + 2\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(a_i, a_j)}} \end{aligned}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li>The double sum is over all pairs <span class="math inline">\((i,j)\)</span> with <span class="math inline">\(i &lt; j\)</span></li>
</ul></li>
<li><strong><em>Ref</em></strong>
<ul>
<li>Wackerly, Mendenhall and Scheaffer</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Applications of this fact</strong>
<ul>
<li><strong>Sampling variance/standard error</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The sampling variance or standard error of a linear function/combination of random variables can be found using this fact</li>
<li>Most statistics are a linear combination of random variables (e.g. sample mean, sample beta, etc.)</li>
</ul></li>
<li><strong>Using the fact to derive the sampling variance of the mean</strong>
<ul>
<li>Let <span class="math inline">\(\bar{y_i}\)</span> be a sample mean</li>
<li>The sample mean is a linear function
<ul>
<li><span class="math inline">\(\bar{y_i} = \frac{x_1 + x_2 + x_3 + \cdots + x_n}{n} \\ \bar{y_i} = \frac{1}{n}x_1 + \frac{1}{n}x_2 + \frac{1}{n}x_3 + \cdots + \frac{1}{n}x_n \\ \bar{y}_i = \sum_{i = 1}^{n}{\frac{1}{n}x_i} \\ \displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)\sum_{i = 1}^{n}{\left(\frac{1}{n}\right)^2} + 2\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(x_i, x_j)}}\)</span></li>
</ul></li>
<li>Assuming that <span class="math inline">\(x_i\)</span> are independent, which implies that <span class="math inline">\(\sum_{}^{}{\sum_{1≤i&lt;j≤n}^{}{\text{Cov}(x_i, x_j)}} = 0\)</span>
<ul>
<li><span class="math inline">\(\displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)\sum_{i = 1}^{n}{\left(\frac{1}{n}\right)^2} \\ \displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)n{\left(\frac{1}{n}\right)^2} \\ \displaystyle \text{Var}(\bar{y_i}) = \text{Var}(x_i)\frac{1}{n} \\ \displaystyle \text{Var}(\bar{y_i}) = \frac{\sigma^2}{n} \\ \displaystyle \text{sd}(\bar{y_i}) = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>The F statistic</strong>
</p>
<ul>
<li><strong>The F statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The <span class="math inline">\(F\)</span> statistic is the ratio of a independent random variable with a Chi-squared distribution divided by its degrees of freedom to another independent random variable with a Chi-squared distribution divided by its degrees of freedom</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(F = \frac{\frac{S_1}{df_1}}{\frac{S_2}{df_2}}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(S_1\)</span> is an independent random variable with a Chi-squared distribution with degrees of freedom of <span class="math inline">\(df_1\)</span></li>
<li><span class="math inline">\(S_2\)</span> is another independent random variable with a Chi-squared distribution with degrees of freedom of <span class="math inline">\(df_2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The F-distribution: The distribution of the F statistic</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The <span class="math inline">\(F\)</span> statistic has an <span class="math inline">\(F\)</span> distribution</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li>I am not including the PDF maths here</li>
</ul></li>
</ul></li>
<li><strong>Sum of Squared Deviations</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka Sum of Squares</li>
<li>A measure of variation</li>
<li>The Sum of Squared Deviation is the sum of all the squared deviations where a deviation is the difference between an observed value and its expected value</li>
<li>It is the standardized version of the variance</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle SS = \sum_{i=1}^{n}{\left( y_i - E(y)\right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> is a variable</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>ANOVA as a Linear Model</strong>
</p>
<ul>
<li><p><strong>Introduction</strong></p>
<ul>
<li>ANOVA can be implemented in a Linear Model</li>
</ul></li>
<li><p><strong>2-sample ANOVA as a Linear Model</strong></p>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{ik} = \beta_{0} + \beta_{k}x_k + \epsilon_{ik}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y_{ik}\)</span>
<ul>
<li>Observation <span class="math inline">\(i\)</span> in group/sample <span class="math inline">\(k\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{0}\)</span>
<ul>
<li>The reference group/sample - The group/sample against which each of the other samples are compared</li>
<li>If there is a control group, it would make the most sense if the control group is used as the reference group</li>
</ul></li>
<li><span class="math inline">\(\beta_{k}\)</span>
<ul>
<li>The difference between the mean of sample/group <span class="math inline">\(k\)</span> and <span class="math inline">\(\beta_0\)</span></li>
<li><span class="math inline">\(\beta_k = \mu_k - \mu_0\)</span></li>
<li>Represents the effect of level <span class="math inline">\(k\)</span> of the categorical predictor variable relative to the reference group</li>
</ul></li>
<li><span class="math inline">\(x_k\)</span>
<ul>
<li>A dummy or indicator variable for <span class="math inline">\(\beta_k\)</span></li>
<li><span class="math inline">\(x_k = \begin{cases} 1 ~~~ \text{if the observation is from the group} ~ k \\ 0 ~~~ \text{if the observation is not from the group}~k \end{cases}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{0} + \beta_{k}x_k\)</span>
<ul>
<li>The expected value of group <span class="math inline">\(k\)</span></li>
<li>This is equivalent to <span class="math inline">\(\mu_k\)</span></li>
</ul></li>
<li><span class="math inline">\(\epsilon_{ik}\)</span>
<ul>
<li>The random error of observation <span class="math inline">\(i\)</span> in group <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\epsilon_{ik}\)</span> is a random independent normal variable with <span class="math inline">\(E(\epsilon_{ik}) = 0\)</span> and <span class="math inline">\(Var(\epsilon_{ik}) = \sigma^2\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>NHST</strong></p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>To test the null hypothesis that the <span class="math inline">\(\beta_1 = \mu_1 - \mu_0 = 0\)</span></li>
<li>For this 2-sample test, you can use a t-statistic or an F statistic</li>
</ul></li>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null hypothesis</strong>
<ul>
<li>Beta 1 is 0</li>
<li><span class="math inline">\(H_0: \beta_1 = 0\)</span></li>
</ul></li>
</ul></li>
<li><strong>t test</strong>
<ul>
<li><strong>The t statistic</strong>
<ul>
<li><span class="math inline">\(t = \frac{\hat\mu_1 - \hat\mu_0}{\text{sd}(\hat\mu_1 - \hat\mu_0)} = \frac{\hat\beta_1}{\text{sd}(\hat\beta)}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\text{sd}(\hat\beta)\)</span> is the standard error of the <span class="math inline">\(\hat\beta_1\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>F test</strong>
<ul>
<li><strong>The F-statistic</strong>
<ul>
<li><span class="math inline">\(F = \frac{MS_B}{MS_E}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>3-sample ANOVA as a Linear Model with Dummy Coding</strong></p>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{ik} = \beta_{0} + \beta_{1}x_1 + \beta_{2}x_2 + \epsilon_{ik}\)</span>
<ul>
<li><strong>Where</strong>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span>
<ul>
<li>The mean of the sample at the reference/baseline group/level</li>
<li>It represents the predicted value in the outcome variable when all other levels of the Factor are 0 (controlling for the effect of all other levels of the Factor)</li>
<li><span class="math inline">\(\beta_{0} = \mu_{reference ~ level}\)</span></li>
<li>For example, if level 1 is chosen as the reference level, then <span class="math inline">\(\beta_{0} = \mu_{1}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{1}\)</span>
<ul>
<li>Dummy variable 1</li>
<li>Represents the effect of one level of the Factor (by default, R chooses the second level of the Factor)</li>
<li>The difference between the mean of the sample at one level of the Factor and the intercept</li>
<li>For example, if this level is level 2 of the Factor, then <span class="math inline">\(\beta_{1} = \mu_{2} - \mu_{1}\)</span></li>
</ul></li>
<li><span class="math inline">\(x_{1}\)</span>
<ul>
<li>Weight or indicator function for dummy variable 1</li>
<li>Dummy coding
<ul>
<li><span class="math inline">\(x_{1} = \begin{cases} 1 ~~~ \text{if the observation is from sample 1} \\ 0 ~~~ \text{if the observation is not from sample 1} \end{cases}\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\beta_{2}\)</span>
<ul>
<li><span class="math inline">\(\beta_{2} = \mu_{2} - \mu_{0}\)</span></li>
<li>Dummy variable 2</li>
<li>Represents the effect of the other level of the Factor compared to the intercept (by default, R chooses the third level of the Factor)</li>
<li>The difference between the mean of the sample at the other level of the Factor and the intercept</li>
</ul></li>
<li><span class="math inline">\(x_{2}\)</span>
<ul>
<li>Weight or indicator function for dummy variable 2</li>
<li>Dummy contrast coding</li>
<li><span class="math inline">\(x_{2} = \begin{cases} 1 ~~~ \text{if the observation is from sample 1} \\ 0 ~~~ \text{if the observation is not from sample 1} \end{cases}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>3-sample ANOVA as a Linear Model with Helmert Contrast Coding</strong></p>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{ik} = \beta_{0} + \beta_{1}x_1 + \beta_{2}x_2 + \epsilon_{ik}\)</span>
<ul>
<li><strong>Where</strong>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span>
<ul>
<li><span class="math inline">\(\beta_{0} = \mu_{grand}\)</span></li>
<li>The grand mean</li>
<li>The mean of group means</li>
<li>With a 3-sample ANOVA with Helmert coding, it is <span class="math inline">\(\beta_{0} = \mu_{grand} = \frac{\mu_{1} + \mu_{2} + \mu_{3}}{3}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{1}\)</span>
<ul>
<li><span class="math inline">\(\displaystyle \beta_{1} = \mu_{1} - \left(\frac{\mu_{2} + \mu_{3}}{2} \right)\)</span></li>
<li>Helmert Contrast variable 1</li>
<li>The difference between the mean of the sample at level 1 of the factor and the mean of the sample at level 2 and 3 lumped together</li>
<li>Represents the effect of level 2 and 3 compared to level 1</li>
</ul></li>
<li><span class="math inline">\(x_{1}\)</span>
<ul>
<li>Weight or indicator function for Helmert Contrast variable 1</li>
<li><span class="math inline">\(x_{1} = \begin{cases} \frac{2}{3} ~~~ \text{if the observation is from the sample at level 1} \\ -\frac{1}{3} ~~~ \text{if the observation is from the sample at level 2} \\ -\frac{1}{3} ~~~ \text{if the observation is from the sample at level 3}\end{cases}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{2}\)</span>
<ul>
<li><span class="math inline">\(\beta_{2} = \mu_{2} - \mu_{3}\)</span></li>
<li>Helmert Contrast variable 2</li>
<li>Dummy variable for category 2</li>
<li>The difference between the mean of the sample at level 2 of the factor and the mean of the sample at level 3 of the factor</li>
<li>Represents the effect of level 2 of the factor compared to level 3 of the factor</li>
</ul></li>
<li><span class="math inline">\(x_{2}\)</span>
<ul>
<li>Weight or indicator function for Helmert Contrast variable 2</li>
<li><span class="math inline">\(x_{2} = \begin{cases} 0 ~~~ \text{if the observation is from the sample at level 1} \\ \frac{1}{2} ~~~ \text{if the observation is from the sample at level 2} \\ -\frac{1}{2} ~~~ \text{if the observation is from the sample at level 3}\end{cases}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Helmert Contrast</strong></p></li>
<li><p><strong>Bootstrap</strong></p>
<ul>
<li>For robust confidence intervals and p-values</li>
</ul></li>
<li><p><strong>Non-parametric F</strong></p>
<ul>
<li>Kruskal-Wallis test</li>
</ul></li>
<li><p><strong>Robust F</strong></p>
<ul>
<li>Welch’s F</li>
</ul></li>
<li><p><strong>Mann-Whitney test</strong></p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Aka Wilcoxon rank sum test</li>
<li>Non-parametric test for difference between 2 independent samples</li>
<li>Observations from both samples are grouped together and ranked in ascending order</li>
<li>The test statistic the rank sum (sum of the ranks) of one of the samples (this statistic is called R)</li>
<li>Statistical significance is assessed by finding the probability of the observed statistic in the probability distribution of the R test statistic under the null hypothesis</li>
</ul></li>
<li><strong>Procedure</strong>
<ul>
<li>Group all observations from both samples together and rank the observations in ascending order</li>
<li>Calculate the sample R statistic (the R statistic is the sum of the ranks of those observations in either one of the 2 groups)</li>
<li>Find the probability distribution of R under the null hypothesis</li>
<li>Find or estimate the probability of the observed R in the probability distribution of R under the null hypothesis</li>
</ul></li>
<li><strong>Finding the probability distribution of R under the null hypothesis</strong>
<ul>
<li>Find all possible combination of ranks (the number of combination of ranks should be <span class="math inline">\(n\choose r\)</span> where n is the total number of ranks and r is the number of ranks in the group of interest)</li>
<li>Under the null hypothesis, each of all possible combination of ranks have an equal chance, hence, each of all possible combination occurs only once</li>
<li>Find the R of each of all possible combinations of ranks</li>
<li>The distribution of the set of Rs found is the distribution of R under the null hypothesis</li>
</ul></li>
<li><strong>Example</strong>
<ul>
<li>The raw data:
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;">
group
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Control
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Control
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
4
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Experimental
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
1
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
Experimental
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
3
</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li>Group all observations from both samples together and rank the observations in ascending order
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;">
Rank Combinations
</th>
<th style="text-align:right;">
W
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 2
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
3
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
4
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
3, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
7
</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li>Calculate the sample W statistic
<ul>
<li>The W statistic is the sum of the ranks of those observations in either one of the 2 groups</li>
<li>Here, the Control group is chosen. The W statistic is 7 (<span class="math inline">\(W = 7\)</span>)</li>
</ul></li>
<li>Find all possible rank combinations for either one of groups and calculate the W statistic for each of the rank combinations</li>
<li>Find the probability distribution of W under the null hypothesis
<ul>
<li>Find all possible rank combinations for either one of the groups</li>
<li>Find the probability distribution of the rank combinations under the null hypothesis
<ul>
<li>Under the null hypothesis, each of the rank combination has an equal chance. Hence, each of the rank combination occur only once.</li>
</ul></li>
<li>Calculate the W statistic for each of the rank combinations.
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;">
Rank Combinations
</th>
<th style="text-align:right;">
W
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 2
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
3
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
4
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
1, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 3
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
5
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
2, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
6
</td>
</tr>
<tr>
<td style="text-align:left;background-color: #36394A !important;border: 1px solid #36394A;">
3, 4
</td>
<td style="text-align:right;background-color: #36394A !important;border: 1px solid #36394A;">
7
</td>
</tr>
</tbody>
</table></li>
</ul></li>
<li>The probability distribution of the resultant set of W statistics is the probability distribution of W under the null hypothesis
<ul>
<li><table class="table table-condensed" style="font-size: 12px; font-family: inherit; width: auto !important; ">
<thead>
<tr>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
W
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
3
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
4
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
5
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
6
</th>
<th style="text-align:left;border-bottom: 1px solid inherit; border-top: 0px solid black;color: inherit !important;">
7
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
Freq
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
2
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
<td style="text-align:left;border-bottom: 5px solid #36394A;color: inherit !important;background-color: #36394A !important;">
1
</td>
</tr>
<tr>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
prob
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.333
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
<td style="text-align:left;border-top: 0px solid transparent;color: inherit !important;background-color: #36394A !important;">
0.167
</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul></li>
<li>Find the probability of the observed W statistic in the probability distribution under the null hypothesis
<ul>
<li>As mentioned, <span class="math inline">\(W = 7\)</span></li>
<li><span class="math inline">\(P(W = 7) = 0.167\)</span></li>
</ul></li>
</ul></li>
<li><strong>Critical regions</strong>
<ul>
<li>The critical regions can be found in published tables</li>
<li>If the probability of the observed W statistic falls outside the critical boundary, then it is statistically significant</li>
</ul></li>
<li><strong>Ties</strong>
<ul>
<li>If there are ties in the data, then the ranks for the ties will be the same and it will be the mean of the supposed ranks of the ties</li>
<li><strong>Example</strong>
<ul>
<li>If the data is 10.5, 11.2, 15.7, 15.7, 15.7, 15.7, 16.2, 18.5</li>
<li>The ranks would be 1, 2, 3, 4.5, 4.5, 4.5, 4.5, 8, 9</li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Kruskal Wallis test</strong></p>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A nonparametric test of difference between several samples (One-way ANOVA)</li>
<li>A generalisation of the Wilcoxon test</li>
<li>Based on ranked scores</li>
<li>Scores are grouped together and then transformed into ranked scores, the ranked scores are analysed</li>
<li>The null hypothesis is tested by the <span class="math inline">\(K\)</span> statistic (a chi-squared transformed ranked version of the <span class="math inline">\(SS_B\)</span>; see below)</li>
<li>It involves assessing the probability of having a <span class="math inline">\(K\)</span> statistic equal to or more extreme than the observed <span class="math inline">\(K\)</span> statistic in the distribution of <span class="math inline">\(K\)</span> under the null hypothesis (that the samples are from the same population - That there are no differences between samples) (because the statistic is standardised, the K statistic basically assess how much bigger the observed SSB is compared to the SSB in under the null, because the SSB statistic is converted to a standardised Chi-squared statistic, it compares how much bigger the standardised version of the SSB is compare to 1, because the sd (or se) of the chi-squared distribution is 1, this is the natural variation measured as sd in the null chi-squared distribution, the K statistic tells you how many times the observed SSB (or estimate of the variance of the population) is larger than the variance of the null distribution, again the variance is measured as sd)</li>
<li>It involves enumerating the null distribution of <span class="math inline">\(SS_B\)</span> for all possible combinations of <span class="math inline">\(k\)</span> and <span class="math inline">\(n_k\)</span></li>
</ul></li>
<li><strong>The Kruskal Wallis <span class="math inline">\(SS_B\)</span></strong>
<ul>
<li>Let <span class="math inline">\(R_{ik}\)</span> be the rank of <span class="math inline">\(y_{ik}\)</span></li>
<li><span class="math inline">\(SS_B = \sum_{k=1}^{k}{n_k\left(\bar{R}_{k} - \bar{R}\right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\bar{R}_{k}\)</span>
<ul>
<li>The mean of the ranked scores of sample <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\displaystyle \bar{R}_{k} = \frac{1}{n_k}\sum_{i = 1}^{n_k}{R_{ik}}\)</span></li>
</ul></li>
<li><span class="math inline">\(\bar{R}\)</span>
<ul>
<li>The overall mean of the ranked scores</li>
<li><span class="math inline">\(\displaystyle \bar{R} = \frac{1}{N}\sum_{k=1}^{k}{\sum_{i = 1}^{n_k}{R_{ik}}} = \frac{N+1}{2}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>The Kruskal Wallis statistic (<span class="math inline">\(K\)</span>)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The <span class="math inline">\(SS_B\)</span> statistic converted to a standardised ch-square score</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle K = \frac{12}{N(N+1)}SS_B\)</span></li>
</ul></li>
</ul></li>
<li><strong>Null Distribution of the Kruskal Wallis statistic</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(K \sim \chi^2\left( k-1 \right)\)</span></li>
</ul></li>
</ul></li>
<li><strong>Kruskal Wallis test in R</strong>
<ul>
<li><details>
<summary>
<img src="image/r.svg" width = 3% style=" margin-top: 0px; margin-bottom: 0px; padding-top: 1px;"/>
</summary>
<pre style=" margin-top: 10px"><code>
  # Kruskal Wallis test <br>
  kruskal.test(outcome_variable ~ predictor_variable, data = data) <br>
  </code></pre>
</details></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Factorial ANOVA</strong>
</p>
<ul>
<li><strong>Two-way ANOVA</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>A continuous categorical variable predicted by 2 categorical variables</li>
</ul></li>
<li><strong>The statistical model of 2-way ANOVA</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{ijk} = \mu + \alpha_{j} + \beta{k} + \delta_{jk} + \epsilon_{ijk}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y_{ijk}\)</span>
<ul>
<li>Observation <span class="math inline">\(i\)</span> in level <span class="math inline">\(j\)</span> of one factor and level <span class="math inline">\(k\)</span> of another factor</li>
</ul></li>
<li><span class="math inline">\(\mu\)</span>
<ul>
<li>The overall/grand mean</li>
</ul></li>
<li><span class="math inline">\(\alpha_{j}\)</span>
<ul>
<li>The effect of level <span class="math inline">\(j\)</span> factor 1</li>
<li>The difference between the mean of sample <span class="math inline">\(j\)</span> of factor 1 and the overall mean</li>
<li><span class="math inline">\(\alpha_{j} = \mu{}_{j\cdot{}} - \mu_0\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta{k}\)</span>
<ul>
<li>The effect of level <span class="math inline">\(k\)</span> of factor 2</li>
<li>The difference between the mean of sample <span class="math inline">\(k\)</span> of factor 2 and the overall mean</li>
<li><span class="math inline">\(\beta{k} = \mu{}_{\cdot{}k} - \mu_0\)</span></li>
</ul></li>
<li><span class="math inline">\(\delta_{jk}\)</span>
<ul>
<li>The interaction effect</li>
<li>Here the interaction effect is the effect of level <span class="math inline">\(k\)</span> of factor 1 on level <span class="math inline">\(j\)</span> of factor 2</li>
<li><span class="math inline">\(\delta_{jk} = (\mu_{jk} - \mu{}_{j\boldsymbol{\cdot}}) - (\mu_{\boldsymbol{\cdot}k} - \mu_0)\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(\mu_{jk}\)</span> - The mean of sample in level <span class="math inline">\(j\)</span> of factor 1 and level <span class="math inline">\(k\)</span> of factor 2</li>
<li><span class="math inline">\(\mu{}_{j\boldsymbol{\cdot}}\)</span> - The mean of the sample in level <span class="math inline">\(j\)</span> of factor 1 regardless of the level of factor 2</li>
<li><span class="math inline">\(\mu_{\boldsymbol{\cdot}k}\)</span> - The mean of level <span class="math inline">\(k\)</span> of factor 2 regardless of the level of factor 1</li>
<li><span class="math inline">\(\mu_0\)</span> - Overall mean</li>
<li><span class="math inline">\((\mu_{jk} - \mu{}_{j\boldsymbol{\cdot}})\)</span>
<ul>
<li>The difference between the mean of a specific sample (here it is the sample at level <span class="math inline">\(j\)</span> of factor 1 and level <span class="math inline">\(k\)</span> of factor 2) and the overall mean for sample at level <span class="math inline">\(j\)</span> of factor 1 (overall in the sense that it is regardless of the level of factor 2)</li>
<li>The effect of level <span class="math inline">\(k\)</span> of factor 2 on level <span class="math inline">\(j\)</span> of factor 1</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Two-Way ANOVA partitioning variances</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Total Sum of Squares
<ul>
<li>Between-Group Sum of Squares
<ul>
<li>Between-Group Sum of Squares for Factor 1</li>
<li>Between-Group Sum of Squares for Factor 2</li>
<li>Between-Group Sum of Squares for the interaction between Factor 1 and Factor 2</li>
</ul></li>
<li>The Error Sum of Squares</li>
</ul></li>
</ul></li>
<li><strong>Total Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The sum of squared deviation between each observation and the overall/grand mean</li>
<li>This represents the total variation in the outcome variable</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\begin{aligned} \displaystyle SS_T &amp;= SS_B + SS_W \\ \displaystyle SS_T &amp;= \left( SS_A + SS_B + SS_{A\times B} \right) + SS_W \end{aligned}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Operationalsing variability</strong>
<ul>
<li><strong>The Model Sum of Squares</strong>
<ul>
<li><strong>Components of the Model Sum of Squares</strong>
<ul>
<li>Sum of Squares for Factor A</li>
<li>Sum of Squares for Factor B</li>
<li>Sum of Squares for the Interaction between Factor A and B</li>
</ul></li>
<li><strong>Sum of Squares for Factor A (<span class="math inline">\(SS_A\)</span>)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The sum of squared deviation between the predicted value of observation <span class="math inline">\(i\)</span> in level <span class="math inline">\(j\)</span> of Factor A regardless of the level of Factor B and the overall/grand mean</li>
<li>It represents the main effect of Factor A (the effect of Factor A regardless of levels of the other factor) in the form of variation in the outcome variable that is attributable to Factor A</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle SS_A = \sum_{j = 1}^{J}{n_{j \boldsymbol{\cdot}} \left( \bar{y}_{j\boldsymbol{\cdot}} - \bar{y} \right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(n_{j \boldsymbol{\cdot}}\)</span> - Number of observations in level <span class="math inline">\(j\)</span> of Factor A regardless of the level of Factor B</li>
<li><span class="math inline">\(\bar{y}_{j\boldsymbol{\cdot}}\)</span> - The mean of sample in the level <span class="math inline">\(j\)</span> of Factor regardless of the level of Factor B - The predicted value for observation <span class="math inline">\(i\)</span> in level <span class="math inline">\(j\)</span> of Factor A regardless of the level of Factor B</li>
<li><span class="math inline">\(\bar{y}\)</span> - Overall/grand mean</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Degrees of freedom</strong>
<ul>
<li><span class="math inline">\(df_A = J - 1\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(J\)</span> - The total number of levels in Factor A</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Mean Factor A Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Variance in the outcome variable that is attributable to Factor A</li>
<li>The mean/average amount of squared deviation in the outcome variable that is attributable to Factor A (the average in terms of between levels)</li>
<li>Represents the main effect of Factor A in the form of variance in the outcome variable induced by Factor A</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle MS_{A} = \frac{SS_A}{df_A}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Sum of Squares for Factor B (<span class="math inline">\(SS_B\)</span>)</strong>
<ul>
<li><strong>Concept</strong></li>
<li>The sum of squared deviation between the predicted value for observation <span class="math inline">\(i\)</span> in level <span class="math inline">\(k\)</span> of Factor B regardless of the level in Factor A and the overall mean</li>
<li>It represents the main effect of Factor B (the effect of Factor B regardless of levels of the other Factor) in the form of variation in the outcome variable that is attributable to Factor B</li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle SS_B = \sum_{k = 1}^{K}{n_{\boldsymbol{\cdot}k} \left( \bar{y}_{\boldsymbol{\cdot}k} - \bar{y} \right)^2}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(n_{\boldsymbol{\cdot}k}\)</span> - The number of observations in level <span class="math inline">\(k\)</span> of Factor B regardless of the level of Factor A</li>
<li><span class="math inline">\(\bar{y}_{\boldsymbol{\cdot}k}\)</span> - The mean of level <span class="math inline">\(k\)</span> of Factor B regardless of the level of Factor A - The predicted value for observation <span class="math inline">\(i\)</span> in level <span class="math inline">\(k\)</span> of Factor B regardless of the level of Factor A</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Degree of Freedom</strong>
<ul>
<li><span class="math inline">\(df_B = K - 1\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(K\)</span> - The number of levels in Factor B</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Mean Factor B Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Variance in the outcome variable that is attributable to Factor A</li>
<li>The mean/average amount of squared deviation in the outcome variable that is attributable to Factor B (the average in terms of between levels)</li>
<li>It represents the main effect of Factor B in the form of variance in the outcome variable induced by Factor A</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle MS = \frac{SS_B}{df_B}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Sum of Squares for the Interaction between Factor A and Factor B (<span class="math inline">\(SS_{A\times B}\)</span>)</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It represents the interaction/combined effect of Factor A and B (intereffect?) in the form of variation in the outcome variable that is attributable to the interaction of Factor A and B
Factor A and B)</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle SS_{A \times B} = \sum_{j = 1}^{J}{\sum_{k = 1}^{K}{n_{ij}\left[ (\bar{y}_{\boldsymbol{\cdot}k} - \bar{y}) - (\bar{y}_{jk} - \bar{y}_{j\boldsymbol{\cdot}}) \right]^2 }}\)</span>
<ul>
<li><strong>Where</strong>
<ul>
<li><span class="math inline">\(n_{jk}\)</span> - The number of observations in level <span class="math inline">\(j\)</span> of Factor A and level <span class="math inline">\(k\)</span> of Factor B</li>
<li><span class="math inline">\((\bar{y}_{jk} - \bar{y}_{j\boldsymbol{\cdot}})\)</span>
<ul>
<li>The difference between the mean of the sample in level <span class="math inline">\(j\)</span> of Factor A and level <span class="math inline">\(k\)</span> of Factor B and the grand mean of level <span class="math inline">\(j\)</span></li>
<li>The effect of level <span class="math inline">\(k\)</span> of Factor B on level <span class="math inline">\(j\)</span> of Factor A</li>
</ul></li>
<li><span class="math inline">\((\bar{y}_{\boldsymbol{\cdot}k} - \bar{y})\)</span>
<ul>
<li>The difference between the grand mean of level <span class="math inline">\(k\)</span> of Factor A and the overall mean</li>
<li>This represents the main effect of level <span class="math inline">\(k\)</span> of Factor A</li>
</ul></li>
<li>$({y}<em>{k} - {y}) - ({y}</em>{jk} - {y}_{j}) $
<ul>
<li>The interaction effect</li>
<li>The effect of Factor A on the effect of Factor B</li>
<li>The effect of level <span class="math inline">\(j\)</span> of Factor A on the effect of Factor B<br />
</li>
<li>The effect of level <span class="math inline">\(k\)</span> of Factor B on the effect of Factor A</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Degrees of Freedom</strong>
<ul>
<li><span class="math inline">\(df_{A\times B} = (J-1)(K-1)\)</span></li>
</ul></li>
<li><strong>Mean Interaction Sum of Squares</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Variance in the outcome variable that is attributable to the interaction between Factor A and B</li>
<li>The mean amount of squared deviation in the outcome variable that is attributable to the interaction between Factor A and B</li>
<li>The average amount of squared deviation in the outcome variable that is attributable to the interaction between Factor A and B per each of the <span class="math inline">\(J\times K\)</span> samples</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(MS_{AB} = \frac{SS_{A \times B}}{df_{A\times B}}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Proof</strong>
<ul>
<li>$SS_T = <em>{j = 1}^{J}{</em>{k = 1}^{K}{<em>{i = 1}^{n</em>{jk}}{(y_{ijk} - {y})^2}}} \ E(SS_T) = E\ E(SS_T) = (N-1)^2 + <em>{j = 1}^{J}{</em>{k = 1}<sup>{K}{<em>{i = 1}<sup>{n_{jk}}{(<em>{j}^{2} + </em>{k}</sup>{2} + </em>{jk}</sup>{2})^2}}} $</li>
<li>Since <span class="math inline">\(\sum_{j = 1}^{J}{\alpha_{j}} = \sum_{k = 1}^{K}{\beta_{k}} = \sum_{j = 1}^{J}{\sum_{k = 1}^{K}{\delta_{jk}}} = 0\)</span>, all the cross-level products are 0, leaving us with the following:
<span class="math inline">\(\\ \displaystyle E(SS_T) = (N-1)\sigma^2 + \sum_{j = 1}^{J}{n_{j}\alpha_{j}^{2}} + \sum_{k = 1}^{K}{n_{k}\beta_{k}^{2}} + \sum_{j = 1}^{J}{\sum_{k = 1}^{K}{n_{jk}\delta_{jk}^{2}}} \\ \displaystyle E(SS_T) = \sum_{j = 1}^{J}{n_{j}\alpha_{j}^{2}} + \sum_{k = 1}^{K}{n_{k}\beta_{k}^{2}} + \sum_{j = 1}^{J}{\sum_{k = 1}^{K}{n_{jk}\delta_{jk}^{2}}} + (N-1)\sigma^2 \\ \displaystyle E(SS_T) = E(SS_A) + E(SS_B) + E(SS_{AB}) + E(SS_W)\)</span></li>
</ul></li>
<li><strong>Estimating the common variance</strong>
<ul>
<li><strong>Estimators of the population variance</strong>
<ul>
<li><span class="math inline">\(MS_W\)</span></li>
<li><span class="math inline">\(MS_A\)</span></li>
<li><span class="math inline">\(MS_B\)</span></li>
<li><span class="math inline">\(MS_{AB}\)</span></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_W\)</span> estimator</strong>
<ul>
<li>See One-Way ANOVA</li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_A\)</span> estimator</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle E(MS_A) = \sigma^2 + \frac{1}{J-1}K\sum_{j = 1}^{J}{n_j\alpha_{j}^{2}}\)</span></li>
</ul></li>
<li><strong>Implication of <span class="math inline">\(MS_A\)</span> as an estimator of the population variance</strong>
<ul>
<li>Under the null hypothesis, that is when all <span class="math inline">\(\alpha{}_{j}s = 0\)</span>, the linear combination <span class="math inline">\(\frac{1}{J-1}K\sum_{j = 1}^{J}{n_j\alpha_{j}^{2}} = 0\)</span>, therefore, <span class="math inline">\(\displaystyle E(MS_A) = \sigma^2\)</span>, the observed <span class="math inline">\(MS_A\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
<li>However, if the null hypothesis is false, that is when at least one of the <span class="math inline">\(\alpha{}_{j}s≠ 0\)</span>, the linear combination <span class="math inline">\(\frac{1}{J-1}K\sum_{j = 1}^{J}{n_j\alpha_{j}^{2}} &gt; 0\)</span>, hence, <span class="math inline">\(MS_A\)</span> is a positively biased estimator of <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_B\)</span> estimator</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle E(MS_B) = \sigma^2 + \frac{1}{K-1}J\sum_{k = 1}^{K}{n_k\beta_{k}^{2}}\)</span></li>
</ul></li>
<li><strong>Implication of <span class="math inline">\(MS_B\)</span> as an estimator of the population variance</strong>
<ul>
<li>Under the null hypothesis, that is when all <span class="math inline">\(\beta{}_{k}s = 0\)</span>, the linear combination <span class="math inline">\(\frac{1}{K-1}J\sum_{k = 1}^{K}{n_k\beta_{k}^{2}} = 0\)</span>, therefore, $ E(MS_A) = ^2$, the observed <span class="math inline">\(MS_B\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
<li>However, if the null hypothesis is false, that is when at least one of the <span class="math inline">\(\beta{}_{k}s≠ 0\)</span>, the linear combination <span class="math inline">\(\frac{1}{K-1}J\sum_{k = 1}^{K}{n_k\beta_{k}^{2}} &gt; 0\)</span>, hence, <span class="math inline">\(MS_B\)</span> is a positively biased estimator of <span class="math inline">\(\sigma^2\)</span></li>
</ul></li>
</ul></li>
<li><strong>The <span class="math inline">\(MS_{AB}\)</span> estimator</strong>
<ul>
<li><h2 id="concept" class="hasAnchor"><strong>Concept</strong><a href="anova.html#concept" class="anchor-section" aria-label="Anchor link to header"></a></h2></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle E(MS_{AB}) = \sigma^2 + \frac{1}{(J-1)(K-1)}\sum_{j = 1}^{J}{\sum_{k = 1}^{K}{n_{jk}\delta{}_{jk}^{2}}}\)</span></li>
</ul></li>
<li><strong>Implication of <span class="math inline">\(MS_B\)</span> as an estimator of the population variance</strong>
<ul>
<li>Under the null hypothesis, that is when all <span class="math inline">\(\delta{}_{jk}s = 0\)</span>, the linear combination <span class="math inline">\(\frac{1}{(J-1)(K-1)}\sum_{j = 1}^{J}{\sum_{k = 1}^{K}{n_{jk}\delta{}_{jk}^{2}}} = 0\)</span>, therefore, $ E(MS_{AB}) = ^2$, the observed <span class="math inline">\(MS_{AB}\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span></li>
<li>However, if the null hypothesis is false, that is when at least one of the <span class="math inline">\(\delta{}_{jk}s ≠ 0\)</span>, the linear combination <span class="math inline">\(\frac{1}{(J-1)(K-1)}\sum_{j = 1}^{J}{\sum_{k = 1}^{K}{n_{jk}\delta{}_{jk}^{2}}} &gt; 0\)</span>, hence, <span class="math inline">\(MS_B\)</span> is a positively biased estimator of <span class="math inline">\(\sigma^2\)</span><br />
</li>
<li></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Effects in a Two-Way ANOVA</strong>
<ul>
<li>Main Effect of Factor A</li>
<li>Main Effect of Factor B</li>
<li>Interaction effect of Factor A and B</li>
</ul></li>
<li><strong>Main Effect of Factor A</strong>
<ul>
<li><strong>General concept</strong>
<ul>
<li>The effect of Factor A on the outcome variable regardless of the levels of other categorical predictors in the model</li>
</ul></li>
<li><strong>Conceptual Hypotheses</strong>
<ul>
<li><strong>Null Hypothesis</strong>
<ul>
<li>Populations at different levels of Factor A have the same mean (the samples are from the same population)</li>
<li>Factor A, regardless of the levels of other categorical predictors), has no effect on the outcome variable</li>
<li>There are no differences in the outcome variable across levels of Factor A regardless of the level of all other categorical predictors in the model</li>
<li><span class="math inline">\(H_{0,A}: \mu_{1\boldsymbol{\cdot}} = \mu_{2\boldsymbol{\cdot}} = \cdots = \mu_{J\boldsymbol{\cdot}}\)</span></li>
</ul></li>
<li><strong>Alternative Hypothesis</strong>
<ul>
<li>Populations at different levels of Factor A have different means (the samples are from different populations)</li>
<li>Factor A, regardless of the levels of the other categorical predictors in the model), has an effect on the outcome variable regardless of the levels of other categorical predictors in the model</li>
<li>There are differences in the outcome variable across levels of Factor A (regardless of the level of all other categorical predictors in the model)</li>
<li><span class="math inline">\(H_{1,A}: \mu_{1\boldsymbol{\cdot}} ≠ \mu_{2\boldsymbol{\cdot}} ≠ \cdots ≠ \mu_{J\boldsymbol{\cdot}}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Operationalising the null hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It is based on the agreement/disagreement between the <span class="math inline">\(MS_{A}\)</span> estimator and the <span class="math inline">\(MS_{W}\)</span> estimator of the population variance (<span class="math inline">\(\sigma^2\)</span>)</li>
</ul></li>
<li><strong>The Test statistic: The F-ratio</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The first conceptual null hypothesis is formally operationalised as <span class="math inline">\(\sum_{j = 1}^{J}{n_j\alpha_{j}^{2}} = 0\)</span></li>
<li>The null hypothesis is tested by a test statistic that compares the <span class="math inline">\(MS_A\)</span> estimator with the unbiased <span class="math inline">\(MS_W\)</span> estimator as the <span class="math inline">\(F\)</span> ratio of <span class="math inline">\(MS_A\)</span> to <span class="math inline">\(MS_W\)</span></li>
<li>A standardised F-score</li>
<li>How much larger is the <span class="math inline">\(MS_A\)</span> estimate compared to <span class="math inline">\(MS_W\)</span></li>
<li>How big is the positive bias?</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle F_A = \frac{MS_A}{MS_W}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Null Hypothesis Significance Testing</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>To assess the probability of an F-statistic as extreme as or more extreme than the observed F-statistic in the distribution of the central F-statistic (the distribution of the F-statistic under the null hypothesis)</li>
</ul></li>
<li><strong>The Central F-distribution</strong>
<ul>
<li><span class="math inline">\(F \sim F\left(J-1 , N-JK \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Main Effect of Factor B</strong>
<ul>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null Hypothesis</strong>
<ul>
<li>Populations for different levels of Factor B have the same mean (the samples are from the same population)</li>
<li>Factor B has no effect on the outcome variable regardless of the levels of all other categorical predictors in the model</li>
<li>There are no systematic or real differences in the outcome variable across levels of Factor B regardless of the level of all other categorical predictors in the model</li>
<li><span class="math inline">\(H_{1,B}: \mu_{\boldsymbol{\cdot}1} = \mu_{\boldsymbol{\cdot}2} = \cdots = \mu_{\boldsymbol{\cdot}K}\)</span></li>
</ul></li>
<li><strong>Alternative Hypothesis</strong>
<ul>
<li>Populations for different levels of Factor B have different means (the samples are from different populations)</li>
<li>Factor B has an effect on the outcome variable regardless of the levels of all other categorical predictors in the model</li>
<li>There are systematic or real differences in the outcome variable across levels of Factor B regardless of the levels of all other categorical predictors in the model</li>
<li><span class="math inline">\(H_{1,B}: \mu_{\boldsymbol{\cdot}1} ≠ \mu_{\boldsymbol{\cdot}2} ≠ \cdots≠ \mu_{\boldsymbol{\cdot}K}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Operationalising the null hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It is based on the agreement/disagreement between the <span class="math inline">\(MS_{B}\)</span> estimator and the <span class="math inline">\(MS_{W}\)</span> estimator of the population variance (<span class="math inline">\(\sigma^2\)</span>)</li>
</ul></li>
<li><strong>The Test statistic: The F-ratio</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The first conceptual null hypothesis is formally operationalised as <span class="math inline">\(\sum_{k = 1}^{K}{n_k\beta_{k}^{2}} = 0\)</span></li>
<li>The null hypothesis is tested by a test statistic that compares the <span class="math inline">\(MS_B\)</span> estimator with the unbiased <span class="math inline">\(MS_W\)</span> estimator as the <span class="math inline">\(F\)</span> ratio of <span class="math inline">\(MS_B\)</span> to <span class="math inline">\(MS_W\)</span></li>
<li>A standardised F-score</li>
<li>How much larger is the <span class="math inline">\(MS_B\)</span> estimate compared to <span class="math inline">\(MS_W\)</span></li>
<li>How big is the positive bias?</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle F_B = \frac{MS_B}{MS_W}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Null Hypothesis Significance Testing</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>To assess the probability of an F-statistic as extreme as or more extreme than the observed F-statistic in the distribution of the central F-statistic (the distribution of the F-statistic under the null hypothesis)</li>
</ul></li>
<li><strong>The Central Distribution of <span class="math inline">\(F_B\)</span></strong>
<ul>
<li><span class="math inline">\(F_B \sim F\left(K-1 , N-JK \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Interaction effect of Factor A and B</strong>
<ul>
<li><strong>General Concept</strong>
<ul>
<li>The effect of one Factor on the other Factor</li>
<li>The effect of Factor A on Factor B, or equivalently, the effect of Factor B on Factor A</li>
<li>The combined effect of Factor A and B</li>
<li>It is something like the dot product of 2 vectors</li>
</ul></li>
<li><strong>Hypotheses</strong>
<ul>
<li><strong>Null Hypothesis</strong>
<ul>
<li>There is no interaction effect between Factor A and B - The effect of Factor A is the same across levels of Factor B, or equivalently, the effect of Factor B is the same across levels of Factor A</li>
<li>The additive main effect of particular levels of Factor A is the same across all levels of Factor B</li>
<li><span class="math inline">\(\begin{bmatrix} \alpha_1 = \alpha_{1,1} = \alpha_{1, 2} = \cdots = \alpha_{1, k} \\ \alpha_2 = \alpha_{2,1} = \alpha_{2, 2} = \cdots = \alpha_{2, k} \\ \vdots \\ \alpha_j = \alpha_{j,1} = \alpha_{j, 2} = \cdots = \alpha_{j, k} \end{bmatrix} \\ \begin{bmatrix} \beta_1 = \beta_{1,1} = \beta_{2, 1} = \cdots = \beta_{3, 1} \\ \beta_2 = \beta_{1,2} = \beta_{2, 2} = \cdots = \beta_{3, 2} \\ \vdots \\ \beta_k = \beta_{1,k} = \beta_{2, k} = \cdots = \beta_{j, k} \end{bmatrix}\)</span></li>
</ul></li>
<li><strong>Alternative Hypothesis</strong>
<ul>
<li>There is interaction effect between Factor A and B - The effect of Factor A is different in different levels of Factor B, or equivalently, the effect of Factor A is different in different levels of Factor A</li>
<li>The additive main effect of particular levels of Factor A is different in different levels of Factor B</li>
<li><span class="math inline">\(\begin{bmatrix} \alpha_1 ≠ \alpha_{1,1} ≠ \alpha_{1, 2} ≠ \cdots ≠ \alpha_{1, k} \\ \alpha_2 ≠ \alpha_{2,1} ≠ \alpha_{2, 2} ≠ \cdots ≠ \alpha_{2, k} \\ \vdots \\ \alpha_j ≠ \alpha_{j,1} ≠ \alpha_{j, 2} ≠ \cdots ≠ \alpha_{j, k} \end{bmatrix} \\ \begin{bmatrix} \beta_1 ≠ \beta_{1,1} ≠ \beta_{2, 1} ≠ \cdots ≠ \beta_{3, 1} \\ \beta_2 ≠ \beta_{1,2} ≠ \beta_{2, 2} ≠ \cdots ≠ \beta_{3, 2} \\ \vdots \\ \beta_k ≠ \beta_{1,k} ≠ \beta_{2, k} ≠ \cdots ≠ \beta_{j, k} \end{bmatrix}\)</span></li>
</ul></li>
</ul></li>
<li><strong>Operationalising the null hypothesis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>It is based on the agreement/disagreement between the <span class="math inline">\(MS_{AB}\)</span> estimator and the <span class="math inline">\(MS_{W}\)</span> estimator of the population variance (<span class="math inline">\(\sigma^2\)</span>)</li>
</ul></li>
<li><strong>The Test statistic: The F-ratio</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The first conceptual null hypothesis is formally operationalised as <span class="math inline">\(\sum_{j = 1}^{J}{\sum_{k = 1}^{K}{n_{jk}\delta{}_{jk}^{2}}} = 0\)</span></li>
<li>The null hypothesis is tested by a test statistic that compares the <span class="math inline">\(MS_{AB}\)</span> estimator with the unbiased <span class="math inline">\(MS_W\)</span> estimator as the <span class="math inline">\(F\)</span> ratio of <span class="math inline">\(MS_{AB}\)</span> to <span class="math inline">\(MS_W\)</span></li>
<li>A standardised F-score</li>
<li>How much larger is the <span class="math inline">\(MS_{AB}\)</span> estimate compared to <span class="math inline">\(MS_W\)</span></li>
<li>How big is the positive bias?</li>
</ul></li>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(\displaystyle F_{AB} = \frac{MS_{AB}}{MS_W}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Null Hypothesis Significance Testing</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>To assess the probability of an F-statistic as extreme as or more extreme than the observed F-statistic in the distribution of the central F-statistic (the distribution of the F-statistic under the null hypothesis)</li>
</ul></li>
<li><strong>The Central Distribution of <span class="math inline">\(F_B\)</span></strong>
<ul>
<li><span class="math inline">\(F_{AB} \sim F\left((J-1)(K-1) , N-JK \right)\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Analysing Interactions</strong>
<ul>
<li><strong>Methods for analysiing Interactions</strong>
<ul>
<li>Simple Effect Analysis</li>
<li>Johnson Neyman Interval</li>
<li></li>
</ul></li>
<li><strong>Simple Effect Analysis</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>The process of assessing the effect of one Factor at each level of another Factor</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Evaluation for ANOVA as a Linear Model</strong>
</p>
<ul>
<li>It requires <span class="math inline">\(k-1\)</span> dummy variables to be defined</li>
<li></li>
</ul>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>Two-Way ANOVA as a Linear Model</strong>
</p>
<ul>
<li><strong>2x2 ANOVA with interaction with Dummy Coding</strong>
<ul>
<li><strong>Mathematics</strong>
<ul>
<li><span class="math inline">\(y_{ijk} = \beta_{0} + \beta_{A}x_{A} + \beta_{B}x_{B} + \beta_{AB}x_{AB} + \epsilon_{ijk}\)</span>
<ul>
<li><strong><em>Where</em></strong>
<ul>
<li><span class="math inline">\(y_{ijk}\)</span>
<ul>
<li>Observation <span class="math inline">\(i\)</span> in level <span class="math inline">\(j\)</span> of Factor A and level <span class="math inline">\(k\)</span> of Factor B</li>
</ul></li>
<li><span class="math inline">\(\beta_{0}\)</span>
<ul>
<li>The intercept</li>
<li>The mean of the reference group</li>
<li><span class="math inline">\(\beta_{0} = \mu_{1,1}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{A}\)</span>
<ul>
<li>The effect of Factor A controlling for the effect of Factor B and their interaction</li>
<li>The difference between the mean of the sample at level 2 of Factor A and level 1 of Factor B (which is same as the intercept) and the intercept</li>
<li><span class="math inline">\(\beta_{A} = \mu_{2, 1} - \mu_{1, 1}\)</span></li>
</ul></li>
<li><span class="math inline">\(x_{A}\)</span>
<ul>
<li>Weight for dummy variable A</li>
<li><strong>Dummy Coding</strong>
<ul>
<li><span class="math inline">\(x_{A} = \begin{cases} 1 ~~~ \text{if the observation is in level 2 of Factor A} \\ 0 ~~~ \text{if the observation is not in level 2 of Factor A} \end{cases}\)</span></li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\beta_{B}\)</span>
<ul>
<li>The effect of Factor B controlling for the effect of Factor A and their interaction</li>
<li>The difference between the mean of the sample at level 1 of Factor A (which is same as the intercept) and level 2 of Factor B and the intercept</li>
</ul></li>
<li><span class="math inline">\(x_{B}\)</span>
<ul>
<li>Weight for dummy variable B</li>
<li><strong>Dummy Coding</strong>
<ul>
<li><span class="math inline">\(x_{B} = \begin{cases} 1 ~~~ \text{if the observation is in level 2 of Factor B} \\ 0 ~~~ \text{if the observation is not in level 2 of Factor B} \end{cases}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{B} = \mu_{1, 2} - \mu_{1, 1}\)</span></li>
</ul></li>
<li><span class="math inline">\(\beta_{AB}\)</span>
<ul>
<li>The interaction effect of Factor A and B controlling for the effect of Factor A and B</li>
<li>The combined effect of Factor A and B controlling for the effect of Factor A and B</li>
<li>The effect of level 2 of Factor A on <span class="math inline">\(\beta_{B}\)</span></li>
</ul></li>
<li><span class="math inline">\(x_{AB}\)</span>
<ul>
<li>Weight for dummy variable AB</li>
<li><strong>Dummy Coding</strong>
<ul>
<li><span class="math inline">\(x_{AB} = \begin{cases} 1 ~~~ \text{if the observation is in level 2 of Factor A and level 2 of Factor B} \\ 0 ~~~ \text{if the observation is not in level 2 of Factor A and level 2 of Factor B} \end{cases}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Other Types of Contrast Coding</strong>
<ul>
<li><strong>Introduction</strong>
<ul>
<li>In general, if you do other types of coding, the intercept will be the grand mean (the mean of group means) instead of the mean of the reference group (e.g. control group)</li>
</ul></li>
<li><strong>Types of coding</strong>
<ul>
<li>Dummy coding</li>
<li>Simple coding</li>
<li>Deviation Coding</li>
<li>Orthogonal Polynomial Coding</li>
<li>Helmert Coding</li>
<li>Reverse Helmert Coding</li>
<li>Forward Difference Coding</li>
<li>Backward Difference Coding</li>
</ul></li>
</ul></li>
<li><strong>Dummy Contrast Coding</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Each of the dummy variables represents the difference between the mean of the sample at each of the levels and the intercept which is the mean of the sample at the reference level (the reference group)</li>
</ul></li>
</ul></li>
<li><strong>Simple Contrast Coding</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Each dummy variable represents the difference between the mean of the sample at each of the levels and the mean of the sample at the reference level (the reference level), the intercept is the grand mean</li>
<li>This is similar to dummy coding, the only thing that is different is the intercept</li>
</ul></li>
</ul></li>
<li><strong>Deviation Contrast coding</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Each of the dummy variables represents the difference between the mean of each of the levels and the intercept which is the grand mean</li>
<li>Basically it is like dummy coding, however, the reference level is now the grand mean instead of the mean of the reference group, this is useful if you don’t have an apparent reference group</li>
</ul></li>
</ul></li>
<li><strong>Polynomial contrast</strong>
<ul>
<li><strong>Concept</strong>
<ul>
<li>Used for polynomial trend analysis - To test whether there are certain polynomial trends along the levels</li>
</ul></li>
</ul></li>
<li><strong>Resources</strong>
<ul>
<li><a href="https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/">UCLA Statistical Methods and Data Analytics</a></li>
</ul></li>
</ul>
<pre><code>## Rows: 200 Columns: 11
## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (11): id, female, race, ses, schtyp, prog, read, write, math, science, socst
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre><code>## # A tibble: 200 × 11
##       id female  race   ses schtyp  prog  read write  math science socst
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;
##  1    70      0     4     1      1     1    57    52    41      47    57
##  2   121      1     4     2      1     3    68    59    53      63    61
##  3    86      0     4     3      1     1    44    33    54      58    31
##  4   141      0     4     3      1     3    63    44    47      53    56
##  5   172      0     4     2      1     2    47    52    57      53    61
##  6   113      0     4     2      1     2    44    52    51      63    61
##  7    50      0     3     2      1     1    50    59    42      53    61
##  8    11      0     1     2      1     2    34    46    45      39    36
##  9    84      0     4     2      1     1    63    57    54      58    51
## 10    48      0     3     2      1     2    57    55    52      50    51
## # … with 190 more rows</code></pre>
<pre><code>##              .L   .Q         .C
## [1,] -0.6708204  0.5 -0.2236068
## [2,] -0.2236068 -0.5  0.6708204
## [3,]  0.2236068 -0.5 -0.6708204
## [4,]  0.6708204  0.5  0.2236068</code></pre>
<p>Deviation coding with interaction (both with deviation coding)
The intercept is the grand mean
Beta 1 is the effect of categorical variable 1 compared to the grand mean when the other categorical variable is at its mean level (the intercept)
Beta 2 is the effect of categorical variable 2 compared to the grand mean when the other categorical variable is at its mean level (the intercept)</p>
<p>Dummy coding for the first variable and deviation coding for the second variable with interaction
The intercept is the value in the outcome variable when categorical variable 1 is at 0 and when categorical variable 2 is at the grand mean
Beta 1 is the effect of categorical variable 1 at the mean level of categorical variable 2
Beta 2 is the effect of categorical variable 2 (the difference between the level of interest and the grand mean) when categorical variable 1 is at 0</p>
<p>Deviation coding for the first variable and dummy coding for the second variable with interaction
Beta 1 is the effect of categorical variable 1 (the difference between the level of interest and the mean) when the categorical variable 2 is at the intercept (in this case it is at the meat level of categorical variable 2)
Beta 2 is the effect of categorical variable 2 (the difference between the level of interest and its intercept) when categorical variable 1 is at the intercept (in this case it is the mean level)</p>
<p>General
Beta 1 is the effect of categorical variable 1 (the difference between the mean of the level of interest and its intercept) when beta 2 is held constant at its intercept
The same goes for beta 2
Beta 2 is the effect of categorical variable 2 (the difference between the mean of the level of interest and its intercept) when beta 1 is held constant at its intercept
The specific interpretation depends on how the intercept is defined</p>
<p>Let’s look at some examples:</p>
<ul>
<li><strong>Dummy coding for both categorical variables and without interaction</strong>
<ul>
<li><strong>The intercept</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The intercept is the fitted value of the outcome variable when both categorical variables 1 and 2 are at 0 (their reference levels)</li>
</ul></li>
<li><strong>How it is calculated</strong>
<ul>
<li>The value of the intercept is the predicted value on the regression slope for the effect of categorical variable 1 at 0 (the reference level) of categorical variable 2 (which has a slope of beta 1 and crosses the mean value of categorical variable 2’s reference level) when categorical variable 2 is at 0 (the reference level)</li>
</ul></li>
</ul></li>
<li><strong>Beta 1</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Beta 1 is the difference between the fitted value for categorical variable 1’s level of interest when categorical variable 2 is at 0 (its reference level) and the fitted value for its reference level when categorical variable 2 is at 0 (its reference level)</li>
</ul></li>
<li><strong>How it is calculated</strong>
<ul>
<li>This difference is the difference between the mean of categorical variable 1’s level of interest at the mean level of categorical variable 2 (or what I like to describe as the mean at categorical variable 1’s level of interest regardless of the levels of the other categorical variable) and the mean of the reference level of categorical variable 1 at the mean level of categorical variable 2 (or what I like to describe as the mean of categorical variable 1’s reference level regardless of the level of the other categorical variable)</li>
<li>Hence, this difference is not the actual difference between the means but it is the difference between the fitted values.</li>
</ul></li>
</ul></li>
<li><strong>Beta 2</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Beta 2 is similar to beta 1</li>
<li>Beta 2 is the difference between the fitted value for categorical variable 2’s level of interest when categorical variable 1 is at 0 (its reference level) and the fitted value for its reference level when categorical variable 1 is at 0 (its reference level)</li>
</ul></li>
<li><strong>How it is calculated</strong>
<ul>
<li>This difference is the difference between the mean of categorical variable 2’s level of interest at the mean level of categorical variable 1 (or what I like to describe as the mean at categorical variable 2’s level of interest regardless of the levels of the other categorical variable) and the mean of the reference level of categorical variable 2 at the mean level of categorical variable 1 (or what I like to describe as the mean of categorical variable 2’s reference level regardless of the level of the other categorical variable)</li>
<li>Similar to beta 1, this difference is not the actual difference between the means but it is the difference between the fitted values.</li>
</ul></li>
</ul></li>
<li><strong>Homogeneity of regression slopes</strong>
<ul>
<li>A model without an interaction term assumes Homogeneity of Regression Slopes.</li>
<li><strong>Homogeneity of Regression Slopes</strong> - The effect of one predictor is the same across values of other predictors (there is no interaction effect)</li>
</ul></li>
</ul></li>
<li><strong>Dummy coding for both categorical variables and with interaction</strong>
<ul>
<li><strong>The intercept</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>The intercept is the fitted value of the outcome variable when both categorical variables 1 and 2 are at 0 (their reference levels) taking into account interaction effects (if any)</li>
</ul></li>
<li><strong>How it is calculated</strong>
<ul>
<li>The value of the intercept is the predicted value on the regression slope for the effect of categorical variable 1 at 0 (the reference level) of categorical variable 2 (which has a slope of beta 1 and crosses the mean value of categorical variable 2’s reference level) when categorical variable 2 is at 0 (the reference level) adding the interaction effect (the effect of categorical variable 2’s reference level on the effect of categorical variable 1’s reference level)</li>
</ul></li>
</ul></li>
<li><strong>Beta 1</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Beta 1 is the difference between the fitted value for categorical variable 1’s level of interest when categorical variable 2 is at 0 (its reference level) at the mean level of the interaction effect and the fitted value for its reference level when categorical variable 2 is at 0 (its reference level) at the mean level of the interaction effect<br />
</li>
</ul></li>
<li><strong>How it is calculated</strong>
<ul>
<li>This difference is the difference between the mean of categorical variable 1’s level of interest at the mean level of categorical variable 2 (or what I like to describe as the mean at categorical variable 1’s level of interest regardless of the levels of the other categorical variable) at the mean level of interaction effect (the effect of categorical variable 2’s level of interest on the effect of categorical variable 1’s reference level) and the mean of the reference level of categorical variable 1 at the mean level of categorical variable 2 (or what I like to describe as the mean of categorical variable 1’s reference level regardless of the level of the other categorical variable) at the mean level of interaction effect (the effect of categorical variable 2’s reference level on the effect of categorical variable 1’s reference level)</li>
</ul></li>
</ul></li>
<li><strong>Beta 2</strong>
<ul>
<li><strong>Description</strong>
<ul>
<li>Beta 2 would be similar to beta 1 but flipped</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p>Deviation coding for both categorical variables and without interaction</p>
<p>Deviation coding for both categorical variables and with interaction
The intercept is the grand mean
Beta 1 is the difference between the categorical variable 1’s level of interest and its mean level when categorical variable 2 is at its mean level
Beta 2 is the difference between the categorical variable 2’s level of interest and its mean level when categorical variable 2 is at its mean level</p>
<p style="margin-bottom: 0px; font-size: 20px; ">
<strong>ANCOVA</strong>
</p>
<ul>
<li><strong>Types of Sum of Squares</strong>
<ul>
<li>This matters only if you are doing ANOVA as a linear model (using OLS)</li>
<li><strong>Types of Sum of Squares</strong>
<ul>
<li>Type 1</li>
<li>Type 2</li>
<li>Type 3</li>
<li>Type 4</li>
</ul></li>
</ul></li>
<li><h2 id="two-way-anova-as-a-linear-model" class="hasAnchor"><strong>Two-Way ANOVA as a Linear Model</strong><a href="anova.html#two-way-anova-as-a-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h2></li>
</ul>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="mathematics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="derivation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "night",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/samuel-mak/statistics/edit/master/04-Linear_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/samuel-mak/statistics/blob/master/04-Linear_Regression.Rmd",
"text": null
},
"download": ["bookdownproj.pdf", "bookdownproj.epub", "bookdownproj.mobi"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
